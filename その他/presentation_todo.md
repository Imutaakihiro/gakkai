# 学科発表までのTODOチェックリスト（卒業研究）

本プロジェクトの現状を確認し、学科発表に向けて必要な作業を整理しました。優先度順にチェックボックス形式で記載します。

## 最優先（本日〜明日）
- [ ] 実行環境の確認（Python 3.8+ / CUDA 11+ 推奨、GPU利用可否）
- [ ] 依存パッケージを整備（`pip install -r 99_その他/requirements.txt`）
- [ ] 既存モデルの評価結果を最新化（再実行・JSON出力の確認）
  - [ ] 感情分類モデルの評価: `python 00_スクリプト/evaluate_sentiment_model.py`
  - [ ] 評価スコア回帰モデルの学習: `python 00_スクリプト/train_score_model.py`
  - [ ] マルチタスクモデルの学習: `python 00_スクリプト/train_multitask_model.py`
- [ ] 比較レポート・図表の生成: `python 00_スクリプト/compare_models.py`
  - 生成物の所在: `03_結果/モデル評価/*.json`, `03_結果/モデル比較/*.png`, `03_結果/モデル比較/comparison_report.md`

## モデル・評価の準備（T-7〜5日）
- [ ] データ整合性チェック（欠損・ラベル分布・重複の再点検）
  - 想定データ: `01_データ/（自由記述→感情、自由記述→評価スコア、マルチタスク用）`
- [ ] ハイパラ最終確認（`batch_size`, `learning_rate`, `num_epochs`, `max_length`）
- [ ] マルチタスクの重み最終確認（`ALPHA`, `BETA`）
- [ ] 評価指標の一貫性（分類: Accuracy/F1/Precision/Recall、回帰: RMSE/MAE/R²/相関）
- [ ] 出力JSONのキー・値の確認（スライドへの転記用に主要数値を確定）

## 可視化・解釈（T-5〜3日）
- [ ] SHAP関連の出力確認・選定（代表例・集計図・誤分類例）
  - 想定出力: `03_結果/SHAP関連/**`
- [ ] モデル比較図の最終版を保存（解像度・フォント・配色）
- [ ] 誤分類/誤差の大きい事例の短いケーススタディ（2〜3例）
- [ ] 図表のキャプション・注記（データ数・前処理条件・指標定義）

## スライド（T-3〜2日）
- [ ] 目的・背景（課題設定、データ概要、ラベル設計）
- [ ] 提案・方法（単一タスク2種＋マルチタスク、モデル構成、学習条件）
- [ ] 実験設定（データ分割、前処理、評価指標）
- [ ] 結果（主要指標の表・グラフ、比較の要点）
- [ ] 解釈（SHAPで得られた示唆、具体例）
- [ ] 考察（得失・限界・誤差分析・改善余地）
- [ ] まとめ・今後（適用可能性、追加実験案、運用面）
- [ ] 参考文献（モデル、ライブラリ、関連研究）

## デモ・再現（T-2〜1日）
- [ ] 再現手順の最終確認（`04_Docs/README_実行手順.md` の更新点反映）
- [ ] 入出力のサンプル（予測デモ用の短いテキスト例 3〜5件）
- [ ] 予備実行（オフラインで主要スクリプトを1度通す）
- [ ] バックアップ（生成済みJSON/PNG/MDをZIP化、USB等に保存）

## リスク対応
- [ ] GPUが使えない場合の代替（`BATCH_SIZE`縮小、CPU実行時間の把握）
- [ ] フォント/文字化け対策（図のフォント埋め込み、英語版図の用意）
- [ ] 大容量ファイルの扱い（モデル/画像の圧縮・持ち運び）
- [ ] 外部モデル参照パスの確認
  - 例: `C:\Users\...\Downloads\BERT\git_excluded\finetuned_bert_model_...`

## 当日運用
- [ ] 発表原稿（読み上げメモ、注釈、質疑想定）
- [ ] 時間配分リハーサル（持ち時間内で収まるか）
- [ ] 予備機/会場PCでの表示確認（フォント・日本語表示・動画/画像）
- [ ] 連絡先・QR・リポジトリ/資料配布方法の用意

## 補足（技術メモ）
- 主なスクリプト（`00_スクリプト`）
  - `evaluate_sentiment_model.py`（感情分類の評価／外部Finetuned BERTを使用）
  - `train_score_model.py`（自由記述→評価スコア回帰の学習・評価）
  - `train_multitask_model.py`（感情＋評価スコアのマルチタスク学習）
  - `compare_models.py`（3モデルの比較図・サマリ生成）
- 主な出力先
  - `02_モデル/**`（ベストモデル・トークナイザ・設定JSON）
  - `03_結果/モデル評価/**.json`（各モデルの評価記録）
  - `03_結果/モデル比較/*.png, comparison_report.md`（図・Markdown）
  - `03_結果/SHAP関連/**`（解釈可視化）

---

不明点や追加してほしい観点（例: 倫理・プライバシー配慮、アブレーション計画、ベースライン比較の強化）があれば指示してください。必要に応じてスライド雛形（pptx/tex）も用意します。
