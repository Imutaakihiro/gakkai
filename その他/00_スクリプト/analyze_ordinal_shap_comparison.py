#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é †åºå›å¸°ãƒ¢ãƒ‡ãƒ« SHAPåˆ†æï¼ˆæ¯”è¼ƒç”¨ãƒ»ç°¡æ˜“ç‰ˆï¼‰
ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒã®ãŸã‚ã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿åˆ†æ

**ä½œæˆæ—¥**: 2025å¹´1æœˆ
**ç›®çš„**: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒåˆ†æ

å‡ºåŠ›å½¢å¼:
- word_importance_sentiment_production.csv (æ„Ÿæƒ…ã‚¹ã‚³ã‚¢)
- word_importance_course_production.csv (æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢)
- analysis_summary_production.json
- ordinal_shap_analysis_summary_production.md
- factor_categories_production.json
- å¯è¦–åŒ–PNGãƒ•ã‚¡ã‚¤ãƒ«
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

os.environ['TORCH_DISABLE_SAFETENSORS_WARNING'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import json
from datetime import datetime
import shap

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(BASE_DIR, "02_ãƒ¢ãƒ‡ãƒ«", "æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«", "class_level_ordinal_llp_20251030_162353.pth")
CSV_PATH = os.path.join(BASE_DIR, "01_ãƒ‡ãƒ¼ã‚¿", "ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿", "æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv")
OUTPUT_DIR = os.path.join(BASE_DIR, "03_åˆ†æçµæœ", "é †åºå›å¸°SHAPåˆ†æ_æ¯”è¼ƒç”¨")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼ˆGPUæœ€å„ªå…ˆï¼‰
def get_device():
    """GPUã‚’æœ€å„ªå…ˆã§é¸æŠï¼ˆCUDA â†’ DirectML â†’ CPUï¼‰"""
    # 1. CUDAã‚’è©¦ã™
    if torch.cuda.is_available():
        try:
            device = torch.device("cuda")
            _ = torch.tensor([1.0]).to(device)
            print(f"âœ… CUDAä½¿ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"   GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            return device
        except Exception as e:
            print(f"âš ï¸ CUDAã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. DirectMLã‚’è©¦ã™
    try:
        import torch_directml as dml
        if dml.is_available():
            device = dml.device()
            print(f"âœ… DirectMLä½¿ç”¨")
            return device
    except Exception:
        pass
    
    # 3. CPUï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
    print("âš ï¸ GPUãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CPUã§å®Ÿè¡Œã—ã¾ã™")
    return torch.device("cpu")

device = get_device()
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"PyTorch version: {torch.__version__}")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
print("="*60)
print("é †åºå›å¸°ãƒ¢ãƒ‡ãƒ« SHAPåˆ†æï¼ˆæ¯”è¼ƒç”¨ï¼‰")
print("="*60)

from train_class_level_ordinal_llp import CourseOrdinalLLPModel, BASE_MODEL
from transformers import BertJapaneseTokenizer

print("ğŸ“¥ é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
tokenizer = BertJapaneseTokenizer.from_pretrained(BASE_MODEL)
model = CourseOrdinalLLPModel(BASE_MODEL)
model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
model.to(device)
model.eval()
print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
df = pd.read_csv(CSV_PATH)
texts = df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].fillna("").astype(str).tolist()
print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(texts)}")

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
SAMPLE_SIZE = 1000
if len(texts) > SAMPLE_SIZE:
    np.random.seed(42)
    sample_indices = np.random.choice(len(texts), SAMPLE_SIZE, replace=False)
    sample_texts = [texts[i] for i in sample_indices]
else:
    sample_texts = texts
print(f"åˆ†æã‚µãƒ³ãƒ—ãƒ«æ•°: {len(sample_texts)}")

# ======================== äºˆæ¸¬é–¢æ•° ========================

MAX_LENGTH = 192
BATCH_SIZE = 16

def predict_sentiment(list_of_texts):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    pred = []
    for i in range(0, len(list_of_texts), BATCH_SIZE):
        batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH_SIZE]]
        encoding = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
        with torch.no_grad():
            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            _, _, _, y_sent, _ = model(input_ids, attention_mask, chunk_mask)
            pred.extend(y_sent.cpu().numpy().tolist())
    return np.array(pred).reshape(-1, 1)

def predict_course(list_of_texts):
    """æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    pred = []
    for i in range(0, len(list_of_texts), BATCH_SIZE):
        batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH_SIZE]]
        encoding = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
        with torch.no_grad():
            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            _, _, _, _, y_course = model(input_ids, attention_mask, chunk_mask)
            pred.extend(y_course.cpu().numpy().tolist())
    return np.array(pred).reshape(-1, 1)

# ======================== SHAPåˆ†æå®Ÿè¡Œ ========================

def merge_wordpieces(tokens, shap_vals_pos):
    """WordPieceã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆ##ï¼‰ã‚’å‰ã®èªã«çµåˆã—ã¦é›†ç´„ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: (merged_tokens, merged_shap_vals)
    ï¼ˆanalyze_sentiment_shap_5000.pyã¨åŒã˜å®Ÿè£…ã§æ•´åˆæ€§ã‚’ç¢ºä¿ï¼‰
    """
    merged_tokens = []
    merged_vals = []
    current = ''
    current_val = 0.0
    for tok, val in zip(tokens, shap_vals_pos):
        t = str(tok)
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
        if t in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
            continue
        if t.startswith('##'):
            # é€£çµï¼ˆæ¥é ­ã®##ã‚’é™¤å»ã—ã¦å‰èªã«è¿½åŠ ï¼‰
            current += t[2:]
            current_val += float(val)
        else:
            # ç›´å‰ã®èªã‚’ç¢ºå®š
            if current:
                merged_tokens.append(current)
                merged_vals.append(current_val)
            current = t
            current_val = float(val)
    if current:
        merged_tokens.append(current)
        merged_vals.append(current_val)
    return merged_tokens, merged_vals

def run_shap_analysis(predict_fn, texts, name, output_dir):
    """SHAPåˆ†æã‚’å®Ÿè¡Œï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜æ–¹æ³•ã§çµ±ä¸€ï¼‰
    
    ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼ˆanalyze_classlevel_multitask_shap_beeswarm.pyï¼‰ã¨åŒã˜è¨ˆç®—æ–¹æ³•ï¼š
    - importance = np.abs(shap_values.values).mean(axis=0)
    - WordPieceã®çµåˆã¯è¡Œã‚ãªã„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ã¾ã¾ï¼‰
    
    ãŸã ã—ã€ä¸è¦å‰‡ãªå½¢çŠ¶ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å‡¦ç†ã—ã¦é›†è¨ˆ
    """
    try:
        print(f"\nğŸ” SHAPåˆ†æå®Ÿè¡Œ: {name}")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(texts)}ä»¶")
        print(f"   âš ï¸ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜æ–¹æ³•ã§è¨ˆç®—ï¼ˆWordPieceçµåˆãªã—ï¼‰")
        
        explainer = shap.Explainer(predict_fn, tokenizer)
        shap_values = explainer(texts)
        
        # ä¸è¦å‰‡ãªå½¢çŠ¶ã«å¯¾å¿œï¼šå„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å‡¦ç†
        # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®SHAPå€¤ã‚’é›†è¨ˆï¼ˆWordPieceçµåˆãªã—ï¼‰
        token_importance_dict = defaultdict(lambda: {'shap_values': [], 'count': 0})
        
        # shap_valuesã¯Explanationã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã€å„ã‚µãƒ³ãƒ—ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
        if isinstance(shap_values, shap.Explanation):
            # å„ã‚µãƒ³ãƒ—ãƒ«ã‚’å€‹åˆ¥ã«å‡¦ç†
            for sv in shap_values:
                if hasattr(sv, 'values') and hasattr(sv, 'data'):
                    tokens = sv.data
                    vals = sv.values
                    
                    # å½¢çŠ¶ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å‡¦ç†
                    if hasattr(vals, 'ndim') and vals.ndim > 1:
                        # å›å¸°ã‚¿ã‚¹ã‚¯ã®å ´åˆã€valsã®å½¢çŠ¶ã¯(n_tokens, 1)ã¾ãŸã¯(n_tokens,)
                        if vals.shape[1] == 1:
                            vals_abs = np.abs(vals).flatten()  # (n_tokens, 1) -> (n_tokens,)
                        else:
                            vals_abs = np.abs(vals[:, 0])  # æœ€åˆã®å‡ºåŠ›ã‚’ä½¿ç”¨
                    else:
                        vals_abs = np.abs(vals)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨SHAPå€¤ã‚’å¯¾å¿œä»˜ã‘ï¼ˆWordPieceçµåˆãªã—ï¼‰
                    for token, val in zip(tokens, vals_abs):
                        if token and str(token).strip() and str(token) not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
                            token_importance_dict[str(token)]['shap_values'].append(float(val))
                            token_importance_dict[str(token)]['count'] += 1
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®å¹³å‡é‡è¦åº¦ã‚’è¨ˆç®—ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜æ–¹æ³•ï¼‰
        token_stats = {
            token: np.mean(data['shap_values'])
            for token, data in token_importance_dict.items()
            if data['count'] > 0
        }
        
        # DataFrameã«å¤‰æ›ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜å½¢å¼ï¼‰
        df_importance = pd.DataFrame({
            'word': list(token_stats.keys()),
            'importance': list(token_stats.values())
        }).sort_values('importance', ascending=False)
        
        # å³åº§ã«CSVä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
        csv_path = f"{output_dir}/word_importance_{name.lower().replace(' ', '_').replace('ï¼ˆ', '').replace('ï¼‰', '').replace('[', '').replace(']', '')}_production.csv"
        df_importance.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"âœ… {name} å®Œäº†: {len(df_importance)}èª")
        print(f"   ğŸ“ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}")
        
        return shap_values, df_importance
        
    except Exception as e:
        print(f"âŒ {name} ã®SHAPåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"   ğŸ’¡ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œï¼ˆç©ºã®DataFrameã‚’è¿”ã™ï¼‰
        return None, pd.DataFrame({'word': [], 'importance': []})

# SHAPåˆ†æå®Ÿè¡Œï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰
print("\n" + "="*60)
print("SHAPåˆ†æå®Ÿè¡Œä¸­ï¼ˆæ¯”è¼ƒç”¨ï¼šæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ»æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰...")
print("="*60)
print("âš ï¸  å„åˆ†æãŒå®Œäº†æ¬¡ç¬¬ã€çµæœã‚’å³åº§ã«ä¿å­˜ã—ã¾ã™")
print("   é€”ä¸­ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã€å®Œäº†ã—ãŸåˆ†æçµæœã¯ä¿å­˜ã•ã‚Œã¾ã™")
print("="*60)

shap_results = {}
completed_analyses = []

# æ¯”è¼ƒç”¨ï¼šæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿
analyses = [
    ("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢", predict_sentiment, "sentiment"),
    ("æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢", predict_course, "course"),
]

for name, predict_fn, key in analyses:
    try:
        shap_val, df_imp = run_shap_analysis(predict_fn, sample_texts, name, OUTPUT_DIR)
        # df_impãŒç©ºã§ãªã‘ã‚Œã°æˆåŠŸï¼ˆshap_valã¯Noneã§ã‚‚å•é¡Œãªã„ï¼‰
        if len(df_imp) > 0:
            shap_results[key] = {'shap': shap_val, 'df': df_imp}
            completed_analyses.append(key)
            print(f"âœ… {name} ã®åˆ†æã¨ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            print(f"âš ï¸  {name} ã®åˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼ˆã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ç©ºã®çµæœï¼‰")
    except Exception as e:
        print(f"âŒ {name} ã®åˆ†æã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"   æ¬¡ã®åˆ†æã‚’ç¶šè¡Œã—ã¾ã™...")
        continue

print(f"\nâœ… å®Œäº†ã—ãŸåˆ†æ: {len(completed_analyses)}/{len(analyses)}")
print(f"   å®Œäº†ãƒªã‚¹ãƒˆ: {', '.join(completed_analyses)}")

# ======================== çµæœä¿å­˜ ========================

print("\n" + "="*60)
print("çµæœä¿å­˜ä¸­...")
print("="*60)

# TOP100ä¿å­˜
print("\nğŸ“Š TOP100ã®ä¿å­˜ä¸­...")
for key, data in shap_results.items():
    if 'df' in data and len(data['df']) > 0:
        df = data['df']
        top100_path = f"{OUTPUT_DIR}/word_importance_{key}_top100_production.csv"
        df.head(100).to_csv(top100_path, index=False, encoding='utf-8')
        print(f"   âœ… {key} ã®TOP100ã‚’ä¿å­˜: {top100_path}")

print("âœ… CSVä¿å­˜å®Œäº†")

# ======================== ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ ========================

def categorize_factors(sentiment_df, course_df, threshold=0.0005):
    """è¦å› ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç‰ˆã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    sent_dict = dict(zip(sentiment_df['word'], sentiment_df['importance']))
    course_dict = dict(zip(course_df['word'], course_df['importance']))
    
    all_words = set(sentiment_df['word']) | set(course_df['word'])
    
    categories = {
        'strong_common': [],
        'sentiment_leaning': [],
        'course_leaning': [],
        'sentiment_specific': [],
        'course_specific': []
    }
    
    for word in all_words:
        sent_imp = sent_dict.get(word, 0)
        course_imp = course_dict.get(word, 0)
        
        if sent_imp >= threshold and course_imp >= threshold:
            categories['strong_common'].append((word, sent_imp, course_imp))
        elif sent_imp >= threshold and course_imp < threshold * 0.5:
            categories['sentiment_specific'].append((word, sent_imp, course_imp))
        elif course_imp >= threshold and sent_imp < threshold * 0.5:
            categories['course_specific'].append((word, sent_imp, course_imp))
        elif sent_imp >= threshold * 0.5 and course_imp >= threshold * 0.5:
            if sent_imp > course_imp * 1.5:
                categories['sentiment_leaning'].append((word, sent_imp, course_imp))
            elif course_imp > sent_imp * 1.5:
                categories['course_leaning'].append((word, sent_imp, course_imp))
    
    for cat in categories:
        categories[cat].sort(key=lambda x: x[1] + x[2], reverse=True)
    
    return categories

# ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ãŒä¸¡æ–¹å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
if 'sentiment' in shap_results and 'course' in shap_results:
    df_sent = shap_results['sentiment']['df']
    df_course = shap_results['course']['df']
    categories = categorize_factors(df_sent, df_course)
else:
    print("âš ï¸  æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¾ãŸã¯æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®åˆ†æãŒå®Œäº†ã—ã¦ã„ãªã„ãŸã‚ã€ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    categories = {
        'strong_common': [],
        'sentiment_leaning': [],
        'course_leaning': [],
        'sentiment_specific': [],
        'course_specific': []
    }

# JSONä¿å­˜
categories_json = {}
for category, items in categories.items():
    categories_json[category] = [
        {'word': word, 'sentiment_importance': s_imp, 'course_importance': c_imp}
        for word, s_imp, c_imp in items
    ]

with open(f"{OUTPUT_DIR}/factor_categories_production.json", 'w', encoding='utf-8') as f:
    json.dump(categories_json, f, ensure_ascii=False, indent=2)

print("âœ… ã‚«ãƒ†ã‚´ãƒªåˆ†é¡å®Œäº†")

# ======================== å¯è¦–åŒ– ========================

print("\nğŸ“Š å¯è¦–åŒ–ä½œæˆä¸­...")

# 1. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢TOP30
if 'sentiment' in shap_results:
    df_sent = shap_results['sentiment']['df']
    plt.figure(figsize=(12, 8))
    top30_sent = df_sent.head(30)
    plt.barh(range(len(top30_sent)), top30_sent['importance'].values[::-1])
    plt.yticks(range(len(top30_sent)), top30_sent['word'].values[::-1])
    plt.xlabel('é‡è¦åº¦')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ é‡è¦èªTOP30ï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/sentiment_top30_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… æ„Ÿæƒ…ã‚¹ã‚³ã‚¢TOP30ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")

# 2. æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢TOP30
if 'course' in shap_results:
    df_course = shap_results['course']['df']
    plt.figure(figsize=(12, 8))
    top30_course = df_course.head(30)
    plt.barh(range(len(top30_course)), top30_course['importance'].values[::-1])
    plt.yticks(range(len(top30_course)), top30_course['word'].values[::-1])
    plt.xlabel('é‡è¦åº¦')
    plt.title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ é‡è¦èªTOP30ï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/course_top30_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢TOP30ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")

# 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒãƒ£ãƒ¼ãƒˆ
plt.figure(figsize=(10, 6))
cat_counts = {k: len(v) for k, v in categories.items()}
plt.bar(cat_counts.keys(), cat_counts.values())
plt.ylabel('è¦å› æ•°')
plt.title('è¦å› ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†å¸ƒï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/factor_categories_chart_production.png", dpi=300, bbox_inches='tight')
plt.close()

print("âœ… å¯è¦–åŒ–å®Œäº†")

# ======================== ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ ========================

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# JSONã‚µãƒãƒªãƒ¼
summary = {
    "analysis_date": timestamp,
    "device_used": str(device),
    "pytorch_version": torch.__version__,
    "method": "é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«SHAPåˆ†æï¼ˆæ¯”è¼ƒç”¨ï¼‰",
    "sample_size": len(sample_texts),
    "completed_analyses": completed_analyses,
    "category_counts": {k: len(v) for k, v in categories.items()},
}

# å®Œäº†ã—ãŸåˆ†æã®æƒ…å ±ã‚’è¿½åŠ 
for key in ['sentiment', 'course']:
    if key in shap_results:
        df = shap_results[key]['df']
        summary[f"total_words_{key}"] = len(df)
        summary[f"top_{key}_factors"] = dict(df.head(20).values) if len(df) > 0 else {}

# å…±é€šè¦å› 
if 'sentiment' in shap_results and 'course' in shap_results:
    df_sent = shap_results['sentiment']['df']
    df_course = shap_results['course']['df']
    summary["common_words_count"] = len(set(df_sent['word']) & set(df_course['word']))
    summary["strong_common_factors"] = [
        {"word": word, "sentiment": s_imp, "course": c_imp}
        for word, s_imp, c_imp in categories['strong_common'][:20]
    ]

with open(f"{OUTPUT_DIR}/analysis_summary_production.json", 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ
report = f"""# é †åºå›å¸°ãƒ¢ãƒ‡ãƒ« SHAPåˆ†æçµæœã‚µãƒãƒªãƒ¼ï¼ˆæ¯”è¼ƒç”¨ï¼‰

**ä½œæˆæ—¥**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {timestamp}
- åˆ†æå¯¾è±¡: é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆCORALå‹ã€LLPæå¤±ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(sample_texts)}ä»¶
- ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}
- PyTorch version: {torch.__version__}
- **ç›®çš„**: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒåˆ†æ

## åˆ†æçµæœã‚µãƒãƒªãƒ¼
- å®Œäº†ã—ãŸåˆ†æ: {len(completed_analyses)}/{len(analyses)}ä»¶
- å®Œäº†ãƒªã‚¹ãƒˆ: {', '.join(completed_analyses) if completed_analyses else 'ãªã—'}
"""

# å®Œäº†ã—ãŸåˆ†æã®è¦å› æ•°ã‚’è¿½åŠ 
for key, name in [('sentiment', 'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢'), ('course', 'æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢')]:
    if key in shap_results:
        report += f"- {name}äºˆæ¸¬è¦å› æ•°: {len(shap_results[key]['df'])}å˜èª\n"

# å…±é€šè¦å› æ•°
if 'sentiment' in shap_results and 'course' in shap_results:
    df_sent = shap_results['sentiment']['df']
    df_course = shap_results['course']['df']
    report += f"- å…±é€šè¦å› æ•°: {len(set(df_sent['word']) & set(df_course['word']))}å˜èª\n"

report += f"- å¼·ã„å…±é€šè¦å› æ•°: {len(categories['strong_common'])}å˜èª\n"

report += f"""
## ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°

### å¼·ã„å…±é€šè¦å›  ({len(categories['strong_common'])}ä»¶)
| é †ä½ | å˜èª | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ | ç·åˆé‡è¦åº¦ |
|------|------|------------|------------|------------|
"""

for i, (word, s_imp, c_imp) in enumerate(categories['strong_common'][:20], 1):
    report += f"| {i} | {word} | {s_imp:.6f} | {c_imp:.6f} | {s_imp + c_imp:.6f} |\n"

report += f"""
### æ„Ÿæƒ…ç‰¹åŒ–è¦å›  ({len(categories['sentiment_specific'])}ä»¶)
| é †ä½ | å˜èª | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ |
|------|------|------------|------------|
"""

for i, (word, s_imp, c_imp) in enumerate(categories['sentiment_specific'][:15], 1):
    report += f"| {i} | {word} | {s_imp:.6f} | {c_imp:.6f} |\n"

report += f"""
### è©•ä¾¡ç‰¹åŒ–è¦å›  ({len(categories['course_specific'])}ä»¶)
| é †ä½ | å˜èª | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ |
|------|------|------------|------------|
"""

for i, (word, s_imp, c_imp) in enumerate(categories['course_specific'][:15], 1):
    report += f"| {i} | {word} | {s_imp:.6f} | {c_imp:.6f} |\n"

report += """
## ä¸»è¦ãªç™ºè¦‹

### 1. é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´
- ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã‚’äºˆæ¸¬
- é †åºæ€§ã‚’è€ƒæ…®ã—ãŸãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€ã‚ˆã‚Šé©åˆ‡ãªäºˆæ¸¬ãŒå¯èƒ½

### 2. ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
- å¼·ã„å…±é€šè¦å› : æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã«å½±éŸ¿
- ç‰¹åŒ–è¦å› : ãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ã«ç‰¹æœ‰ã®å½±éŸ¿

### 3. ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ
- é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã§ã€é‡è¦èªã®é•ã„ã‚’ç¢ºèª
- é †åºæ€§ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé©åˆ‡ãªè¦å› åˆ†æãŒå¯èƒ½

---
**åˆ†æå®Œäº†**: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S')

with open(f"{OUTPUT_DIR}/ordinal_shap_analysis_summary_production.md", 'w', encoding='utf-8') as f:
    f.write(report)

print("âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†")

print("\n" + "="*60)
print("âœ… å…¨ã¦ã®åˆ†æå®Œäº†ï¼")
print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {OUTPUT_DIR}")
print("="*60)

