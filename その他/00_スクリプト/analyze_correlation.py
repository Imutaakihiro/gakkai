#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç›¸é–¢åˆ†æã¨ç„¡ç›¸é–¢åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import pearsonr, spearmanr, kendalltau
import warnings
import sys
import os
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆWindowsç”¨ï¼‰
try:
    plt.rcParams['font.family'] = 'MS Gothic'  # Windowsã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
except:
    try:
        plt.rcParams['font.family'] = 'Yu Gothic'  # ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆ
    except:
        plt.rcParams['font.family'] = 'DejaVu Sans'  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
plt.rcParams['axes.unicode_minus'] = False

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    df = pd.read_csv('../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv')
    
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶")
    print(f"åˆ—å: {list(df.columns)}")
    
    return df

def basic_statistics(df):
    """åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—"""
    print("\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆé‡")
    print("=" * 50)
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ
    sentiment_stats = df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].describe()
    print("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ã®çµ±è¨ˆ:")
    print(sentiment_stats)
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ
    score_stats = df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].describe()
    print("\næˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ:")
    print(score_stats)
    
    # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
    print(f"\næ¬ æå€¤:")
    print(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡: {df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].isnull().sum()}ä»¶")
    print(f"æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢: {df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].isnull().sum()}ä»¶")

def correlation_analysis(df):
    """ç›¸é–¢åˆ†æ"""
    print("\nğŸ”— ç›¸é–¢åˆ†æ")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    sentiment = df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].dropna()
    score = df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].dropna()
    
    # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
    common_idx = sentiment.index.intersection(score.index)
    sentiment_common = sentiment.loc[common_idx]
    score_common = score.loc[common_idx]
    
    print(f"åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿æ•°: {len(common_idx)}ä»¶")
    
    # ãƒ”ã‚¢ã‚½ãƒ³ã®ç›¸é–¢ä¿‚æ•°
    pearson_r, pearson_p = pearsonr(sentiment_common, score_common)
    print(f"\nãƒ”ã‚¢ã‚½ãƒ³ã®ç›¸é–¢ä¿‚æ•°: {pearson_r:.4f}")
    print(f"på€¤: {pearson_p:.6f}")
    
    # ã‚¹ãƒ”ã‚¢ãƒãƒ³ã®é †ä½ç›¸é–¢ä¿‚æ•°
    spearman_r, spearman_p = spearmanr(sentiment_common, score_common)
    print(f"\nã‚¹ãƒ”ã‚¢ãƒãƒ³ã®é †ä½ç›¸é–¢ä¿‚æ•°: {spearman_r:.4f}")
    print(f"på€¤: {spearman_p:.6f}")
    
    # ã‚±ãƒ³ãƒ‰ãƒ¼ãƒ«ã®é †ä½ç›¸é–¢ä¿‚æ•°
    kendall_tau, kendall_p = kendalltau(sentiment_common, score_common)
    print(f"\nã‚±ãƒ³ãƒ‰ãƒ¼ãƒ«ã®é †ä½ç›¸é–¢ä¿‚æ•°: {kendall_tau:.4f}")
    print(f"på€¤: {kendall_p:.6f}")
    
    # ç›¸é–¢ã®å¼·ã•ã®è§£é‡ˆ
    print(f"\nç›¸é–¢ã®å¼·ã•ã®è§£é‡ˆ:")
    if abs(pearson_r) < 0.1:
        strength = "ç„¡ç›¸é–¢"
    elif abs(pearson_r) < 0.3:
        strength = "å¼±ã„ç›¸é–¢"
    elif abs(pearson_r) < 0.5:
        strength = "ä¸­ç¨‹åº¦ã®ç›¸é–¢"
    elif abs(pearson_r) < 0.7:
        strength = "å¼·ã„ç›¸é–¢"
    else:
        strength = "éå¸¸ã«å¼·ã„ç›¸é–¢"
    
    print(f"ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•° {pearson_r:.4f} â†’ {strength}")
    
    return pearson_r, pearson_p, spearman_r, spearman_p, kendall_tau, kendall_p

def independence_test(df):
    """ç„¡ç›¸é–¢åˆ†æï¼ˆç‹¬ç«‹æ€§æ¤œå®šï¼‰"""
    print("\nğŸš« ç„¡ç›¸é–¢åˆ†æï¼ˆç‹¬ç«‹æ€§æ¤œå®šï¼‰")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    sentiment = df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].dropna()
    score = df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].dropna()
    
    # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
    common_idx = sentiment.index.intersection(score.index)
    sentiment_common = sentiment.loc[common_idx]
    score_common = score.loc[common_idx]
    
    # ã‚«ã‚¤äºŒä¹—ç‹¬ç«‹æ€§æ¤œå®šã®ãŸã‚ã®åˆ†å‰²
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†å‰²
    sentiment_categories = pd.cut(sentiment_common, 
                                 bins=[-np.inf, -0.1, 0.1, np.inf], 
                                 labels=['ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«', 'ãƒã‚¸ãƒ†ã‚£ãƒ–'])
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†å‰²
    score_categories = pd.cut(score_common, 
                             bins=[-np.inf, 3.0, 3.5, np.inf], 
                             labels=['ä½è©•ä¾¡', 'ä¸­è©•ä¾¡', 'é«˜è©•ä¾¡'])
    
    # ã‚¯ãƒ­ã‚¹é›†è¨ˆè¡¨ã®ä½œæˆ
    contingency_table = pd.crosstab(sentiment_categories, score_categories)
    print("ã‚¯ãƒ­ã‚¹é›†è¨ˆè¡¨:")
    print(contingency_table)
    
    # ã‚«ã‚¤äºŒä¹—ç‹¬ç«‹æ€§æ¤œå®š
    chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
    
    print(f"\nã‚«ã‚¤äºŒä¹—çµ±è¨ˆé‡: {chi2:.4f}")
    print(f"è‡ªç”±åº¦: {dof}")
    print(f"på€¤: {p_value:.6f}")
    
    # æœŸå¾…åº¦æ•°
    print(f"\næœŸå¾…åº¦æ•°:")
    expected_df = pd.DataFrame(expected, 
                             index=contingency_table.index, 
                             columns=contingency_table.columns)
    print(expected_df)
    
    # ç‹¬ç«‹æ€§ã®åˆ¤å®š
    alpha = 0.05
    if p_value < alpha:
        print(f"\nçµè«–: på€¤({p_value:.6f}) < Î±({alpha}) â†’ ç‹¬ç«‹ã§ã¯ãªã„ï¼ˆç›¸é–¢ã‚ã‚Šï¼‰")
    else:
        print(f"\nçµè«–: på€¤({p_value:.6f}) â‰¥ Î±({alpha}) â†’ ç‹¬ç«‹ï¼ˆç„¡ç›¸é–¢ï¼‰")
    
    return chi2, p_value, dof, contingency_table

def create_visualizations(df):
    """å¯è¦–åŒ–ã®ä½œæˆ"""
    print("\nğŸ“Š å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        sentiment = df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].dropna()
        score = df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].dropna()
        
        # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
        common_idx = sentiment.index.intersection(score.index)
        sentiment_common = sentiment.loc[common_idx]
        score_common = score.loc[common_idx]
        
        # å›³ã®è¨­å®š
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Correlation Analysis: Sentiment Score vs Course Evaluation Score', 
                     fontsize=16, fontweight='bold')
        
        # 1. æ•£å¸ƒå›³
        axes[0, 0].scatter(sentiment_common, score_common, alpha=0.6, s=20)
        axes[0, 0].set_xlabel('Sentiment Score Average')
        axes[0, 0].set_ylabel('Course Evaluation Score')
        axes[0, 0].set_title('Scatter Plot')
        axes[0, 0].grid(True, alpha=0.3)
        
        # å›å¸°ç›´ç·šã®è¿½åŠ 
        z = np.polyfit(sentiment_common, score_common, 1)
        p = np.poly1d(z)
        axes[0, 0].plot(sentiment_common, p(sentiment_common), "r--", alpha=0.8)
        
        # 2. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆ2Dãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
        axes[0, 1].hist2d(sentiment_common, score_common, bins=20, cmap='Blues')
        axes[0, 1].set_xlabel('Sentiment Score Average')
        axes[0, 1].set_ylabel('Course Evaluation Score')
        axes[0, 1].set_title('2D Histogram')
        
        # 3. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
        axes[1, 0].hist(sentiment_common, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[1, 0].set_xlabel('Sentiment Score Average')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Sentiment Score Distribution')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
        axes[1, 1].hist(score_common, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
        axes[1, 1].set_xlabel('Course Evaluation Score')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Course Evaluation Score Distribution')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªãƒ»ä½œæˆ
        output_dir = '../03_åˆ†æçµæœ'
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        output_path = os.path.join(output_dir, 'ç›¸é–¢åˆ†æ_æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        plt.close()
        
    except Exception as e:
        print(f"å¯è¦–åŒ–ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œã—ã¾ã™...")

def detailed_analysis(df):
    """è©³ç´°åˆ†æ"""
    print("\nğŸ” è©³ç´°åˆ†æ")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    sentiment = df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].dropna()
    score = df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].dropna()
    
    # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
    common_idx = sentiment.index.intersection(score.index)
    sentiment_common = sentiment.loc[common_idx]
    score_common = score.loc[common_idx]
    
    # å››åˆ†ä½æ•°ã§ã®åˆ†æ
    print("å››åˆ†ä½æ•°ã§ã®åˆ†æ:")
    sentiment_q = sentiment_common.quantile([0.25, 0.5, 0.75])
    score_q = score_common.quantile([0.25, 0.5, 0.75])
    
    print(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢: Q1={sentiment_q[0.25]:.3f}, Q2={sentiment_q[0.5]:.3f}, Q3={sentiment_q[0.75]:.3f}")
    print(f"æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢: Q1={score_q[0.25]:.3f}, Q2={score_q[0.5]:.3f}, Q3={score_q[0.75]:.3f}")
    
    # æ¥µå€¤ã®åˆ†æ
    print(f"\næ¥µå€¤ã®åˆ†æ:")
    print(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢æœ€å°å€¤: {sentiment_common.min():.3f}")
    print(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢æœ€å¤§å€¤: {sentiment_common.max():.3f}")
    print(f"æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢æœ€å°å€¤: {score_common.min():.3f}")
    print(f"æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢æœ€å¤§å€¤: {score_common.max():.3f}")
    
    # å¤–ã‚Œå€¤ã®æ¤œå‡º
    Q1_sentiment = sentiment_common.quantile(0.25)
    Q3_sentiment = sentiment_common.quantile(0.75)
    IQR_sentiment = Q3_sentiment - Q1_sentiment
    lower_bound_sentiment = Q1_sentiment - 1.5 * IQR_sentiment
    upper_bound_sentiment = Q3_sentiment + 1.5 * IQR_sentiment
    
    outliers_sentiment = sentiment_common[(sentiment_common < lower_bound_sentiment) | 
                                        (sentiment_common > upper_bound_sentiment)]
    
    print(f"\næ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®å¤–ã‚Œå€¤: {len(outliers_sentiment)}ä»¶")
    if len(outliers_sentiment) > 0:
        print(f"å¤–ã‚Œå€¤ã®ç¯„å›²: {outliers_sentiment.min():.3f} ~ {outliers_sentiment.max():.3f}")

def save_results(pearson_r, pearson_p, spearman_r, spearman_p, kendall_tau, kendall_p, 
                chi2, chi2_p, contingency_table):
    """çµæœã®ä¿å­˜"""
    print("\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
    
    try:
        # çµæœã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
        results = {
            "analysis_date": pd.Timestamp.now().strftime("%Y%m%d_%H%M%S"),
            "data_source": "æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv",
            "sample_size": int(contingency_table.sum().sum()),
            "correlation_analysis": {
                "pearson": {
                    "correlation_coefficient": float(pearson_r),
                    "p_value": float(pearson_p),
                    "interpretation": "ç·šå½¢ç›¸é–¢ã®å¼·ã•"
                },
                "spearman": {
                    "correlation_coefficient": float(spearman_r),
                    "p_value": float(spearman_p),
                    "interpretation": "é †ä½ç›¸é–¢ã®å¼·ã•"
                },
                "kendall": {
                    "correlation_coefficient": float(kendall_tau),
                    "p_value": float(kendall_p),
                    "interpretation": "é †ä½ç›¸é–¢ã®å¼·ã•ï¼ˆå°æ¨™æœ¬ã«é©ã—ã¦ã„ã‚‹ï¼‰"
                }
            },
            "independence_test": {
                "chi_square_statistic": float(chi2),
                "p_value": float(chi2_p),
                "degrees_of_freedom": int((contingency_table.shape[0] - 1) * (contingency_table.shape[1] - 1)),
                "contingency_table": contingency_table.to_dict()
            },
            "conclusions": {
                "correlation_strength": "ä¸­ç¨‹åº¦ã®æ­£ã®ç›¸é–¢" if 0.3 <= abs(pearson_r) < 0.5 else 
                                       "å¼±ã„ç›¸é–¢" if 0.1 <= abs(pearson_r) < 0.3 else "ç„¡ç›¸é–¢",
                "independence": "ç‹¬ç«‹ã§ã¯ãªã„ï¼ˆç›¸é–¢ã‚ã‚Šï¼‰" if chi2_p < 0.05 else "ç‹¬ç«‹ï¼ˆç„¡ç›¸é–¢ï¼‰"
            }
        }
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªãƒ»ä½œæˆ
        output_dir = '../03_åˆ†æçµæœ'
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        import json
        output_path = os.path.join(output_dir, 'ç›¸é–¢åˆ†æçµæœ_æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        
    except Exception as e:
        print(f"çµæœã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œã—ã¾ã™...")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        print("ğŸ¯ æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã¨æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ç›¸é–¢åˆ†æ")
        print("=" * 60)
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        df = load_data()
        
        # åŸºæœ¬çµ±è¨ˆé‡
        basic_statistics(df)
        
        # ç›¸é–¢åˆ†æ
        pearson_r, pearson_p, spearman_r, spearman_p, kendall_tau, kendall_p = correlation_analysis(df)
        
        # ç„¡ç›¸é–¢åˆ†æ
        chi2, chi2_p, dof, contingency_table = independence_test(df)
        
        # è©³ç´°åˆ†æ
        detailed_analysis(df)
        
        # å¯è¦–åŒ–
        create_visualizations(df)
        
        # çµæœã®ä¿å­˜
        save_results(pearson_r, pearson_p, spearman_r, spearman_p, kendall_tau, kendall_p,
                    chi2, chi2_p, contingency_table)
        
        print("\nâœ… åˆ†æå®Œäº†ï¼")
        print("=" * 60)
        
        # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
        print("\nğŸ“Š ç›¸é–¢åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        print(f"ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°: {pearson_r:.4f} (p={pearson_p:.6f})")
        print(f"ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢ä¿‚æ•°: {spearman_r:.4f} (p={spearman_p:.6f})")
        print(f"ã‚±ãƒ³ãƒ‰ãƒ¼ãƒ«ç›¸é–¢ä¿‚æ•°: {kendall_tau:.4f} (p={kendall_p:.6f})")
        print(f"\nã‚«ã‚¤äºŒä¹—çµ±è¨ˆé‡: {chi2:.4f} (p={chi2_p:.6f})")
        print(f"çµè«–: æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã«ã¯çµ±è¨ˆçš„ã«æœ‰æ„ãªç›¸é–¢é–¢ä¿‚ãŒã‚ã‚‹")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        print("\nåˆ†æã‚’ä¸­æ–­ã—ã¾ã™...")
        sys.exit(1)

if __name__ == "__main__":
    main()
