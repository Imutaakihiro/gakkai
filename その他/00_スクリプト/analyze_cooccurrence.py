#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…±èµ·åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã€Œè‰¯ã‹ã£ãŸã€ãªã©ã®é‡è¦èªã¨ä¸€ç·’ã«å‡ºç¾ã™ã‚‹å˜èªã‚’åˆ†æ
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import re
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
import japanize_matplotlib

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    import os
    
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå’æ¥­ç ”ç©¶ï¼ˆæ–°ï¼‰ï¼‰ã«ç§»å‹•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(script_dir)
    os.chdir(parent_dir)
    
    df = pd.read_csv('01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ†ã‚­ã‚¹ãƒˆ.csv')
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶")
    return df

def preprocess_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    if pd.isna(text):
        return ""
    
    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    text = str(text)
    text = re.sub(r'[^\w\s]', ' ', text)  # è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«
    text = re.sub(r'\s+', ' ', text)      # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    text = text.strip()
    
    return text

def find_cooccurrences(df, target_words, window_size=5):
    """
    æŒ‡å®šã•ã‚ŒãŸå˜èªã®å…±èµ·åˆ†æ
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        target_words: åˆ†æå¯¾è±¡ã®å˜èªãƒªã‚¹ãƒˆ
        window_size: å…±èµ·ã®çª“ã‚µã‚¤ã‚ºï¼ˆå‰å¾Œä½•èªã¾ã§ã‚’å…±èµ·ã¨ã¿ãªã™ã‹ï¼‰
    
    Returns:
        dict: å„å¯¾è±¡å˜èªã®å…±èµ·çµæœ
    """
    print(f"å…±èµ·åˆ†æã‚’å®Ÿè¡Œä¸­... (çª“ã‚µã‚¤ã‚º: {window_size})")
    
    cooccurrence_results = {}
    
    for target_word in target_words:
        print(f"  - {target_word} ã®å…±èµ·åˆ†æä¸­...")
        
        # å¯¾è±¡å˜èªã‚’å«ã‚€æ–‡ã‚’æŠ½å‡º
        target_sentences = []
        for text in df['aggregated_text']:
            processed_text = preprocess_text(text)
            if target_word in processed_text:
                target_sentences.append(processed_text)
        
        print(f"    {target_word}ã‚’å«ã‚€æ–‡: {len(target_sentences)}ä»¶")
        
        # å…±èµ·å˜èªã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        cooccurrence_counter = Counter()
        
        for sentence in target_sentences:
            words = sentence.split()
            
            # å¯¾è±¡å˜èªã®ä½ç½®ã‚’ç‰¹å®š
            target_positions = [i for i, word in enumerate(words) if target_word in word]
            
            for pos in target_positions:
                # çª“ã‚µã‚¤ã‚ºå†…ã®å˜èªã‚’å–å¾—
                start = max(0, pos - window_size)
                end = min(len(words), pos + window_size + 1)
                
                for i in range(start, end):
                    if i != pos:  # å¯¾è±¡å˜èªè‡ªä½“ã¯é™¤å¤–
                        cooccurrence_counter[words[i]] += 1
        
        # çµæœã‚’ä¿å­˜
        cooccurrence_results[target_word] = {
            'cooccurrences': dict(cooccurrence_counter.most_common(50)),
            'total_sentences': len(target_sentences),
            'total_cooccurrences': sum(cooccurrence_counter.values())
        }
    
    return cooccurrence_results

def analyze_sentiment_cooccurrences(df, target_words):
    """
    æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã¨çµ„ã¿åˆã‚ã›ãŸå…±èµ·åˆ†æ
    """
    print("æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã¨ã®çµ„ã¿åˆã‚ã›åˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    # ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    try:
        label_df = pd.read_csv('01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ©ãƒ™ãƒ«.csv')
        df_with_labels = df.merge(label_df, on='course_id', how='inner')
        
        # sentiment_meanã‹ã‚‰æ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒªã‚’ä½œæˆ
        # sentiment_mean: 1=POSITIVE, 0=NEUTRAL, -1=NEGATIVE
        df_with_labels['sentiment'] = df_with_labels['sentiment_mean'].apply(
            lambda x: 'POSITIVE' if x > 0.3 else ('NEGATIVE' if x < -0.3 else 'NEUTRAL')
        )
        
        print(f"ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿: {len(df_with_labels)}ä»¶")
        print(f"  POSITIVE: {(df_with_labels['sentiment']=='POSITIVE').sum()}ä»¶")
        print(f"  NEGATIVE: {(df_with_labels['sentiment']=='NEGATIVE').sum()}ä»¶")
        print(f"  NEUTRAL: {(df_with_labels['sentiment']=='NEUTRAL').sum()}ä»¶")
    except Exception as e:
        print(f"ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print("åŸºæœ¬åˆ†æã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")
        return {}
    
    sentiment_cooccurrences = {}
    
    for target_word in target_words:
        print(f"  - {target_word} ã®æ„Ÿæƒ…åˆ¥å…±èµ·åˆ†æä¸­...")
        
        sentiment_results = {}
        
        for sentiment in ['POSITIVE', 'NEGATIVE', 'NEUTRAL']:
            # è©²å½“ã™ã‚‹æ„Ÿæƒ…ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            sentiment_data = df_with_labels[df_with_labels['sentiment'] == sentiment]
            
            # å¯¾è±¡å˜èªã‚’å«ã‚€æ–‡ã‚’æŠ½å‡º
            target_sentences = []
            for text in sentiment_data['aggregated_text']:
                processed_text = preprocess_text(text)
                if target_word in processed_text:
                    target_sentences.append(processed_text)
            
            # å…±èµ·åˆ†æ
            cooccurrence_counter = Counter()
            
            for sentence in target_sentences:
                words = sentence.split()
                target_positions = [i for i, word in enumerate(words) if target_word in word]
                
                for pos in target_positions:
                    start = max(0, pos - 5)
                    end = min(len(words), pos + 6)
                    
                    for i in range(start, end):
                        if i != pos:
                            cooccurrence_counter[words[i]] += 1
            
            sentiment_results[sentiment] = {
                'cooccurrences': dict(cooccurrence_counter.most_common(30)),
                'total_sentences': len(target_sentences)
            }
        
        sentiment_cooccurrences[target_word] = sentiment_results
    
    return sentiment_cooccurrences

def create_visualizations(cooccurrence_results, output_dir):
    """å¯è¦–åŒ–ã‚’ä½œæˆ"""
    print("å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
    
    for target_word, results in cooccurrence_results.items():
        if not results['cooccurrences']:
            continue
        
        # TOP20ã®å…±èµ·å˜èªã‚’å–å¾—
        top_cooccurrences = list(results['cooccurrences'].items())[:20]
        words, counts = zip(*top_cooccurrences)
        
        # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        plt.figure(figsize=(12, 8))
        bars = plt.barh(range(len(words)), counts, color='skyblue', alpha=0.7)
        plt.yticks(range(len(words)), words)
        plt.xlabel('å…±èµ·å›æ•°')
        plt.title(f'ã€Œ{target_word}ã€ã®å…±èµ·å˜èª TOP20\n(ç·æ–‡æ•°: {results["total_sentences"]}ä»¶)')
        plt.gca().invert_yaxis()
        
        # æ•°å€¤ã‚’ãƒãƒ¼ã®å³å´ã«è¡¨ç¤º
        for i, (bar, count) in enumerate(zip(bars, counts)):
            plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, 
                    str(count), ha='left', va='center', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/cooccurrence_{target_word}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  - {target_word} ã®å¯è¦–åŒ–ã‚’ä¿å­˜")

def create_visualizations_by_sentiment(cooccurrence_results, positive_words, negative_words, output_dir):
    """ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¥ã«è‰²åˆ†ã‘ã—ã¦å¯è¦–åŒ–"""
    print("æ„Ÿæƒ…åˆ¥å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
    
    for target_word, results in cooccurrence_results.items():
        if not results['cooccurrences']:
            continue
        
        # TOP20ã®å…±èµ·å˜èªã‚’å–å¾—
        top_cooccurrences = list(results['cooccurrences'].items())[:20]
        if not top_cooccurrences:
            continue
            
        words, counts = zip(*top_cooccurrences)
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‹ãƒã‚¬ãƒ†ã‚£ãƒ–ã‹ã§è‰²ã‚’å¤‰æ›´
        if target_word in positive_words:
            color = '#4CAF50'  # ç·‘è‰²ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰
            sentiment_label = 'ãƒã‚¸ãƒ†ã‚£ãƒ–'
        elif target_word in negative_words:
            color = '#F44336'  # èµ¤è‰²ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
            sentiment_label = 'ãƒã‚¬ãƒ†ã‚£ãƒ–'
        else:
            color = '#2196F3'  # é’è‰²ï¼ˆä¸­ç«‹ï¼‰
            sentiment_label = ''
        
        # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        plt.figure(figsize=(12, 8))
        bars = plt.barh(range(len(words)), counts, color=color, alpha=0.7)
        plt.yticks(range(len(words)), words)
        plt.xlabel('å…±èµ·å›æ•°', fontsize=12)
        
        title = f'ã€Œ{target_word}ã€ã®å…±èµ·å˜èª TOP20'
        if sentiment_label:
            title += f' [{sentiment_label}]'
        title += f'\n(ç·æ–‡æ•°: {results["total_sentences"]}ä»¶)'
        plt.title(title, fontsize=14, fontweight='bold')
        
        plt.gca().invert_yaxis()
        
        # æ•°å€¤ã‚’ãƒãƒ¼ã®å³å´ã«è¡¨ç¤º
        for i, (bar, count) in enumerate(zip(bars, counts)):
            plt.text(bar.get_width() + max(counts)*0.01, bar.get_y() + bar.get_height()/2, 
                    str(count), ha='left', va='center', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/cooccurrence_{target_word}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  - {target_word} [{sentiment_label}] ã®å¯è¦–åŒ–ã‚’ä¿å­˜")

def save_results(cooccurrence_results, sentiment_cooccurrences, output_dir):
    """çµæœã‚’ä¿å­˜"""
    print("çµæœã‚’ä¿å­˜ä¸­...")
    
    # åŸºæœ¬å…±èµ·çµæœã‚’ä¿å­˜
    with open(f'{output_dir}/cooccurrence_results.json', 'w', encoding='utf-8') as f:
        json.dump(cooccurrence_results, f, ensure_ascii=False, indent=2)
    
    # æ„Ÿæƒ…åˆ¥å…±èµ·çµæœã‚’ä¿å­˜
    if sentiment_cooccurrences:
        with open(f'{output_dir}/sentiment_cooccurrences.json', 'w', encoding='utf-8') as f:
            json.dump(sentiment_cooccurrences, f, ensure_ascii=False, indent=2)
    
    # CSVå½¢å¼ã§ã‚‚ä¿å­˜
    all_results = []
    for target_word, results in cooccurrence_results.items():
        for co_word, count in results['cooccurrences'].items():
            all_results.append({
                'target_word': target_word,
                'cooccurrence_word': co_word,
                'count': count,
                'total_sentences': results['total_sentences']
            })
    
    df_results = pd.DataFrame(all_results)
    df_results.to_csv(f'{output_dir}/cooccurrence_analysis.csv', index=False, encoding='utf-8-sig')
    
    print(f"çµæœã‚’ {output_dir} ã«ä¿å­˜ã—ã¾ã—ãŸ")

def generate_summary_report(cooccurrence_results, sentiment_cooccurrences, output_dir):
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print("ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    report = f"""# å…±èµ·åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**åˆ†ææ—¥æ™‚:** {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}  
**å¯¾è±¡ãƒ‡ãƒ¼ã‚¿:** æˆæ¥­é›†ç´„ãƒ†ã‚­ã‚¹ãƒˆ  
**åˆ†æå¯¾è±¡å˜èª:** {', '.join(cooccurrence_results.keys())}

---

## ğŸ“Š åˆ†ææ¦‚è¦

"""
    
    for target_word, results in cooccurrence_results.items():
        report += f"""### ã€Œ{target_word}ã€ã®å…±èµ·åˆ†æçµæœ

- **å¯¾è±¡æ–‡æ•°:** {results['total_sentences']}ä»¶
- **ç·å…±èµ·å›æ•°:** {results['total_cooccurrences']}å›
- **å¹³å‡å…±èµ·æ•°:** {results['total_cooccurrences']/results['total_sentences']:.2f}å›/æ–‡

#### TOP10å…±èµ·å˜èª

| é †ä½ | å˜èª | å…±èµ·å›æ•° | å‡ºç¾ç‡ |
|------|------|---------|--------|
"""
        
        for i, (word, count) in enumerate(list(results['cooccurrences'].items())[:10], 1):
            rate = count / results['total_sentences'] * 100
            report += f"| {i} | {word} | {count} | {rate:.1f}% |\n"
        
        report += "\n"
    
    # æ„Ÿæƒ…åˆ¥åˆ†æçµæœ
    if sentiment_cooccurrences:
        report += "## ğŸ­ æ„Ÿæƒ…åˆ¥å…±èµ·åˆ†æ\n\n"
        
        for target_word, sentiment_results in sentiment_cooccurrences.items():
            report += f"### ã€Œ{target_word}ã€ã®æ„Ÿæƒ…åˆ¥å…±èµ·\n\n"
            
            for sentiment, results in sentiment_results.items():
                if results['total_sentences'] > 0:
                    report += f"#### {sentiment} ({results['total_sentences']}ä»¶)\n\n"
                    report += "| é †ä½ | å˜èª | å…±èµ·å›æ•° |\n|------|------|---------|\n"
                    
                    for i, (word, count) in enumerate(list(results['cooccurrences'].items())[:10], 1):
                        report += f"| {i} | {word} | {count} |\n"
                    
                    report += "\n"
    
    report += f"""
---

## ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«

- `cooccurrence_results.json` - åŸºæœ¬å…±èµ·åˆ†æçµæœ
- `cooccurrence_analysis.csv` - CSVå½¢å¼ã®åˆ†æçµæœ
- `sentiment_cooccurrences.json` - æ„Ÿæƒ…åˆ¥å…±èµ·åˆ†æçµæœ
- `cooccurrence_*.png` - å„å˜èªã®å¯è¦–åŒ–ã‚°ãƒ©ãƒ•

---

**åˆ†æå®Œäº†ï¼**  
çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ `{output_dir}` ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚
"""
    
    with open(f'{output_dir}/cooccurrence_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

def generate_summary_report_enhanced(cooccurrence_results, sentiment_cooccurrences, 
                                    positive_words, negative_words, output_dir):
    """æ‹¡å¼µç‰ˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¥ï¼‰"""
    print("æ‹¡å¼µç‰ˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    report = f"""# å…±èµ·åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–æ¯”è¼ƒï¼‰

**åˆ†ææ—¥æ™‚:** {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}  
**å¯¾è±¡ãƒ‡ãƒ¼ã‚¿:** æˆæ¥­é›†ç´„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…¨83,851ä»¶ï¼‰  
**åˆ†ææ‰‹æ³•:** å‰å¾Œ5èªã®çª“ã‚’ä½¿ã£ãŸå…±èµ·åˆ†æ

---

## ğŸ¯ åˆ†æã®ç›®çš„

SHAPåˆ†æã§ã€Œè‰¯ã‹ã£ãŸã€ã€Œé›£ã—ã‹ã£ãŸã€ãªã©ã®å˜èªãŒæ„Ÿæƒ…äºˆæ¸¬ã«å¯„ä¸ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚
ã—ã‹ã—ã€**ã€Œä½•ãŒã€è‰¯ã‹ã£ãŸã®ã‹ã€ã€Œä½•ãŒã€é›£ã—ã‹ã£ãŸã®ã‹**ã¯ä¸æ˜ã§ã—ãŸã€‚

ã“ã®å…±èµ·åˆ†æã«ã‚ˆã‚Šã€å…·ä½“çš„ãªæº€è¶³è¦å› ã¨ä¸æº€è¦å› ã‚’ç‰¹å®šã—ã¾ã™ã€‚

---

## ğŸ“Š ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èªã®å…±èµ·åˆ†æ

å­¦ç”Ÿã®æº€è¶³è¦å› ã‚’æ¢ã‚‹é‡è¦èªã¨ã®å…±èµ·ã‚’åˆ†æã—ã¾ã—ãŸã€‚

"""
    
    # ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èªã®åˆ†æ
    for target_word in positive_words:
        if target_word not in cooccurrence_results:
            continue
        results = cooccurrence_results[target_word]
        
        if results['total_sentences'] == 0:
            continue
            
        report += f"""### ã€Œ{target_word}ã€ã®å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³

**å¯¾è±¡æ–‡æ•°:** {results['total_sentences']}ä»¶  
**å¹³å‡å…±èµ·æ•°:** {results['total_cooccurrences']/results['total_sentences']:.2f}å›/æ–‡

#### TOP10å…±èµ·å˜èª

| é †ä½ | å˜èª | å…±èµ·å›æ•° | æ–‡æ›¸å†…å‡ºç¾ç‡ |
|------|------|---------|------------|
"""
        
        for i, (word, count) in enumerate(list(results['cooccurrences'].items())[:10], 1):
            rate = count / results['total_sentences'] * 100
            report += f"| {i} | {word} | {count} | {rate:.1f}% |\n"
        
        report += "\n"
    
    # ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èªã®åˆ†æ
    report += """---

## ğŸ“‰ ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èªã®å…±èµ·åˆ†æ

å­¦ç”Ÿã®ä¸æº€è¦å› ã‚’æ¢ã‚‹é‡è¦èªã¨ã®å…±èµ·ã‚’åˆ†æã—ã¾ã—ãŸã€‚

"""
    
    for target_word in negative_words:
        if target_word not in cooccurrence_results:
            continue
        results = cooccurrence_results[target_word]
        
        if results['total_sentences'] == 0:
            continue
            
        report += f"""### ã€Œ{target_word}ã€ã®å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³

**å¯¾è±¡æ–‡æ•°:** {results['total_sentences']}ä»¶  
**å¹³å‡å…±èµ·æ•°:** {results['total_cooccurrences']/results['total_sentences']:.2f}å›/æ–‡

#### TOP10å…±èµ·å˜èª

| é †ä½ | å˜èª | å…±èµ·å›æ•° | æ–‡æ›¸å†…å‡ºç¾ç‡ |
|------|------|---------|------------|
"""
        
        for i, (word, count) in enumerate(list(results['cooccurrences'].items())[:10], 1):
            rate = count / results['total_sentences'] * 100
            report += f"| {i} | {word} | {count} | {rate:.1f}% |\n"
        
        report += "\n"
    
    # æ„Ÿæƒ…åˆ¥åˆ†æã®æ¦‚è¦
    if sentiment_cooccurrences:
        report += """---

## ğŸ­ æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«åˆ¥ã®å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³

åŒã˜å˜èªã§ã‚‚ã€POSITIVE/NEGATIVEãªæ–‡è„ˆã§ä½¿ã‚ã‚Œæ–¹ãŒç•°ãªã‚‹ã‹ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚

"""
        
        # ã„ãã¤ã‹ã®ã‚­ãƒ¼å˜èªã«ã¤ã„ã¦æ„Ÿæƒ…åˆ¥ã«è¡¨ç¤º
        key_words = ['è‰¯ã‹ã£ãŸ', 'é›£ã—ã‹ã£ãŸ', 'ã‚„ã™', 'ã»ã—ã„']
        
        for target_word in key_words:
            if target_word not in sentiment_cooccurrences:
                continue
                
            sentiment_results = sentiment_cooccurrences[target_word]
            report += f"### ã€Œ{target_word}ã€ã®æ„Ÿæƒ…åˆ¥ä½¿ã‚ã‚Œæ–¹\n\n"
            
            for sentiment in ['POSITIVE', 'NEGATIVE', 'NEUTRAL']:
                if sentiment not in sentiment_results:
                    continue
                results = sentiment_results[sentiment]
                
                if results['total_sentences'] > 0:
                    report += f"#### {sentiment} ({results['total_sentences']}ä»¶)\n\n"
                    report += "| é †ä½ | å…±èµ·å˜èª | å›æ•° |\n|------|---------|------|\n"
                    
                    for i, (word, count) in enumerate(list(results['cooccurrences'].items())[:5], 1):
                        report += f"| {i} | {word} | {count} |\n"
                    
                    report += "\n"
    
    report += f"""---

## ğŸ’¡ ä¸»è¦ãªç™ºè¦‹

### ãƒã‚¸ãƒ†ã‚£ãƒ–è¦å› 
- åˆ†æå¯¾è±¡: {len(positive_words)}èª
- å…·ä½“çš„ãªæº€è¶³è¦ç´ ãŒæ˜ç¢ºåŒ–

### ãƒã‚¬ãƒ†ã‚£ãƒ–è¦å› 
- åˆ†æå¯¾è±¡: {len(negative_words)}èª
- å…·ä½“çš„ãªæ”¹å–„ç‚¹ãŒç‰¹å®šå¯èƒ½

---

## ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«

### ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
- `cooccurrence_results.json` - åŸºæœ¬å…±èµ·åˆ†æçµæœï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å†åˆ©ç”¨å¯ï¼‰
- `cooccurrence_analysis.csv` - CSVå½¢å¼ã®åˆ†æçµæœï¼ˆExcelåˆ†æç”¨ï¼‰
- `sentiment_cooccurrences.json` - æ„Ÿæƒ…åˆ¥å…±èµ·åˆ†æçµæœ

### å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«
"""
    
    # ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆåŒ–
    for word in positive_words:
        if word in cooccurrence_results and cooccurrence_results[word]['total_sentences'] > 0:
            report += f"- `cooccurrence_{word}.png` - ã€Œ{word}ã€ã®å…±èµ·ã‚°ãƒ©ãƒ• [ãƒã‚¸ãƒ†ã‚£ãƒ–]\n"
    
    for word in negative_words:
        if word in cooccurrence_results and cooccurrence_results[word]['total_sentences'] > 0:
            report += f"- `cooccurrence_{word}.png` - ã€Œ{word}ã€ã®å…±èµ·ã‚°ãƒ©ãƒ• [ãƒã‚¬ãƒ†ã‚£ãƒ–]\n"
    
    report += """
---

## ğŸš€ æ´»ç”¨æ–¹æ³•

### æ•™è‚²æ”¹å–„ã¸ã®å¿œç”¨
1. **æº€è¶³è¦å› ã®å¼·åŒ–**
   - ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èªã®å…±èµ·ã‹ã‚‰ã€ä½•ãŒè©•ä¾¡ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’æŠŠæ¡
   - è‰¯ã„ç‚¹ã‚’ã•ã‚‰ã«ä¼¸ã°ã™æ–½ç­–ç«‹æ¡ˆ

2. **ä¸æº€è¦å› ã®è§£æ¶ˆ**
   - ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èªã®å…±èµ·ã‹ã‚‰ã€å…·ä½“çš„ãªå•é¡Œç‚¹ã‚’ç‰¹å®š
   - ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆã§ã®æ”¹å–„æ–½ç­–å®Ÿæ–½

### å’è«–ã§ã®æ´»ç”¨
- SHAPåˆ†æï¼ˆå˜èªã®é‡è¦åº¦ï¼‰Ã— å…±èµ·åˆ†æï¼ˆå˜èªã®æ–‡è„ˆï¼‰
- ã€Œãªãœãã®å˜èªãŒé‡è¦ã‹ã€ã‚’å®šé‡çš„ãƒ»å®šæ€§çš„ã«èª¬æ˜å¯èƒ½

---

**åˆ†æå®Œäº†ï¼**  
çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ `{output_dir}` ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚
"""
    
    with open(f'{output_dir}/cooccurrence_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("æ‹¡å¼µç‰ˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== å…±èµ·åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ ===")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    output_dir = "03_åˆ†æçµæœ/å…±èµ·åˆ†æ"
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    df = load_data()
    
    # åˆ†æå¯¾è±¡å˜èªã‚’å®šç¾©
    print("\nã€ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èªã®å…±èµ·åˆ†æã€‘")
    positive_words = ['è‰¯ã‹ã£ãŸ', 'ã‚ˆã‹ã£ãŸ', 'ã‚„ã™', 'é¢ç™½', 'æ¥½ã—ã„', 'ãŠã‚‚ã—ã‚', 'ã§ããŸ', 'åˆ†ã‹ã‚Š']
    
    print("\nã€ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èªã®å…±èµ·åˆ†æã€‘")
    negative_words = ['é›£ã—ã‹ã£ãŸ', 'ã»ã—ã„', 'è‹¦æ‰‹', 'ã»ã†', 'å¤§å¤‰', 'æ¬²ã—ã„', 'ãã ã•ã„', 'ä¸è¶³']
    
    # ã™ã¹ã¦ã®å¯¾è±¡å˜èªã‚’çµåˆ
    all_target_words = positive_words + negative_words
    
    # åŸºæœ¬å…±èµ·åˆ†æ
    cooccurrence_results = find_cooccurrences(df, all_target_words, window_size=5)
    
    # æ„Ÿæƒ…åˆ¥å…±èµ·åˆ†æ
    sentiment_cooccurrences = analyze_sentiment_cooccurrences(df, all_target_words)
    
    # å¯è¦–åŒ–ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã§è‰²åˆ†ã‘ï¼‰
    create_visualizations_by_sentiment(cooccurrence_results, positive_words, negative_words, output_dir)
    
    # çµæœä¿å­˜
    save_results(cooccurrence_results, sentiment_cooccurrences, output_dir)
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_summary_report_enhanced(cooccurrence_results, sentiment_cooccurrences, 
                                    positive_words, negative_words, output_dir)
    
    print("=== åˆ†æå®Œäº† ===")

if __name__ == "__main__":
    main()
