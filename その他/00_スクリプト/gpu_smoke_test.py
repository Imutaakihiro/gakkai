#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUå‹•ä½œç¢ºèªç”¨ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

ç¢ºèªå†…å®¹:
- PyTorch/CUDAã®æœ‰åŠ¹æ€§
- GPUåãƒ»Compute Capability
- ç°¡å˜ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ãƒ»MatMulã‚’GPUã§å®Ÿè¡Œ
"""

import time
import torch


def main() -> None:
    print("=" * 60)
    print("ğŸ” GPU ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"CUDA (built) version: {torch.version.cuda}")

    if not torch.cuda.is_available():
        print("âŒ CUDA is not available. Running on CPU only.")
        return

    device = torch.device("cuda")
    print(f"GPU count: {torch.cuda.device_count()}")
    print(f"GPU name: {torch.cuda.get_device_name(0)}")
    print(f"Compute capability: {torch.cuda.get_device_capability(0)}")

    # ç°¡å˜ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
    torch.cuda.synchronize()
    start = time.time()
    a = torch.randn(4096, 4096, device=device)
    b = torch.randn(4096, 4096, device=device)
    c = a @ b  # è¡Œåˆ—ç©ï¼ˆGPUã‚«ãƒ¼ãƒãƒ«ãŒå‘¼ã°ã‚Œã‚‹ï¼‰
    torch.cuda.synchronize()
    elapsed = time.time() - start
    print(f"âœ… MatMul OK. Shape: {tuple(c.shape)} | Time: {elapsed:.3f}s")

    # é€†ä¼æ’­ã®ä¸€å›å®Ÿè¡Œ
    x = torch.randn(1024, 1024, device=device, requires_grad=True)
    y = (x * x).sum()
    y.backward()
    print("âœ… Backward OK (simple gradient)")

    print("\nğŸ‰ GPUã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Œäº†ã€‚å­¦ç¿’ã®æº–å‚™ãŒã§ãã¦ã„ã¾ã™ã€‚")


if __name__ == "__main__":
    main()


