#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆæ¥­å˜ä½ Ã— é †åºå›å¸°ï¼ˆ1â€“4ï¼‰Ã— LLP å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæœ€å°å®Ÿè£…ï¼‰
- å…¥åŠ›: æˆæ¥­ã”ã¨ã®è‡ªç”±è¨˜è¿°ï¼ˆè¤‡æ•°ä»¶ã§ã‚‚å¯ï¼‰
- æ•™å¸«: æˆæ¥­ã”ã¨ã®åˆ†å¸ƒ q = [q1..q4]ï¼ˆã¾ãŸã¯ count_1..4 â†’ æ¯”ç‡åŒ–ï¼‰ã€å›ç­”è€…æ•° respondents
- å‡ºåŠ›: ç´¯ç© 3 ãƒ­ã‚¸ãƒƒãƒˆ â†’ P(1..4) å†æ§‹æˆ
- æå¤±: æˆæ¥­å†…å¹³å‡ã®äºˆæ¸¬åˆ†å¸ƒ pÌ„ ã‚’ç”¨ã„ãŸ respondents åŠ é‡ KL(q || pÌ„)
"""

import os
import glob
import json
from datetime import datetime
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, BertJapaneseTokenizer

# ------------------------- åŸºæœ¬è¨­å®š -------------------------
BASE_MODEL = "koheiduck/bert-japanese-finetuned-sentiment"
MAX_LENGTH = 192
BATCH_SIZE = 2
NUM_EPOCHS = 3
LEARNING_RATE = 2e-5
USE_AMP = True
CHUNK_LEN = 192
STRIDE = 150
MAX_CHUNKS = 4
NUM_WORKERS = 0  # Windows å®‰å…¨è¨­å®š

# ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æå¤±ã®ä¿‚æ•°ï¼ˆLLP + å›å¸°1æœ¬ï¼‰
ALPHA_SENT = 0.3   # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ã®é‡ã¿ï¼ˆè£œåŠ©æƒ…å ±ãªã®ã§å°ã•ã‚ï¼‰
# æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯æœŸå¾…å€¤ã‹ã‚‰è¨ˆç®—ã™ã‚‹ãŸã‚ã€å›å¸°ãƒ˜ãƒƒãƒ‰ã¨æå¤±ã¯ä¸è¦

# ------------------------- ãƒ‡ãƒã‚¤ã‚¹é¸æŠ -------------------------

def get_device() -> torch.device:
    if torch.cuda.is_available():
        try:
            _ = torch.tensor([1.0]).cuda()
            print("âœ… CUDA åˆ©ç”¨")
            return torch.device("cuda")
        except Exception:
            pass
    try:
        import torch_directml as dml
        if dml.is_available():
            print("âœ… DirectML åˆ©ç”¨")
            return dml.device()
    except Exception:
        pass
    print("ğŸ”„ CPU åˆ©ç”¨")
    return torch.device("cpu")


device = get_device()
IS_CUDA = torch.cuda.is_available()
AMP_ENABLED = USE_AMP and IS_CUDA

# ------------------------- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ -------------------------

AGG_DIR_CANDIDATES = [
    "01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿",
    "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿",
    "../../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿",
]


def find_latest_agg_csv() -> str:
    # ã¾ãšå„ªå…ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå›ç­”åˆ†å¸ƒä»˜ãï¼‰ã‚’æ¢ã™
    preferred = [
        "01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv",
        "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv",
        "../../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv",
    ]
    for p in preferred:
        if os.path.exists(p):
            print(f"ğŸ“ ä½¿ç”¨CSV(å„ªå…ˆ): {p}")
            return p

    paths = []
    for base in AGG_DIR_CANDIDATES:
        paths.extend(glob.glob(os.path.join(base, "æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_*.csv")))
    if not paths:
        # åºƒåŸŸæ¢ç´¢
        for root, _, files in os.walk("."):
            for f in files:
                if f.startswith("æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_") and f.endswith(".csv"):
                    paths.append(os.path.join(root, f))
    if not paths:
        raise FileNotFoundError("æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_*.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    latest = max(paths, key=os.path.getctime)
    print(f"ğŸ“ ä½¿ç”¨CSV: {latest}")
    return latest


# åˆ—åå€™è£œ
COURSE_ID_CANDS = ["course_id", "æˆæ¥­ID", "ç§‘ç›®ID", "è¬›ç¾©ID", "courseId"]
TEXT_CANDS = ["è‡ªç”±è¨˜è¿°ã¾ã¨ã‚", "text", "è‡ªç”±è¨˜è¿°", "comments"]
COUNT_PREFIXES = [["count_1", "count_2", "count_3", "count_4"],
                  ["n1", "n2", "n3", "n4"],
                  ["äººæ•°_1", "äººæ•°_2", "äººæ•°_3", "äººæ•°_4"],
                  ["åˆ†å¸ƒ_ååˆ†æ„ç¾©ã‚ã‚Š_äººæ•°", "åˆ†å¸ƒ_ã‚ã‚‹ç¨‹åº¦æ„ç¾©ã‚ã‚Š_äººæ•°", "åˆ†å¸ƒ_ã‚ã¾ã‚Šæ„ç¾©ãªã—_äººæ•°", "åˆ†å¸ƒ_å…¨ãæ„ç¾©ãªã—_äººæ•°"]]
RATIO_PREFIXES = [["ratio_1", "ratio_2", "ratio_3", "ratio_4"],
                  ["r1", "r2", "r3", "r4"],
                  ["å‰²åˆ_1", "å‰²åˆ_2", "å‰²åˆ_3", "å‰²åˆ_4"],
                  ["åˆ†å¸ƒ_ååˆ†æ„ç¾©ã‚ã‚Š_å‰²åˆ(%)", "åˆ†å¸ƒ_ã‚ã‚‹ç¨‹åº¦æ„ç¾©ã‚ã‚Š_å‰²åˆ(%)", "åˆ†å¸ƒ_ã‚ã¾ã‚Šæ„ç¾©ãªã—_å‰²åˆ(%)", "åˆ†å¸ƒ_å…¨ãæ„ç¾©ãªã—_å‰²åˆ(%)"]]
RESPONDENTS_CANDS = ["respondents", "å›ç­”è€…æ•°", "n_respondents", "äººæ•°åˆè¨ˆ"]

# å›å¸°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã®å€™è£œ
SENT_MEAN_CANDS = ["æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡", "sentiment_mean"]
COURSE_SCORE_CANDS = ["æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢", "course_score"]


def pick_first_exist(df: pd.DataFrame, candidates: List[str]) -> str:
    for c in candidates:
        if c in df.columns:
            return c
    return ""


def pick_block(df: pd.DataFrame, blocks: List[List[str]]) -> List[str]:
    for block in blocks:
        if all(c in df.columns for c in block):
            return block
    return []


def prepare_targets(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    # å„ªå…ˆ: ratio_* â†’ 0-1ã«æ­£è¦åŒ–ã€æ¬¡: count_* â†’ åˆè¨ˆã§å‰²ã‚‹
    ratio_cols = pick_block(df, RATIO_PREFIXES)
    count_cols = pick_block(df, COUNT_PREFIXES)
    if ratio_cols:
        q = df[ratio_cols].values.astype(np.float32)
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå½¢å¼ãªã‚‰0-1ã¸
        if np.nanmax(q) > 1.5:
            q = q / 100.0
        q = np.clip(q, 1e-8, 1.0)
        q = q / q.sum(axis=1, keepdims=True)
        respondents_col = pick_first_exist(df, RESPONDENTS_CANDS)
        if respondents_col:
            w = df[respondents_col].fillna(1).astype(np.float32).values
        else:
            w = np.ones((len(df),), dtype=np.float32)
    elif count_cols:
        counts = df[count_cols].values.astype(np.float32)
        total = counts.sum(axis=1, keepdims=True).clip(min=1.0)
        q = counts / total
        w = counts.sum(axis=1).astype(np.float32)
    else:
        raise ValueError("ratio_1..4 / count_1..4 / åˆ†å¸ƒ_* åˆ—ãŒå¿…è¦ã§ã™")

    sent_col = pick_first_exist(df, SENT_MEAN_CANDS)
    course_col = pick_first_exist(df, COURSE_SCORE_CANDS)
    if not sent_col or not course_col:
        raise ValueError("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ / æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ ã®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    y_sent = df[sent_col].astype(np.float32).values
    y_course = df[course_col].astype(np.float32).values
    return q, w, y_sent, y_course


def load_agg_dataframe() -> pd.DataFrame:
    path = find_latest_agg_csv()
    df = pd.read_csv(path)
    print(f"è¡Œæ•°: {len(df)}, åˆ—: {len(df.columns)}")
    return df

# ------------------------- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ -------------------------

class CourseLLPDataset(Dataset):
    """æˆæ¥­å˜ä½LLPç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒ»course_idã§é›†ç´„å¯ï¼‰"""

    def __init__(self, df: pd.DataFrame, tokenizer: BertJapaneseTokenizer,
                 chunk_len: int = CHUNK_LEN, stride: int = STRIDE, max_chunks: int = MAX_CHUNKS):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.chunk_len = chunk_len
        self.stride = stride
        self.max_chunks = max_chunks

        self.course_col = pick_first_exist(df, COURSE_ID_CANDS)
        self.text_col = pick_first_exist(df, TEXT_CANDS)
        if not self.text_col:
            raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å€™è£œ: {TEXT_CANDS}")

        self.q, self.w, self.y_sent, self.y_course = prepare_targets(df)
        self.cls_id = tokenizer.cls_token_id
        self.sep_id = tokenizer.sep_token_id

    def __len__(self):
        return len(self.df)

    def _chunk(self, text: str) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        ids = self.tokenizer.encode(str(text), add_special_tokens=False)
        inner_max = self.chunk_len - 2
        chunks: List[Tuple[List[int], List[int]]] = []
        if len(ids) == 0:
            tokens = [self.cls_id, self.sep_id]
            pad = self.chunk_len - 2
            input_ids = tokens + [0] * pad
            attn = [1, 1] + [0] * pad
            return [torch.tensor(input_ids, dtype=torch.long)], [torch.tensor(attn, dtype=torch.long)]
        for start in range(0, len(ids), self.stride):
            inner = ids[start:start + inner_max]
            toks = [self.cls_id] + inner + [self.sep_id]
            if len(toks) < self.chunk_len:
                pad = self.chunk_len - len(toks)
                input_ids = toks + [0] * pad
                attn = [1] * len(toks) + [0] * pad
            else:
                input_ids = toks[:self.chunk_len]
                attn = [1] * self.chunk_len
            chunks.append((input_ids, attn))
            if len(chunks) >= self.max_chunks:
                break
        input_ids_list = [torch.tensor(x[0], dtype=torch.long) for x in chunks]
        attention_list = [torch.tensor(x[1], dtype=torch.long) for x in chunks]
        return input_ids_list, attention_list

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = row[self.text_col]
        input_ids_list, attn_list = self._chunk(text)
        course_id = row[self.course_col] if self.course_col else idx
        q = torch.tensor(self.q[idx], dtype=torch.float)
        w = torch.tensor(self.w[idx], dtype=torch.float)
        y_sent = torch.tensor(self.y_sent[idx], dtype=torch.float)
        y_course = torch.tensor(self.y_course[idx], dtype=torch.float)
        return {
            "input_ids_list": input_ids_list,
            "attention_mask_list": attn_list,
            "num_chunks": len(input_ids_list),
            "course_id": course_id,
            "q": q,    # [4]
            "w": w,    # scalar
            "y_sent": y_sent,
            "y_course": y_course
        }


def collate_batch(batch):
    B = len(batch)
    C = MAX_CHUNKS
    L = CHUNK_LEN
    input_ids = torch.zeros((B, C, L), dtype=torch.long)
    attention_mask = torch.zeros((B, C, L), dtype=torch.long)
    chunk_mask = torch.zeros((B, C), dtype=torch.bool)
    q = torch.zeros((B, 4), dtype=torch.float)
    w = torch.zeros((B,), dtype=torch.float)
    y_sent = torch.zeros((B,), dtype=torch.float)
    y_course = torch.zeros((B,), dtype=torch.float)
    course_ids = []

    for i, item in enumerate(batch):
        n = min(item["num_chunks"], C)
        for j in range(n):
            input_ids[i, j] = item["input_ids_list"][j]
            attention_mask[i, j] = item["attention_mask_list"][j]
            chunk_mask[i, j] = True
        q[i] = item["q"]
        w[i] = item["w"]
        y_sent[i] = item["y_sent"]
        y_course[i] = item["y_course"]
        course_ids.append(item["course_id"])

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "chunk_mask": chunk_mask,
        "q": q,
        "w": w,
        "y_sent": y_sent,
        "y_course": y_course,
        "course_ids": course_ids,
    }

# ------------------------- ãƒ¢ãƒ‡ãƒ« -------------------------

class OrdinalHead(nn.Module):
    def __init__(self, hidden_size: int):
        super().__init__()
        self.fc = nn.Linear(hidden_size, 3)  # y>=2, y>=3, y>=4

    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        logits = self.fc(features)          # [B,3]
        probs_ge = torch.sigmoid(logits)    # [B,3]
        return logits, probs_ge


class CourseOrdinalLLPModel(nn.Module):
    def __init__(self, base_model: str, dropout: float = 0.1):
        super().__init__()
        # transformersã®äº’æ›æ€§å•é¡Œã‚’å›é¿
        try:
            # ã¾ãšuse_safetensors=Falseã§è©¦ã™ï¼ˆå¤ã„transformerså¯¾å¿œï¼‰
            self.bert = BertModel.from_pretrained(base_model, use_safetensors=False)
        except Exception as e1:
            try:
                # use_safetensors=Trueã§è©¦ã™
                self.bert = BertModel.from_pretrained(base_model, use_safetensors=True)
            except Exception as e2:
                # æœ€å¾Œã®æ‰‹æ®µï¼štrust_remote_codeã‚’è¿½åŠ 
                try:
                    self.bert = BertModel.from_pretrained(base_model, trust_remote_code=True)
                except Exception as e3:
                    raise RuntimeError(f"BertModelèª­ã¿è¾¼ã¿å¤±æ•—: {e1}, {e2}, {e3}")
        hidden = self.bert.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.head = OrdinalHead(hidden)
        # å›å¸°ãƒ˜ãƒƒãƒ‰ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ã®ã¿ã€è£œåŠ©æƒ…å ±ï¼‰
        self.sent_head = nn.Sequential(
            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1)
        )
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯æœŸå¾…å€¤ã‹ã‚‰è¨ˆç®—ã™ã‚‹ãŸã‚ã€å›å¸°ãƒ˜ãƒƒãƒ‰ã¯ä¸è¦

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, chunk_mask: torch.Tensor):
        # input: [B,C,L]
        if input_ids.dim() == 3:
            B, C, L = input_ids.shape
            x_ids = input_ids.view(B*C, L)
            x_mask = attention_mask.view(B*C, L)
            out = self.bert(input_ids=x_ids, attention_mask=x_mask)
            cls = out.last_hidden_state[:, 0, :].view(B, C, -1)  # [B,C,H]
            mask = chunk_mask.float().unsqueeze(-1)
            summed = (cls * mask).sum(dim=1)
            denom = mask.sum(dim=1).clamp_min(1e-6)
            pooled = summed / denom
        else:
            out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            pooled = out.last_hidden_state[:, 0, :]
        pooled = self.dropout(pooled)
        logits, p_ge = self.head(pooled)  # p_ge: [p_ge2, p_ge3, p_ge4]
        # å¾©å…ƒ: P1..P4
        p1 = 1.0 - p_ge[:, 0]
        p2 = (p_ge[:, 0] - p_ge[:, 1]).clamp(min=0.0)
        p3 = (p_ge[:, 1] - p_ge[:, 2]).clamp(min=0.0)
        p4 = p_ge[:, 2]
        P = torch.stack([p1, p2, p3, p4], dim=1)  # [B,4]
        # æ•°å€¤å®‰å®šåŒ–ãƒ»æ­£è¦åŒ–
        P = P.clamp(min=1e-8)
        P = P / P.sum(dim=1, keepdim=True)
        # å›å¸°å‡ºåŠ›ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼‰
        y_sent_pred = self.sent_head(pooled).squeeze(-1)
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯æœŸå¾…å€¤ã‹ã‚‰è¨ˆç®—ï¼ˆå›å¸°ãƒ˜ãƒƒãƒ‰ä¸è¦ï¼‰
        # E[y] = 1Ã—P1 + 2Ã—P2 + 3Ã—P3 + 4Ã—P4
        expected_values = torch.tensor([1.0, 2.0, 3.0, 4.0], device=P.device)
        y_course_pred = (P @ expected_values).squeeze(-1)
        return logits, p_ge, P, y_sent_pred, y_course_pred

# ------------------------- å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆLLPï¼‰ -------------------------

def groupby_course_mean(P: torch.Tensor, course_ids: List) -> Tuple[torch.Tensor, List]:
    # ãƒãƒƒãƒå†…ã§åŒä¸€ course_id ã‚’å¹³å‡
    unique_ids = []
    pbar_list = []
    id_to_indices: Dict = {}
    for i, cid in enumerate(course_ids):
        id_to_indices.setdefault(cid, []).append(i)
    for cid, idxs in id_to_indices.items():
        unique_ids.append(cid)
        pbar_list.append(P[idxs].mean(dim=0, keepdim=True))
    return torch.cat(pbar_list, dim=0), unique_ids


def gather_targets(q: torch.Tensor, w: torch.Tensor, course_ids: List, unique_ids: List) -> Tuple[torch.Tensor, torch.Tensor]:
    # unique ids ã«å¯¾å¿œã™ã‚‹ q, w ã‚’ã¾ã¨ã‚ã‚‹ï¼ˆå¹³å‡/åˆè¨ˆï¼‰
    id_to_idx = {}
    for i, cid in enumerate(course_ids):
        id_to_idx.setdefault(cid, []).append(i)
    q_list, w_list = [], []
    for cid in unique_ids:
        idxs = id_to_idx[cid]
        q_list.append(q[idxs].mean(dim=0, keepdim=True))
        w_list.append(w[idxs].sum().unsqueeze(0))
    return torch.cat(q_list, dim=0), torch.cat(w_list, dim=0)


def kl_divergence(q: torch.Tensor, p: torch.Tensor) -> torch.Tensor:
    # KL(q || p) = sum q * (log q - log p)
    return (q * (q.clamp(1e-8).log() - p.clamp(1e-8).log())).sum(dim=1)


def train_loop(model: nn.Module, loader: DataLoader, optimizer, scaler, epoch: int):
    model.train()
    total_loss = 0.0
    last_print = 0.0
    mse = nn.MSELoss(reduction="none")
    for step, batch in enumerate(loader):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        chunk_mask = batch["chunk_mask"].to(device)
        q = batch["q"].to(device)
        w = batch["w"].to(device)
        y_sent = batch["y_sent"].to(device)
        y_course = batch["y_course"].to(device)
        course_ids = batch["course_ids"]

        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast(enabled=AMP_ENABLED):
            _, _, P, y_sent_pred, y_course_pred = model(input_ids, attention_mask, chunk_mask)
            pbar, uniq = groupby_course_mean(P, course_ids)
            q_bar, w_bar = gather_targets(q, w, course_ids, uniq)
            loss_per = kl_divergence(q_bar, pbar)  # [B_unique]
            kl_loss = (loss_per * w_bar).sum() / (w_bar.sum().clamp_min(1e-6))
            sent_loss = mse(y_sent_pred, y_sent).mean()
            # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯æœŸå¾…å€¤ã‹ã‚‰è¨ˆç®—ã™ã‚‹ãŸã‚ã€æå¤±ã¯ä¸è¦
            loss = kl_loss + ALPHA_SENT * sent_loss
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += float(loss.item())
        if (step + 1) % 10 == 0:
            print(f"Epoch {epoch+1} | Step {step+1}/{len(loader)} | Loss {total_loss/(step+1):.4f}")
    return total_loss / max(1, len(loader))


def eval_loop(model: nn.Module, loader: DataLoader):
    model.eval()
    total_loss = 0.0
    mse = nn.MSELoss(reduction="none")
    with torch.no_grad():
        for step, batch in enumerate(loader):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            chunk_mask = batch["chunk_mask"].to(device)
            q = batch["q"].to(device)
            w = batch["w"].to(device)
            y_sent = batch["y_sent"].to(device)
            y_course = batch["y_course"].to(device)
            course_ids = batch["course_ids"]
            with torch.cuda.amp.autocast(enabled=AMP_ENABLED):
                _, _, P, y_sent_pred, y_course_pred = model(input_ids, attention_mask, chunk_mask)
                pbar, uniq = groupby_course_mean(P, course_ids)
                q_bar, w_bar = gather_targets(q, w, course_ids, uniq)
                loss_per = kl_divergence(q_bar, pbar)
                kl_loss = (loss_per * w_bar).sum() / (w_bar.sum().clamp_min(1e-6))
                sent_loss = mse(y_sent_pred, y_sent).mean()
                # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯æœŸå¾…å€¤ã‹ã‚‰è¨ˆç®—ã™ã‚‹ãŸã‚ã€æå¤±ã¯ä¸è¦
                loss = kl_loss + ALPHA_SENT * sent_loss
            total_loss += float(loss.item())
    return total_loss / max(1, len(loader))

# ------------------------- ãƒ¡ã‚¤ãƒ³ -------------------------

def main():
    # ãƒ«ãƒ¼ãƒˆã«ç§»å‹•ï¼ˆ00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ ã‹ã‚‰ã®ç›¸å¯¾å®Ÿè¡Œã«å¯¾å¿œï¼‰
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        os.chdir(project_root)
    except Exception:
        pass
    print(f"ğŸ“‚ CWD: {os.getcwd()}")

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    tokenizer = BertJapaneseTokenizer.from_pretrained(BASE_MODEL)
    try:
        # é•·æ–‡è­¦å‘Šã®æŠ‘åˆ¶ï¼ˆè‡ªå‰ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹ãŸã‚ä¸Šé™ã‚’ååˆ†å¤§ããï¼‰
        tokenizer.model_max_length = 10**6
    except Exception:
        pass

    # ãƒ‡ãƒ¼ã‚¿èª­è¾¼
    df = load_agg_dataframe()

    # åˆ†å‰²ï¼ˆæˆæ¥­å˜ä½ã®å±¤åŒ–ãŒæœ›ã¾ã—ã„ãŒã€æœ€å°å®Ÿè£…ã¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    rng = np.random.RandomState(42)
    idx = np.arange(len(df))
    rng.shuffle(idx)
    n = len(idx)
    n_train = int(n * 0.7)
    n_val = int(n * 0.15)
    train_idx = idx[:n_train]
    val_idx = idx[n_train:n_train + n_val]
    test_idx = idx[n_train + n_val:]

    train_ds = CourseLLPDataset(df.iloc[train_idx].copy(), tokenizer)
    val_ds = CourseLLPDataset(df.iloc[val_idx].copy(), tokenizer)
    test_ds = CourseLLPDataset(df.iloc[test_idx].copy(), tokenizer)

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=NUM_WORKERS, collate_fn=collate_batch)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,
                            num_workers=NUM_WORKERS, collate_fn=collate_batch)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,
                             num_workers=NUM_WORKERS, collate_fn=collate_batch)

    # ãƒ¢ãƒ‡ãƒ«
    model = CourseOrdinalLLPModel(BASE_MODEL).to(device)
    # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯CUDAç’°å¢ƒã®ã¿æœ‰åŠ¹åŒ–ï¼ˆDirectMLã§ã¯ç„¡åŠ¹ï¼‰
    try:
        if torch.cuda.is_available() and hasattr(model.bert, "gradient_checkpointing_enable"):
            model.bert.gradient_checkpointing_enable()
            print("ğŸ§  Gradient Checkpointing æœ‰åŠ¹åŒ– (CUDA)")
        else:
            print("â„¹ï¸ Gradient Checkpointing ç„¡åŠ¹ (CUDAéåˆ©ç”¨)")
    except Exception:
        print("â„¹ï¸ Gradient Checkpointing è¨­å®šã‚¹ã‚­ãƒƒãƒ—")
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)
    # AMPã¯CUDAã®ã¿æœ‰åŠ¹
    scaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED)

    # å­¦ç¿’
    best_val = float("inf")
    best_state = None
    for epoch in range(NUM_EPOCHS):
        tr = train_loop(model, train_loader, optimizer, scaler, epoch)
        va = eval_loop(model, val_loader)
        print(f"[Epoch {epoch+1}] Train {tr:.4f} | Val {va:.4f}")
        if va < best_val:
            best_val = va
            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}

    if best_state is not None:
        model.load_state_dict(best_state)

    # ãƒ†ã‚¹ãƒˆ
    test_loss = eval_loop(model, test_loader)
    print(f"ğŸ§ª Test KL (weighted): {test_loss:.4f}")

    # ä¿å­˜
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = os.path.join("02_ãƒ¢ãƒ‡ãƒ«", "æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«")
    os.makedirs(out_dir, exist_ok=True)
    model_path = os.path.join(out_dir, f"class_level_ordinal_llp_{ts}.pth")
    torch.save(model.state_dict(), model_path)

    results = {
        "timestamp": ts,
        "test_weighted_KL": float(test_loss),
        "base_model": BASE_MODEL,
        "max_length": MAX_LENGTH,
        "batch_size": BATCH_SIZE,
        "epochs": NUM_EPOCHS,
        "lr": LEARNING_RATE,
        "notes": "course-level LLP with ordinal (1-4)"
    }
    with open(os.path.join(out_dir, f"class_level_ordinal_llp_{ts}.json"), "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ä¿å­˜: {model_path}")


if __name__ == "__main__":
    main()
