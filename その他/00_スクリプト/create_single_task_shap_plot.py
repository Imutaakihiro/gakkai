#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®SHAPãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦SHAP summary plotã‚’ç”Ÿæˆ
"""

import torch
import pandas as pd
import numpy as np
from transformers import BertForSequenceClassification, BertJapaneseTokenizer
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from tqdm import tqdm
import json
import os
from datetime import datetime

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

print("="*60)
print("å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®SHAPãƒ—ãƒ­ãƒƒãƒˆä½œæˆ")
print("="*60)

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
MODEL_PATH = r"C:\Users\takahashi.DESKTOP-U0T5SUB\Downloads\BERT\git_excluded\finetuned_bert_model_20250718_step2_fixed_classweights_variant1_positiveé‡ç‚¹å¼·åŒ–"
print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_PATH}")

try:
    tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_PATH)
    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
    model.to(device)
    model.eval()
    print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
except Exception as e:
    print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ”„ ä»£æ›¿ãƒ‘ã‚¹ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
    # ä»£æ›¿ãƒ‘ã‚¹
    MODEL_PATH = "../02_ãƒ¢ãƒ‡ãƒ«/å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«2_è©•ä¾¡ã‚¹ã‚³ã‚¢"
    tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_PATH)
    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
    model.to(device)
    model.eval()
    print("âœ… ä»£æ›¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
DATA_PATH = "../01_ãƒ‡ãƒ¼ã‚¿/è‡ªç”±è¨˜è¿°â†’æ„Ÿæƒ…ã‚¹ã‚³ã‚¢/finetuning_val_20250710_220621.csv"
print(f"\nãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {DATA_PATH}")

try:
    df = pd.read_csv(DATA_PATH)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
except Exception as e:
    print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ”„ ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œã—ã¾ã™...")
    # ä»£æ›¿ãƒ‡ãƒ¼ã‚¿
    DATA_PATH = "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv"
    df = pd.read_csv(DATA_PATH)
    print(f"âœ… ä»£æ›¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ100ä»¶ã§ãƒ†ã‚¹ãƒˆï¼‰
SAMPLE_SIZE = 100
print(f"\nã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {SAMPLE_SIZE}ä»¶")

if 'è‡ªç”±è¨˜è¿°ã¾ã¨ã‚' in df.columns:
    texts = df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].dropna().tolist()
elif 'è‡ªç”±è¨˜è¿°' in df.columns:
    texts = df['è‡ªç”±è¨˜è¿°'].dropna().tolist()
else:
    print("âŒ é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    exit()

# ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
sample_texts = np.random.choice(texts, size=min(SAMPLE_SIZE, len(texts)), replace=False).tolist()
print(f"âœ… ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(sample_texts)}ä»¶")

# äºˆæ¸¬é–¢æ•°ï¼ˆSHAPç”¨ï¼‰
def predict_proba(texts):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’è¿”ã™"""
    if isinstance(texts, str):
        texts = [texts]
    elif isinstance(texts, np.ndarray):
        texts = texts.tolist()
    elif not isinstance(texts, list):
        try:
            texts = list(texts)
        except:
            texts = [str(texts)]
    
    # ç©ºæ–‡å­—åˆ—ã‚„ç„¡åŠ¹ãªå…¥åŠ›ã‚’å‡¦ç†
    texts = [str(t) if t else "" for t in texts]
    
    probs = []
    for text in texts:
        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # äºˆæ¸¬
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                prob = torch.softmax(logits, dim=-1)
                probs.append(prob.cpu().numpy()[0])
        except Exception as e:
            print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å‡ç­‰ãªç¢ºç‡ã‚’è¿”ã™
            probs.append(np.array([0.33, 0.33, 0.34]))
    
    return np.array(probs)

# SHAPåˆ†æå®Ÿè¡Œ
print("\nğŸ”¬ SHAPåˆ†æå®Ÿè¡Œä¸­...")
print("âš ï¸ å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")

try:
    # SHAP Explainerä½œæˆ
    explainer = shap.Explainer(predict_proba, tokenizer)
    
    # SHAPå€¤è¨ˆç®—ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ï¼‰
    shap_values = explainer(sample_texts[:20])  # 20ä»¶ã§ãƒ†ã‚¹ãƒˆ
    
    print("âœ… SHAPåˆ†æå®Œäº†")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = "../03_åˆ†æçµæœ/å˜ä¸€ã‚¿ã‚¹ã‚¯SHAPãƒ—ãƒ­ãƒƒãƒˆ"
    os.makedirs(output_dir, exist_ok=True)
    
    # SHAP summary plotä½œæˆ
    print("\nğŸ“Š SHAPãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
    
    # 1. Summary plot (beeswarm plot)
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, sample_texts[:20], show=False)
    plt.title("å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®SHAP Summary Plot", fontsize=16, pad=20)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/shap_summary_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Waterfall plot (æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«)
    plt.figure(figsize=(12, 8))
    shap.waterfall_plot(shap_values[0], show=False)
    plt.title("å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®SHAP Waterfall Plot (ã‚µãƒ³ãƒ—ãƒ«1)", fontsize=16, pad=20)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/shap_waterfall_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Bar plot (é‡è¦åº¦é †)
    plt.figure(figsize=(12, 8))
    shap.plots.bar(shap_values, show=False)
    plt.title("å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®SHAP Bar Plot", fontsize=16, pad=20)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/shap_bar_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… SHAPãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
    
    # çµæœã®ä¿å­˜
    results = {
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "model_type": "single_task_sentiment",
        "sample_size": len(sample_texts[:20]),
        "shap_values_shape": shap_values.shape,
        "output_files": [
            f"shap_summary_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
            f"shap_waterfall_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
            f"shap_bar_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        ]
    }
    
    with open(f"{output_dir}/shap_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… çµæœä¿å­˜å®Œäº†: {output_dir}")
    
except Exception as e:
    print(f"âŒ SHAPåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ”„ ç°¡æ˜“ç‰ˆSHAPåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
    
    # ç°¡æ˜“ç‰ˆSHAPåˆ†æ
    try:
        # ã‚ˆã‚Šå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã§å†è©¦è¡Œ
        explainer = shap.Explainer(predict_proba, tokenizer)
        shap_values = explainer(sample_texts[:5])  # 5ä»¶ã§ãƒ†ã‚¹ãƒˆ
        
        print("âœ… ç°¡æ˜“ç‰ˆSHAPåˆ†æå®Œäº†")
        
        # ç°¡æ˜“ç‰ˆãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
        output_dir = "../03_åˆ†æçµæœ/å˜ä¸€ã‚¿ã‚¹ã‚¯SHAPãƒ—ãƒ­ãƒƒãƒˆ"
        os.makedirs(output_dir, exist_ok=True)
        
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values, sample_texts[:5], show=False)
        plt.title("å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®SHAP Summary Plot (ç°¡æ˜“ç‰ˆ)", fontsize=14, pad=20)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/shap_summary_plot_simple_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                    dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ç°¡æ˜“ç‰ˆSHAPãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
    except Exception as e2:
        print(f"âŒ ç°¡æ˜“ç‰ˆSHAPåˆ†æã‚‚ã‚¨ãƒ©ãƒ¼: {e2}")
        print("ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

print("\nğŸ‰ å˜ä¸€ã‚¿ã‚¹ã‚¯SHAPãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†ï¼")
print("ğŸ“ çµæœã¯ '../03_åˆ†æçµæœ/å˜ä¸€ã‚¿ã‚¹ã‚¯SHAPãƒ—ãƒ­ãƒƒãƒˆ' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
