#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CUDAç’°å¢ƒè¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import subprocess

def run_command(cmd):
    """ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout.strip(), result.stderr.strip()
    except Exception as e:
        return "", str(e)

def main():
    print("=" * 60)
    print("ğŸ” CUDAç’°å¢ƒè¨ºæ–­")
    print("=" * 60)
    
    # Pythonæƒ…å ±
    print(f"Python version: {sys.version}")
    print(f"Python executable: {sys.executable}")
    
    # PyTorchæƒ…å ±
    try:
        import torch
        print(f"\nğŸ“¦ PyTorchæƒ…å ±:")
        print(f"  PyTorch version: {torch.__version__}")
        print(f"  CUDA available: {torch.cuda.is_available()}")
        print(f"  CUDA version (PyTorch): {torch.version.cuda}")
        print(f"  cuDNN version: {torch.backends.cudnn.version()}")
        
        if torch.cuda.is_available():
            print(f"  GPU count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"  GPU {i}: {props.name}")
                print(f"    Compute capability: {props.major}.{props.minor}")
                print(f"    Memory: {props.total_memory / 1024**3:.1f} GB")
        else:
            print("  âŒ CUDA is not available")
            
    except ImportError as e:
        print(f"âŒ PyTorch import error: {e}")
    except Exception as e:
        print(f"âŒ PyTorch error: {e}")
    
    # NVIDIAæƒ…å ±
    print(f"\nğŸ® NVIDIAæƒ…å ±:")
    nvidia_smi, nvidia_err = run_command("nvidia-smi")
    if nvidia_smi:
        print("nvidia-smi output:")
        print(nvidia_smi)
    else:
        print(f"âŒ nvidia-smi error: {nvidia_err}")
    
    # CUDA Toolkitæƒ…å ±
    print(f"\nğŸ”§ CUDA Toolkitæƒ…å ±:")
    nvcc_version, nvcc_err = run_command("nvcc --version")
    if nvcc_version:
        print("nvcc version:")
        print(nvcc_version)
    else:
        print(f"âŒ nvcc not found: {nvcc_err}")
    
    # è¨ºæ–­çµæœ
    print(f"\nğŸ“‹ è¨ºæ–­çµæœ:")
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            if "RTX 5070" in gpu_name:
                print("âœ… RTX 5070 Ti detected - æœ€æ–°ä¸–ä»£GPU")
                print("âš ï¸  PyTorch 2.5.1ã§ã¯ sm_120 (compute capability 12.0) ãŒæœªå¯¾å¿œ")
                print("ğŸ’¡ è§£æ±ºç­–:")
                print("   1. PyTorch nightlyç‰ˆã‚’è©¦ã™")
                print("   2. ä¸€æ™‚çš„ã«CPUã§å®Ÿè¡Œ")
                print("   3. å¤ã„GPUã§ãƒ†ã‚¹ãƒˆ")
            else:
                print(f"âœ… GPU detected: {gpu_name}")
        else:
            print("âŒ CUDA not available")
    except:
        print("âŒ PyTorch not available")
    
    print("\n" + "=" * 60)

if __name__ == "__main__":
    main()
    input("\nPress Enter to exit...")
