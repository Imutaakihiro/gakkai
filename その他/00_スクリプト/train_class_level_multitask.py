#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆæ¥­å˜ä½ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’
æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ + æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã‚’åŒæ™‚ã«å­¦ç¿’

ãƒ‡ãƒ¼ã‚¿æ§‹æˆ:
- å…¥åŠ›: æˆæ¥­ã®å…¨è‡ªç”±è¨˜è¿°ï¼ˆé›†å›£ãƒ¬ãƒ™ãƒ«ï¼‰
- å‡ºåŠ›1: æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ï¼ˆé›†å›£ãƒ¬ãƒ™ãƒ«ï¼‰
- å‡ºåŠ›2: æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆé›†å›£ãƒ¬ãƒ™ãƒ«ï¼‰
â†’ ãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´ã«ã‚ˆã‚Šå­¦ç¿’å¯èƒ½
"""

import os
# CUDAåŒæœŸãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFF
DEBUG_CUDA_SYNC = False
if DEBUG_CUDA_SYNC:
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
else:
    # ä»¥å‰ã®å®Ÿè¡Œã§ç’°å¢ƒå¤‰æ•°ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã®ä¿é™º
    if os.environ.get("CUDA_LAUNCH_BLOCKING") == "1":
        del os.environ["CUDA_LAUNCH_BLOCKING"]

# ãƒ‡ãƒã‚¤ã‚¹è‡ªå‹•åˆ‡æ›¿æ©Ÿèƒ½
def get_available_device():
    """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•é¸æŠï¼ˆCUDA â†’ DirectML â†’ CPUï¼‰"""
    try:
        # ã¾ãšCUDAã‚’è©¦è¡Œ
        if torch.cuda.is_available():
            # CUDAãŒåˆ©ç”¨å¯èƒ½ã§ã‚‚ã€å®Ÿéš›ã«ãƒ†ã‚¹ãƒˆã—ã¦ç¢ºèª
            try:
                test_tensor = torch.tensor([1.0]).cuda()
                _ = test_tensor + test_tensor  # ç°¡å˜ãªæ¼”ç®—ã§ãƒ†ã‚¹ãƒˆ
                print("âœ… CUDA ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
                return torch.device('cuda')
            except RuntimeError as e:
                if "no kernel image" in str(e):
                    print("âš ï¸ CUDA ã§ 'no kernel image' ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                    print("ğŸ”„ DirectML ã¾ãŸã¯ CPU ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
                else:
                    print(f"âš ï¸ CUDA ã‚¨ãƒ©ãƒ¼: {e}")
                    print("ğŸ”„ DirectML ã¾ãŸã¯ CPU ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
    except Exception as e:
        print(f"âš ï¸ CUDA ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
    
    # DirectMLã‚’è©¦è¡Œ
    try:
        import torch_directml as dml
        if dml.is_available():
            # DirectMLãƒ‡ãƒã‚¤ã‚¹ã‚’å®Ÿéš›ã«ãƒ†ã‚¹ãƒˆ
            try:
                device = dml.device()
                test_tensor = torch.randn(2, 2, device=device)
                _ = test_tensor @ test_tensor  # ç°¡å˜ãªæ¼”ç®—ã§ãƒ†ã‚¹ãƒˆ
                print("âœ… DirectML ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
                return device
            except Exception as dml_error:
                print(f"âš ï¸ DirectML ãƒ‡ãƒã‚¤ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {dml_error}")
                if "staticmethod" in str(dml_error):
                    print("   ã“ã‚Œã¯ PyTorch 2.4.x + torch-directml ã®äº’æ›æ€§å•é¡Œã§ã™")
                    print("   æ¨å¥¨: PyTorch 2.2.2 + torch-directml 0.2.3.dev240715")
    except ImportError:
        print("â„¹ï¸ DirectML ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    except Exception as e:
        print(f"âš ï¸ DirectML ã‚¨ãƒ©ãƒ¼: {e}")
    
    # CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    print("ğŸ”„ CPU ãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™")
    return torch.device('cpu')

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, BertJapaneseTokenizer
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
from datetime import datetime
import json
import warnings
import sys
import time
warnings.filterwarnings('ignore')

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®šï¼ˆè‡ªå‹•åˆ‡æ›¿æ©Ÿèƒ½ã‚’ä½¿ç”¨ï¼‰
device = get_available_device()
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# GPUè©³ç´°æƒ…å ±ã®è¡¨ç¤º
print(f"ğŸ” CUDAç’°å¢ƒãƒã‚§ãƒƒã‚¯:")
print(f"   CUDA available: {torch.cuda.is_available()}")
print(f"   Device count: {torch.cuda.device_count()}")
print(f"   PyTorch version: {torch.__version__}")
print(f"   CUDA version: {torch.version.cuda}")
if torch.cuda.is_available():
    print(f"   Device name: {torch.cuda.get_device_name(0)}")
    print(f"ğŸš€ GPUè©³ç´°æƒ…å ±:")
    print(f"   ğŸ“Š GPUå: {torch.cuda.get_device_name(0)}")
    print(f"   ğŸ’¾ ç·ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")
    print(f"   ğŸ”§ Compute Capability: {torch.cuda.get_device_capability(0)}")
    print(f"   ğŸ“ˆ ç¾åœ¨ã®ä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated(0) / (1024**3):.2f}GB")
    print(f"   ğŸ”§ CUDA_LAUNCH_BLOCKING: {os.environ.get('CUDA_LAUNCH_BLOCKING', 'Not set')}")
else:
    print("âš ï¸  GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã§å®Ÿè¡Œã—ã¾ã™ã€‚")

# ---- CUDA/SDPA å®‰å®šåŒ–è¨­å®šï¼ˆWindows + RTX 40 ç³»ã§ã®åˆå› forward ãƒãƒ³ã‚°å¯¾ç­–ï¼‰----
try:
    import platform
    # cuDNN ã®è‡ªå‹•æœ€é©åŒ–ã‚’ç„¡åŠ¹åŒ–ã—ã€æ±ºå®šè«–çš„ã«ï¼ˆãƒ‡ãƒãƒƒã‚°æ€§å‘ä¸Šï¼‰
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

    if torch.cuda.is_available():
        # PyTorch 2.x ã® SDPA backend ã‚’ math ã®ã¿ã«å›ºå®šã—ã¦ Flash/MemEfficient ã‚’ç„¡åŠ¹åŒ–
        try:
            from torch.backends.cuda import sdp_kernel
            sdp_kernel.enable_flash(False)
            sdp_kernel.enable_mem_efficient(False)
            sdp_kernel.enable_math(True)
            print("ğŸ§¯ SDPA: flash/mem_efficient ç„¡åŠ¹åŒ– (math ã®ã¿)")
        except Exception:
            # æ—§ APIï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³å·®åˆ†ç”¨ï¼‰
            try:
                torch.backends.cuda.enable_flash_sdp(False)
                torch.backends.cuda.enable_mem_efficient_sdp(False)
                torch.backends.cuda.enable_math_sdp(True)
                print("ğŸ§¯ SDPA: æ—§APIã§ math ã®ã¿ã«å›ºå®š")
            except Exception:
                print("â„¹ï¸ SDPA backend ã®å›ºå®šã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆéå¯¾å¿œãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰")
        # Fuser ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ‘æ­¢ï¼ˆç¨€ãªãƒ•ãƒªãƒ¼ã‚ºå›é¿ï¼‰
        os.environ.setdefault("PYTORCH_CUDA_FUSER_DISABLE_FALLBACK", "1")
except Exception as _e:
    print(f"âš ï¸ CUDA/SDPA å®‰å®šåŒ–è¨­å®šã§ä¾‹å¤–: {_e}")

# è¿½åŠ ã®é«˜é€ŸåŒ–/å®‰å®šåŒ–ï¼ˆTF32ï¼‰
try:
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_float32_matmul_precision("high")
        print("âš¡ TF32 æœ‰åŠ¹åŒ–: high precision matmul")
except Exception as _e:
    print(f"â„¹ï¸ TF32è¨­å®šã‚¹ã‚­ãƒƒãƒ—: {_e}")

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
BASE_MODEL = "koheiduck/bert-japanese-finetuned-sentiment"

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé•·æ–‡å¯¾å¿œ: åˆ†å‰² + é›†ç´„ æ–¹å¼ï¼‰
MAX_LENGTH = 256   # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®é•·ã•ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰
BATCH_SIZE = 2     # ç‰©ç†ãƒãƒƒãƒ
ACCUM_STEPS = 4    # å‹¾é…è“„ç©ï¼ˆå®ŸåŠ¹ãƒãƒƒãƒ = BATCH_SIZE * ACCUM_STEPSï¼‰
USE_AMP = True     # è‡ªå‹•æ··åˆç²¾åº¦
# æ—¢å®šã¯é«˜é€Ÿè¨­å®šã ãŒã€å¤ã„Torch/Windowsã§ã¯å®‰å…¨å´ã«è‡ªå‹•ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰
PIN_MEMORY = True         # HtoDè»¢é€ã‚’å›ºå®šãƒ¡ãƒ¢ãƒªåŒ–
NON_BLOCKING = True       # éåŒæœŸè»¢é€ï¼ˆpin_memory=Trueæ™‚ã«æœ‰åŠ¹ï¼‰
USE_GRADIENT_CHECKPOINTING = False  # PyTorch 1.13 ç’°å¢ƒã§ã¯ç„¡åŠ¹åŒ–ãŒå®‰å®š
WARMUP_FORWARD = False    # åˆå›forwardã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåœæ­¢ã®åŸå› ã«ãªã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFFï¼‰

# DataLoader ä¸¦åˆ—è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
NUM_WORKERS = 2
PREFETCH_FACTOR = 2
PERSISTENT_WORKERS = True

# ãƒãƒ£ãƒ³ã‚¯è¨­å®šï¼ˆé•·æ–‡ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
CHUNK_LEN = 256
STRIDE = 200
MAX_CHUNKS = 10
LEARNING_RATE = 2e-5
NUM_EPOCHS = 20
ALPHA = 0.5  # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®é‡ã¿
BETA = 0.5   # è©•ä¾¡ã‚¹ã‚³ã‚¢ã®é‡ã¿

print(f"ğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆé•·æ–‡å¯¾å¿œãƒ»é«˜é€ŸåŒ–ï¼‰:")
print(f"   CHUNK_LEN: {CHUNK_LEN} / STRIDE: {STRIDE} / MAX_CHUNKS: {MAX_CHUNKS}")
print(f"   MAX_LENGTH(=CHUNK_LEN): {MAX_LENGTH}")
print(f"   BATCH_SIZE: {BATCH_SIZE}  ACCUM_STEPS: {ACCUM_STEPS}  USE_AMP: {USE_AMP}")
print(f"   LEARNING_RATE: {LEARNING_RATE}")
print(f"   NUM_EPOCHS: {NUM_EPOCHS}")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
try:
    plt.rcParams['font.family'] = 'MS Gothic'
except:
    try:
        plt.rcParams['font.family'] = 'Yu Gothic'
    except:
        plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


def create_progress_bar(current, total, width=50, prefix="", suffix=""):
    """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ä½œæˆ"""
    percent = current / total
    filled = int(width * percent)
    bar = "â–ˆ" * filled + "â–‘" * (width - filled)
    return f"\r{prefix} |{bar}| {percent:.1%} {suffix}"


def get_gpu_status():
    """GPUä½¿ç”¨çŠ¶æ³ã‚’å–å¾—"""
    if torch.cuda.is_available():
        gpu_memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)  # GB
        gpu_memory_reserved = torch.cuda.memory_reserved(0) / (1024**3)   # GB
        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        gpu_utilization = (gpu_memory_allocated / gpu_memory_total) * 100
        
        return {
            'allocated': gpu_memory_allocated,
            'reserved': gpu_memory_reserved,
            'total': gpu_memory_total,
            'utilization': gpu_utilization
        }
    return None


def print_progress_gauge(current, total, prefix="", suffix="", show_percent=True, show_gpu=True):
    """ã‚²ãƒ¼ã‚¸é¢¨ã®é€²æ—è¡¨ç¤ºï¼ˆGPUçŠ¶æ³ä»˜ãï¼‰- æ¯ç§’æ›´æ–°"""
    percent = current / total
    
    # ã‚²ãƒ¼ã‚¸ã®å¹…
    gauge_width = 20
    filled = int(gauge_width * percent)
    empty = gauge_width - filled
    
    # ã‚²ãƒ¼ã‚¸ã®è‰²ï¼ˆã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§è‰²åˆ†ã‘ï¼‰
    gauge_bar = "â–ˆ" * filled + "â–‘" * empty
    
    # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¡¨ç¤º
    percent_str = f"{percent:.1%}" if show_percent else ""
    
    # GPUçŠ¶æ³ã®è¡¨ç¤º
    gpu_info = ""
    if show_gpu and torch.cuda.is_available():
        gpu_status = get_gpu_status()
        if gpu_status:
            gpu_info = f" | GPU: {gpu_status['utilization']:.1f}% ({gpu_status['allocated']:.1f}GB/{gpu_status['total']:.1f}GB)"
    
    # ç¾åœ¨æ™‚åˆ»ã‚’è¿½åŠ 
    current_time = datetime.now().strftime("%H:%M:%S")
    
    # æ¯ç§’æ›´æ–°å‡ºåŠ›ï¼ˆåŒã˜è¡Œã‚’ä¸Šæ›¸ãï¼‰
    print(f"\r[{current_time}] {prefix} [{gauge_bar}] {percent_str}{gpu_info} {suffix}", end="", flush=True)
    
    if current == total:
        print()  # å®Œäº†æ™‚ã«æ”¹è¡Œ


def print_epoch_summary(epoch, total_epochs, train_loss, val_loss, sent_r2, course_r2, 
                       best_val_loss, current_lr, elapsed_time=None):
    """ã‚¨ãƒãƒƒã‚¯ã‚µãƒãƒªãƒ¼ã‚’ã‚²ãƒ¼ã‚¸é¢¨ã§è¡¨ç¤º"""
    print(f"\n{'='*80}")
    print(f"ğŸ¯ EPOCH {epoch}/{total_epochs} å®Œäº†")
    print(f"{'='*80}")
    
    # GPUçŠ¶æ³ã®è¡¨ç¤º
    if torch.cuda.is_available():
        gpu_status = get_gpu_status()
        if gpu_status:
            print(f"ğŸš€ GPUçŠ¶æ³: {gpu_status['utilization']:.1f}% ä½¿ç”¨ä¸­")
            print(f"   ğŸ“Š ãƒ¡ãƒ¢ãƒª: {gpu_status['allocated']:.1f}GB / {gpu_status['total']:.1f}GB")
            print(f"   ğŸ”’ äºˆç´„æ¸ˆã¿: {gpu_status['reserved']:.1f}GB")
    
    # ã‚¨ãƒãƒƒã‚¯é€²æ—ã‚²ãƒ¼ã‚¸
    epoch_progress = epoch / total_epochs
    print_progress_gauge(epoch, total_epochs, "ğŸ“ˆ ã‚¨ãƒãƒƒã‚¯é€²æ—", f"({epoch}/{total_epochs})", True, False)
    
    # æå¤±ã®ã‚²ãƒ¼ã‚¸è¡¨ç¤º
    max_loss = max(train_loss, val_loss) * 1.2  # æœ€å¤§å€¤ã®120%ã‚’åŸºæº–
    train_loss_gauge = min(train_loss / max_loss, 1.0)
    val_loss_gauge = min(val_loss / max_loss, 1.0)
    
    print(f"ğŸ”¥ å­¦ç¿’æå¤±: {train_loss:.4f}")
    print_progress_gauge(train_loss_gauge, 1.0, "  ", f"({train_loss:.4f})", False, False)
    
    print(f"âœ… æ¤œè¨¼æå¤±: {val_loss:.4f}")
    print_progress_gauge(val_loss_gauge, 1.0, "  ", f"({val_loss:.4f})", False, False)
    
    # RÂ²ã‚¹ã‚³ã‚¢ã®ã‚²ãƒ¼ã‚¸è¡¨ç¤º
    print(f"ğŸ“Š æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ RÂ²: {sent_r2:.4f}")
    print_progress_gauge(max(0, sent_r2), 1.0, "  ", f"({sent_r2:.4f})", False, False)
    
    print(f"ğŸ“Š æˆæ¥­è©•ä¾¡ RÂ²: {course_r2:.4f}")
    print_progress_gauge(max(0, course_r2), 1.0, "  ", f"({course_r2:.4f})", False, False)
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«çŠ¶æ³
    if val_loss < best_val_loss:
        print(f"ğŸ† ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ (Val Loss: {val_loss:.4f})")
    else:
        print(f"â³ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ç¶­æŒ (Best: {best_val_loss:.4f})")
    
    # å­¦ç¿’ç‡ã¨æ™‚é–“
    print(f"ğŸ“š å­¦ç¿’ç‡: {current_lr:.2e}")
    if elapsed_time:
        print(f"â° çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†")
    
    print(f"{'='*80}")


class ClassLevelDataset(Dataset):
    """æˆæ¥­ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆé•·æ–‡å¯¾å¿œ: ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼‰"""
    def __init__(self, texts, sentiment_scores, course_scores, tokenizer,
                 chunk_len=256, stride=200, max_chunks=10):
        self.texts = texts
        self.sentiment_scores = sentiment_scores
        self.course_scores = course_scores
        self.tokenizer = tokenizer
        self.chunk_len = chunk_len
        self.stride = stride
        self.max_chunks = max_chunks

        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ID
        self.cls_id = tokenizer.cls_token_id
        self.sep_id = tokenizer.sep_token_id

    def __len__(self):
        return len(self.texts)

    def _chunk_encode(self, text):
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è‡ªå‰ã§ä»˜ä¸ã™ã‚‹ãŸã‚ add_special_tokens=False
        token_ids = self.tokenizer.encode(str(text), add_special_tokens=False)
        inner_max = self.chunk_len - 2
        chunks_ids = []

        if len(token_ids) == 0:
            # ç©ºæ–‡å­—å¯¾ç­–: [CLS][SEP] ã®ã¿
            ids = [self.cls_id, self.sep_id] + [0]*(self.chunk_len-2)
            attn = [1, 1] + [0]*(self.chunk_len-2)
            return [torch.tensor(ids, dtype=torch.long)], [torch.tensor(attn, dtype=torch.long)]

        for start in range(0, len(token_ids), self.stride):
            inner = token_ids[start:start+inner_max]
            ids = [self.cls_id] + inner + [self.sep_id]
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            pad_len = self.chunk_len - len(ids)
            if pad_len > 0:
                ids = ids + [0]*pad_len
                attn = [1]*len(inner+ [self.cls_id, self.sep_id]) + [0]*pad_len
            else:
                ids = ids[:self.chunk_len]
                attn = [1]*self.chunk_len
            chunks_ids.append((torch.tensor(ids, dtype=torch.long), torch.tensor(attn, dtype=torch.long)))
            if len(chunks_ids) >= self.max_chunks:
                break

        input_ids_list = [x[0] for x in chunks_ids]
        attention_list = [x[1] for x in chunks_ids]
        return input_ids_list, attention_list

    def __getitem__(self, idx):
        text = self.texts[idx]
        sentiment_score = self.sentiment_scores[idx]
        course_score = self.course_scores[idx]

        ids_list, attn_list = self._chunk_encode(text)

        return {
            'input_ids_list': ids_list,
            'attention_mask_list': attn_list,
            'num_chunks': len(ids_list),
            'sentiment_score': torch.tensor(sentiment_score, dtype=torch.float),
            'course_score': torch.tensor(course_score, dtype=torch.float)
        }


def collate_chunked_batch(batch):
    """å¯å¤‰ãƒãƒ£ãƒ³ã‚¯ã‚’ (B, C, L) ã«ã¾ã¨ã‚ã‚‹collate"""
    B = len(batch)
    C = MAX_CHUNKS
    L = CHUNK_LEN

    input_ids = torch.zeros((B, C, L), dtype=torch.long)
    attention_mask = torch.zeros((B, C, L), dtype=torch.long)
    chunk_mask = torch.zeros((B, C), dtype=torch.bool)
    y_sent = torch.zeros((B,), dtype=torch.float)
    y_course = torch.zeros((B,), dtype=torch.float)

    for i, item in enumerate(batch):
        n = min(item['num_chunks'], C)
        for j in range(n):
            input_ids[i, j] = item['input_ids_list'][j]
            attention_mask[i, j] = item['attention_mask_list'][j]
            chunk_mask[i, j] = True
        y_sent[i] = item['sentiment_score']
        y_course[i] = item['course_score']

    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'chunk_mask': chunk_mask,
        'sentiment_score': y_sent,
        'course_score': y_course
    }


class ClassLevelMultitaskModel(nn.Module):
    """æˆæ¥­ãƒ¬ãƒ™ãƒ«ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, base_model_name, dropout_rate=0.1):
        super().__init__()
        
        # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆå…±æœ‰å±¤ï¼‰- safetensorsä½¿ç”¨ã§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶å¯¾å¿œ
        try:
            self.bert = BertModel.from_pretrained(base_model_name, use_safetensors=True)
        except Exception:
            # safetensorsãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•
            self.bert = BertModel.from_pretrained(base_model_name)
        hidden_size = self.bert.config.hidden_size
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.sentiment_head = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 1)
        )
        
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.course_head = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 1)
        )
    
    def forward(self, input_ids, attention_mask, chunk_mask=None):
        # å…¥åŠ›ãŒ (B, C, L) ã®å ´åˆã¯å¹³å¦åŒ–ã—ã¦ã¾ã¨ã‚ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        if input_ids.dim() == 3:
            B, C, L = input_ids.shape
            x_ids = input_ids.view(B*C, L)
            x_mask = attention_mask.view(B*C, L)
            outputs = self.bert(input_ids=x_ids, attention_mask=x_mask)
            cls = outputs.last_hidden_state[:, 0, :]  # (B*C, H)
            H = cls.size(-1)
            cls = cls.view(B, C, H)  # (B, C, H)

            if chunk_mask is None:
                # ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å¹³å‡
                pooled = cls.mean(dim=1)
            else:
                # ãƒã‚¹ã‚¯ä»˜ãå¹³å‡
                mask = chunk_mask.float().unsqueeze(-1)  # (B, C, 1)
                summed = (cls * mask).sum(dim=1)  # (B, H)
                denom = mask.sum(dim=1).clamp_min(1e-6)  # (B, 1)
                pooled = summed / denom
        else:
            # äº’æ›: (B, L)
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            pooled = outputs.last_hidden_state[:, 0, :]

        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        sentiment_pred = self.sentiment_head(pooled).squeeze(-1)
        course_pred = self.course_head(pooled).squeeze(-1)
        return sentiment_pred, course_pred


def find_latest_csv_file():
    """æœ€æ–°ã®æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡º"""
    import os
    import glob
    
    # è¤‡æ•°ã®ãƒ‘ã‚¹å€™è£œã‚’è©¦ã™
    possible_paths = [
        '../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/',
        '../../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/',
        '01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/',
        '../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿',
        '../../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿',
        '01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿'
    ]
    
    csv_files = []
    for base_path in possible_paths:
        pattern = os.path.join(base_path, 'æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_*.csv')
        found_files = glob.glob(pattern)
        csv_files.extend(found_files)
    
    if not csv_files:
        # ã‚ˆã‚Šåºƒç¯„å›²ã§æ¤œç´¢
        for root, dirs, files in os.walk('..'):
            for file in files:
                if file.startswith('æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_') and file.endswith('.csv'):
                    csv_files.append(os.path.join(root, file))
    
    if csv_files:
        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼‰
        latest_file = max(csv_files, key=os.path.getctime)
        print(f"ğŸ“ è¦‹ã¤ã‹ã£ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        return latest_file
    else:
        raise FileNotFoundError("æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def load_data(sample_size=1000):
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ“Š æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
    print("="*60)
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•æ¤œå‡ºã¨èª­ã¿è¾¼ã¿
    csv_file_path = find_latest_csv_file()
    df = pd.read_csv(csv_file_path)
    
    print(f"ç·æˆæ¥­æ•°: {len(df)}ä»¶")
    
    # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãŸã‚ã®å±¤ã‚’ä½œæˆ
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ã¦å±¤ã‚’ä½œæˆ
    sentiment_bins = pd.qcut(df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'], q=3, labels=['ä½', 'ä¸­', 'é«˜'], duplicates='drop')
    course_bins = pd.qcut(df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'], q=3, labels=['ä½', 'ä¸­', 'é«˜'], duplicates='drop')
    
    # å±¤ãƒ©ãƒ™ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦è©³ç´°ãªå±¤ã‚’ä½œæˆ
    stratify_labels = [f'{s}_{c}' for s, c in zip(sentiment_bins, course_bins)]
    
    print(f"\nå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åˆ†å¸ƒ:")
    unique, counts = np.unique(stratify_labels, return_counts=True)
    for label, count in zip(unique, counts):
        print(f"  {label}: {count}ä»¶")
    
    # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ
    np.random.seed(42)
    if sample_size < len(df):
        # å„å±¤ã‹ã‚‰æ¯”ä¾‹çš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        df_sampled = df.groupby(stratify_labels, group_keys=False).apply(
            lambda x: x.sample(n=min(len(x), int(sample_size * len(x) / len(df))), random_state=42)
        ).reset_index(drop=True)
        
        # ç›®æ¨™ä»¶æ•°ã«èª¿æ•´
        if len(df_sampled) < sample_size:
            # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¿½åŠ 
            remaining_indices = df[~df.index.isin(df_sampled.index)].index
            additional_size = sample_size - len(df_sampled)
            if len(remaining_indices) >= additional_size:
                additional_indices = np.random.choice(remaining_indices, additional_size, replace=False)
                df_sampled = pd.concat([df_sampled, df.iloc[additional_indices]]).reset_index(drop=True)
        elif len(df_sampled) > sample_size:
            # è¶…éåˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰Šé™¤
            df_sampled = df_sampled.sample(n=sample_size, random_state=42).reset_index(drop=True)
        
        print(f"å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {len(df_sampled)}ä»¶ã‚’æŠ½å‡º")
    else:
        df_sampled = df
        print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
    
    print(f"ä½¿ç”¨æˆæ¥­æ•°: {len(df_sampled)}ä»¶")
    print(f"åˆ—å: {list(df_sampled.columns)}")
    
    # å¿…è¦ãªåˆ—ã‚’æŠ½å‡º
    texts = df_sampled['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].values
    sentiment_scores = df_sampled['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].values
    course_scores = df_sampled['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].values
    
    print(f"\næ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ã®çµ±è¨ˆ:")
    print(f"  å¹³å‡: {sentiment_scores.mean():.4f}")
    print(f"  æ¨™æº–åå·®: {sentiment_scores.std():.4f}")
    print(f"  ç¯„å›²: {sentiment_scores.min():.4f} ã€œ {sentiment_scores.max():.4f}")
    
    print(f"\næˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ:")
    print(f"  å¹³å‡: {course_scores.mean():.4f}")
    print(f"  æ¨™æº–åå·®: {course_scores.std():.4f}")
    print(f"  ç¯„å›²: {course_scores.min():.4f} ã€œ {course_scores.max():.4f}")
    
    return texts, sentiment_scores, course_scores


def prepare_data(texts, sentiment_scores, course_scores, tokenizer):
    """ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
    print("="*60)
    
    # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãŸã‚ã®å±¤ã‚’ä½œæˆ
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ã¦å±¤ã‚’ä½œæˆ
    sentiment_bins = pd.qcut(sentiment_scores, q=3, labels=['ä½', 'ä¸­', 'é«˜'], duplicates='drop')
    course_bins = pd.qcut(course_scores, q=3, labels=['ä½', 'ä¸­', 'é«˜'], duplicates='drop')
    
    # å±¤ãƒ©ãƒ™ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦è©³ç´°ãªå±¤ã‚’ä½œæˆ
    stratify_labels = [f'{s}_{c}' for s, c in zip(sentiment_bins, course_bins)]
    
    print(f"\nå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åˆ†å¸ƒ:")
    unique, counts = np.unique(stratify_labels, return_counts=True)
    for label, count in zip(unique, counts):
        print(f"  {label}: {count}ä»¶")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆ70% / 15% / 15%ï¼‰å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    X_temp, X_test, y_sent_temp, y_sent_test, y_course_temp, y_course_test, strat_temp, strat_test = train_test_split(
        texts, sentiment_scores, course_scores, stratify_labels,
        test_size=0.15, random_state=42, stratify=stratify_labels
    )
    
    X_train, X_val, y_sent_train, y_sent_val, y_course_train, y_course_val = train_test_split(
        X_temp, y_sent_temp, y_course_temp, 
        test_size=0.176, random_state=42, stratify=strat_temp  # 0.176 â‰ˆ 15/85
    )
    
    print(f"\nãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶ ({len(X_train)/len(texts)*100:.1f}%)")
    print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}ä»¶ ({len(X_val)/len(texts)*100:.1f}%)")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶ ({len(X_test)/len(texts)*100:.1f}%)")
    
    # å„ã‚»ãƒƒãƒˆã®åˆ†å¸ƒã‚’ç¢ºèª
    print(f"\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
    print(f"  å¹³å‡: {y_sent_train.mean():.4f}, æ¨™æº–åå·®: {y_sent_train.std():.4f}")
    print(f"  ç¯„å›²: {y_sent_train.min():.4f} ã€œ {y_sent_train.max():.4f}")
    
    print(f"\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
    print(f"  å¹³å‡: {y_course_train.mean():.4f}, æ¨™æº–åå·®: {y_course_train.std():.4f}")
    print(f"  ç¯„å›²: {y_course_train.min():.4f} ã€œ {y_course_train.max():.4f}")
    
    # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–
    sentiment_scaler = StandardScaler()
    course_scaler = StandardScaler()
    
    y_sent_train_scaled = sentiment_scaler.fit_transform(y_sent_train.reshape(-1, 1)).flatten()
    y_sent_val_scaled = sentiment_scaler.transform(y_sent_val.reshape(-1, 1)).flatten()
    y_sent_test_scaled = sentiment_scaler.transform(y_sent_test.reshape(-1, 1)).flatten()
    
    y_course_train_scaled = course_scaler.fit_transform(y_course_train.reshape(-1, 1)).flatten()
    y_course_val_scaled = course_scaler.transform(y_course_val.reshape(-1, 1)).flatten()
    y_course_test_scaled = course_scaler.transform(y_course_test.reshape(-1, 1)).flatten()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆï¼ˆé•·æ–‡å¯¾å¿œ: ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼‰
    train_dataset = ClassLevelDataset(
        X_train, y_sent_train_scaled, y_course_train_scaled, tokenizer,
        chunk_len=CHUNK_LEN, stride=STRIDE, max_chunks=MAX_CHUNKS
    )
    val_dataset = ClassLevelDataset(
        X_val, y_sent_val_scaled, y_course_val_scaled, tokenizer,
        chunk_len=CHUNK_LEN, stride=STRIDE, max_chunks=MAX_CHUNKS
    )
    test_dataset = ClassLevelDataset(
        X_test, y_sent_test_scaled, y_course_test_scaled, tokenizer,
        chunk_len=CHUNK_LEN, stride=STRIDE, max_chunks=MAX_CHUNKS
    )
    
    # DataLoaderã®ä½œæˆï¼ˆWindowsç’°å¢ƒå¯¾å¿œ: num_workers=0ï¼‰
    print(f"ğŸ“¦ DataLoaderä½œæˆä¸­...")
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³/OSã«å¿œã˜ã¦å®‰å…¨ãªDataLoaderè¨­å®šã‚’è‡ªå‹•é¸æŠ
    import platform
    torch_major = int(torch.__version__.split('.')[0])
    is_windows = platform.system().lower().startswith('win')
    use_safe_loader = (torch_major < 2) or is_windows

    if use_safe_loader:
        print("âš™ï¸ DataLoader: å®‰å…¨è¨­å®šã§èµ·å‹• (num_workers=0, pin_memory=False)")
        train_loader = DataLoader(
            train_dataset, batch_size=BATCH_SIZE, shuffle=True,
            num_workers=0, pin_memory=False, collate_fn=collate_chunked_batch
        )
        val_loader = DataLoader(
            val_dataset, batch_size=BATCH_SIZE, shuffle=False,
            num_workers=0, pin_memory=False, collate_fn=collate_chunked_batch
        )
        test_loader = DataLoader(
            test_dataset, batch_size=BATCH_SIZE, shuffle=False,
            num_workers=0, pin_memory=False, collate_fn=collate_chunked_batch
        )
        # è»¢é€ã®éåŒæœŸã¯ç„¡åŠ¹åŒ–
        global NON_BLOCKING
        NON_BLOCKING = False
    else:
        print(f"âš™ï¸ DataLoader: é«˜é€Ÿè¨­å®š (num_workers={NUM_WORKERS}, pin_memory={PIN_MEMORY}, prefetch={PREFETCH_FACTOR}, persistent={PERSISTENT_WORKERS})")
        dl_kwargs = dict(
            batch_size=BATCH_SIZE,
            pin_memory=PIN_MEMORY,
            num_workers=NUM_WORKERS,
            prefetch_factor=PREFETCH_FACTOR,
            persistent_workers=PERSISTENT_WORKERS,
            collate_fn=collate_chunked_batch,
        )
        train_loader = DataLoader(train_dataset, shuffle=True, **dl_kwargs)
        val_loader = DataLoader(val_dataset, shuffle=False, **dl_kwargs)
        test_loader = DataLoader(test_dataset, shuffle=False, **dl_kwargs)
    print(f"âœ… DataLoaderä½œæˆå®Œäº†")
    
    return train_loader, val_loader, test_loader, sentiment_scaler, course_scaler


def train_epoch(model, train_loader, optimizer, scheduler, scaler, epoch, num_epochs):
    """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’"""
    model.train()
    total_loss = 0
    sentiment_losses = 0
    course_losses = 0
    
    criterion = nn.MSELoss()
    last_update_time = time.time()
    
    print(f"  ğŸ“Š å­¦ç¿’ãƒãƒƒãƒæ•°: {len(train_loader)}")
    print(f"  ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"  ğŸš€ å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹...", flush=True)
    
    optimizer.zero_grad(set_to_none=True)

    for batch_idx, batch in enumerate(train_loader):
        # æœ€åˆã®ãƒãƒƒãƒã§ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        if batch_idx == 0:
            print(f"  ğŸš€ æœ€åˆã®ãƒãƒƒãƒå‡¦ç†é–‹å§‹...")
            print(f"  ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch['input_ids'].shape[0]}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
        if batch_idx == 0:
            print(f"  ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚’{device}ã«è»¢é€ä¸­...", flush=True)
        input_ids = batch['input_ids'].to(device, non_blocking=NON_BLOCKING)
        attention_mask = batch['attention_mask'].to(device, non_blocking=NON_BLOCKING)
        chunk_mask = batch['chunk_mask'].to(device, non_blocking=NON_BLOCKING)
        sentiment_true = batch['sentiment_score'].to(device, non_blocking=NON_BLOCKING)
        course_true = batch['course_score'].to(device, non_blocking=NON_BLOCKING)
        
        # æœ€åˆã®ãƒãƒƒãƒã§ãƒ‡ãƒã‚¤ã‚¹è»¢é€ç¢ºèª
        if batch_idx == 0:
            print(f"  âœ… ãƒ‡ãƒ¼ã‚¿ã‚’{device}ã«è»¢é€å®Œäº†")
            print(f"  ğŸ“Š input_ids device: {input_ids.device}")
            print(f"  ğŸ“Š sentiment_true device: {sentiment_true.device}")
        
        # å‹¾é…ã‚’ã‚¼ãƒ­åŒ–
        optimizer.zero_grad()
        
        # äºˆæ¸¬
        if batch_idx == 0:
            print(f"  ğŸ”® ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬é–‹å§‹...", flush=True)
            print(f"  ğŸ“Š input_ids shape: {input_ids.shape}")
            print(f"  ğŸ“Š attention_mask shape: {attention_mask.shape}")
            print(f"  ğŸ“Š model device: {next(model.parameters()).device}")
        
        try:
            # ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«trainãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            if batch_idx == 0:
                print(f"  ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã‚’trainãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š...", flush=True)
                model.train()
                print(f"  âœ… trainãƒ¢ãƒ¼ãƒ‰è¨­å®šå®Œäº†", flush=True)
                
                # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
                print(f"  ğŸ” ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª...", flush=True)
                cpu_params = []
                for name, param in model.named_parameters():
                    if not param.is_cuda:
                        cpu_params.append(name)
                if cpu_params:
                    print(f"  âš ï¸ CPUä¸Šã«ã‚ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {cpu_params[:3]}...", flush=True)
                    print(f"  ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã‚’å†ã³GPUã«è»¢é€...", flush=True)
                    model.to(device)
                    print(f"  âœ… GPUè»¢é€å®Œäº†", flush=True)
                else:
                    print(f"  âœ… å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒGPUä¸Šã«ã‚ã‚Šã¾ã™", flush=True)
                
                # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å‹ç¢ºèªãƒ»ä¿®æ­£
                print(f"  ğŸ” å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å‹ç¢ºèª...", flush=True)
                print(f"  ğŸ“Š input_ids dtype: {input_ids.dtype}")
                print(f"  ğŸ“Š attention_mask dtype: {attention_mask.dtype}")
                
                # å‹ã‚’æ˜ç¤ºçš„ã«longã«å¤‰æ›
                input_ids = input_ids.long()
                attention_mask = attention_mask.long()
                print(f"  âœ… å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’longå‹ã«å¤‰æ›å®Œäº†", flush=True)
            
            # forwardå®Ÿè¡Œ
            if batch_idx == 0:
                print(f"  ğŸš€ forwardå®Ÿè¡Œé–‹å§‹...", flush=True)
                print(f"  ğŸ“Š input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}")
                print(f"  ğŸ“Š attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}")
            
            with torch.cuda.amp.autocast(enabled=USE_AMP):
                sentiment_pred, course_pred = model(input_ids, attention_mask, chunk_mask)
            
            if batch_idx == 0:
                print(f"  âœ… ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å®Œäº†", flush=True)
                print(f"  ğŸ“Š sentiment_pred shape: {sentiment_pred.shape}")
                print(f"  ğŸ“Š course_pred shape: {course_pred.shape}")
                print(f"  ğŸ“Š sentiment_pred device: {sentiment_pred.device}")
                print(f"  ğŸ“Š sentiment_pred dtype: {sentiment_pred.dtype}")
        except Exception as e:
            print(f"  âŒ ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}", flush=True)
            print(f"  ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°:", flush=True)
            import traceback
            traceback.print_exc()
            print(f"  ğŸ›‘ å­¦ç¿’ã‚’åœæ­¢ã—ã¾ã™", flush=True)
            break
        
        # æå¤±è¨ˆç®—
        with torch.cuda.amp.autocast(enabled=USE_AMP):
            sentiment_loss = criterion(sentiment_pred, sentiment_true)
            course_loss = criterion(course_pred, course_true)
            loss = ALPHA * sentiment_loss + BETA * course_loss
        
        if batch_idx == 0:
            print(f"  ğŸ“Š æå¤±è¨ˆç®—å®Œäº†: {loss.item():.4f}")
        
        # é€†ä¼æ’­
        if batch_idx == 0:
            print(f"  ğŸ”„ é€†ä¼æ’­é–‹å§‹...", flush=True)
        try:
            # å‹¾é…è“„ç©
            loss_acc = loss / ACCUM_STEPS
            scaler.scale(loss_acc).backward()
            if batch_idx == 0:
                print(f"  âœ… é€†ä¼æ’­å®Œäº†", flush=True)
        except Exception as e:
            print(f"  âŒ é€†ä¼æ’­ã‚¨ãƒ©ãƒ¼: {e}", flush=True)
            import traceback
            traceback.print_exc()
            break
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        if batch_idx == 0:
            print(f"  âœ‚ï¸ å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°é–‹å§‹...", flush=True)
        # å‹¾é…æ›´æ–°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®ã¿ã‚¯ãƒªãƒƒãƒ—ã¨step
        if (batch_idx + 1) % ACCUM_STEPS == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        if batch_idx == 0:
            print(f"  âœ… å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å®Œäº†", flush=True)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        if batch_idx == 0:
            print(f"  ğŸ”§ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°é–‹å§‹...", flush=True)
        try:
            if (batch_idx + 1) % ACCUM_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
                if batch_idx == 0:
                    print(f"  âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°å®Œäº†", flush=True)
                    print(f"  ğŸ‰ æœ€åˆã®ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼å­¦ç¿’ã‚’ç¶™ç¶šã—ã¾ã™...", flush=True)
        except Exception as e:
            print(f"  âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", flush=True)
            import traceback
            traceback.print_exc()
            break
        
        # æå¤±ã®è¨˜éŒ²
        total_loss += loss.item()
        sentiment_losses += sentiment_loss.item()
        course_losses += course_loss.item()
        
        # æ¯ç§’æ›´æ–°ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º
        current_time = time.time()
        if current_time - last_update_time >= 1.0:  # 1ç§’ã”ã¨ã«æ›´æ–°
            print_progress_gauge(
                batch_idx + 1, len(train_loader),
                f"ğŸ”¥ Epoch {epoch+1}/{num_epochs}",
                f"Loss: {loss.item():.4f} | Sent: {sentiment_loss.item():.4f} | Course: {course_loss.item():.4f}",
                True, True
            )
            last_update_time = current_time
    
    # å­¦ç¿’ç‡ã®èª¿æ•´
    scheduler.step()
    
    avg_loss = total_loss / len(train_loader)
    avg_sentiment_loss = sentiment_losses / len(train_loader)
    avg_course_loss = course_losses / len(train_loader)
    
    return avg_loss, avg_sentiment_loss, avg_course_loss


def validate(model, val_loader):
    """æ¤œè¨¼"""
    model.eval()
    total_loss = 0
    sentiment_losses = 0
    course_losses = 0
    
    sentiment_preds_list = []
    sentiment_true_list = []
    course_preds_list = []
    course_true_list = []
    
    criterion = nn.MSELoss()
    last_update_time = time.time()
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
            input_ids = batch['input_ids'].to(device, non_blocking=NON_BLOCKING)
            attention_mask = batch['attention_mask'].to(device, non_blocking=NON_BLOCKING)
            chunk_mask = batch['chunk_mask'].to(device, non_blocking=NON_BLOCKING)
            sentiment_true = batch['sentiment_score'].to(device, non_blocking=NON_BLOCKING)
            course_true = batch['course_score'].to(device, non_blocking=NON_BLOCKING)
            
            # äºˆæ¸¬
            with torch.cuda.amp.autocast(enabled=USE_AMP):
                sentiment_pred, course_pred = model(input_ids, attention_mask, chunk_mask)
            
            # æå¤±è¨ˆç®—
            sentiment_loss = criterion(sentiment_pred, sentiment_true)
            course_loss = criterion(course_pred, course_true)
            loss = ALPHA * sentiment_loss + BETA * course_loss
            
            # æå¤±ã®è¨˜éŒ²
            total_loss += loss.item()
            sentiment_losses += sentiment_loss.item()
            course_losses += course_loss.item()
            
            # äºˆæ¸¬å€¤ã®è¨˜éŒ²
            sentiment_preds_list.extend(sentiment_pred.cpu().numpy())
            sentiment_true_list.extend(sentiment_true.cpu().numpy())
            course_preds_list.extend(course_pred.cpu().numpy())
            course_true_list.extend(course_true.cpu().numpy())
            
            # æ¯ç§’æ›´æ–°ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œè¨¼é€²æ—è¡¨ç¤º
            current_time = time.time()
            if current_time - last_update_time >= 1.0:  # 1ç§’ã”ã¨ã«æ›´æ–°
                print_progress_gauge(
                    batch_idx + 1, len(val_loader),
                    "âœ… Validation",
                    f"Loss: {loss.item():.4f} | Sent: {sentiment_loss.item():.4f} | Course: {course_loss.item():.4f}",
                    True, True
                )
                last_update_time = current_time
    
    avg_loss = total_loss / len(val_loader)
    avg_sentiment_loss = sentiment_losses / len(val_loader)
    avg_course_loss = course_losses / len(val_loader)
    
    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    sentiment_preds = np.array(sentiment_preds_list)
    sentiment_true = np.array(sentiment_true_list)
    course_preds = np.array(course_preds_list)
    course_true = np.array(course_true_list)
    
    sentiment_r2 = r2_score(sentiment_true, sentiment_preds)
    sentiment_corr = np.corrcoef(sentiment_true, sentiment_preds)[0, 1]
    
    course_r2 = r2_score(course_true, course_preds)
    course_corr = np.corrcoef(course_true, course_preds)[0, 1]
    
    return avg_loss, avg_sentiment_loss, avg_course_loss, sentiment_r2, sentiment_corr, course_r2, course_corr


def train_model(model, train_loader, val_loader, num_epochs):
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    print("\n" + "="*60)
    print("ğŸš€ æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã‚’é–‹å§‹")
    print("="*60)
    
    # GPUåˆæœŸçŠ¶æ³ã®è¡¨ç¤º
    if torch.cuda.is_available():
        gpu_status = get_gpu_status()
        if gpu_status:
            print(f"ğŸš€ GPUåˆæœŸçŠ¶æ³:")
            print(f"   ğŸ“Š ãƒ¡ãƒ¢ãƒª: {gpu_status['allocated']:.1f}GB / {gpu_status['total']:.1f}GB")
            print(f"   ğŸ”’ äºˆç´„æ¸ˆã¿: {gpu_status['reserved']:.1f}GB")
            print(f"   ğŸ“ˆ ä½¿ç”¨ç‡: {gpu_status['utilization']:.1f}%")
    else:
        print("âš ï¸  GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã§å®Ÿè¡Œä¸­...")
    
    # é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = datetime.now()
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)
    scheduler = torch.optim.lr_scheduler.LinearLR(
        optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs
    )
    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)
    
    # å­¦ç¿’å±¥æ­´
    history = {
        'train_loss': [],
        'train_sentiment_loss': [],
        'train_course_loss': [],
        'val_loss': [],
        'val_sentiment_loss': [],
        'val_course_loss': [],
        'val_sentiment_r2': [],
        'val_sentiment_corr': [],
        'val_course_r2': [],
        'val_course_corr': []
    }
    
    best_val_loss = float('inf')
    best_model_state = None
    
    for epoch in range(num_epochs):
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Epoch {epoch+1}/{num_epochs} é–‹å§‹")
        print(f"{'='*60}")
        
        # å­¦ç¿’
        print(f"\nğŸ”¥ å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹...")
        train_loss, train_sent_loss, train_course_loss = train_epoch(
            model, train_loader, optimizer, scheduler, scaler, epoch, num_epochs
        )
        
        # æ¤œè¨¼
        print(f"\nâœ… æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹...")
        val_loss, val_sent_loss, val_course_loss, sent_r2, sent_corr, course_r2, course_corr = validate(
            model, val_loader
        )
        
        # å±¥æ­´ã®è¨˜éŒ²
        history['train_loss'].append(train_loss)
        history['train_sentiment_loss'].append(train_sent_loss)
        history['train_course_loss'].append(train_course_loss)
        history['val_loss'].append(val_loss)
        history['val_sentiment_loss'].append(val_sent_loss)
        history['val_course_loss'].append(val_course_loss)
        history['val_sentiment_r2'].append(sent_r2)
        history['val_sentiment_corr'].append(sent_corr)
        history['val_course_r2'].append(course_r2)
        history['val_course_corr'].append(course_corr)
        
        # ã‚²ãƒ¼ã‚¸é¢¨ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        current_lr = optimizer.param_groups[0]['lr']
        elapsed_time = (datetime.now() - start_time).total_seconds()
        
        print_epoch_summary(
            epoch + 1, num_epochs, train_loss, val_loss, 
            sent_r2, course_r2, best_val_loss, current_lr, elapsed_time
        )
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model.load_state_dict(best_model_state)
    
    return model, history


def evaluate_model(model, test_loader, sentiment_scaler, course_scaler):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("\n" + "="*60)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡")
    print("="*60)
    
    model.eval()
    
    sentiment_preds_list = []
    sentiment_true_list = []
    course_preds_list = []
    course_true_list = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            input_ids = batch['input_ids'].to(device, non_blocking=NON_BLOCKING)
            attention_mask = batch['attention_mask'].to(device, non_blocking=NON_BLOCKING)
            chunk_mask = batch['chunk_mask'].to(device, non_blocking=NON_BLOCKING)
            sentiment_true = batch['sentiment_score'].to(device, non_blocking=NON_BLOCKING)
            course_true = batch['course_score'].to(device, non_blocking=NON_BLOCKING)
            
            # äºˆæ¸¬
            with torch.cuda.amp.autocast(enabled=USE_AMP):
                sentiment_pred, course_pred = model(input_ids, attention_mask, chunk_mask)
            
            # äºˆæ¸¬å€¤ã®è¨˜éŒ²
            sentiment_preds_list.extend(sentiment_pred.cpu().numpy())
            sentiment_true_list.extend(sentiment_true.cpu().numpy())
            course_preds_list.extend(course_pred.cpu().numpy())
            course_true_list.extend(course_true.cpu().numpy())
            
            # é€²æ—è¡¨ç¤ºï¼ˆã‚²ãƒ¼ã‚¸é¢¨ï¼‰
            if batch_idx % 2 == 0:
                print_progress_gauge(
                    batch_idx + 1, len(test_loader),
                    "ğŸ“Š ãƒ†ã‚¹ãƒˆè©•ä¾¡",
                    "",
                    True
                )
    
    # numpyé…åˆ—ã«å¤‰æ›
    sentiment_preds = np.array(sentiment_preds_list)
    sentiment_true = np.array(sentiment_true_list)
    course_preds = np.array(course_preds_list)
    course_true = np.array(course_true_list)
    
    # æ­£è¦åŒ–ã‚’æˆ»ã™
    sentiment_preds_original = sentiment_scaler.inverse_transform(sentiment_preds.reshape(-1, 1)).flatten()
    sentiment_true_original = sentiment_scaler.inverse_transform(sentiment_true.reshape(-1, 1)).flatten()
    course_preds_original = course_scaler.inverse_transform(course_preds.reshape(-1, 1)).flatten()
    course_true_original = course_scaler.inverse_transform(course_true.reshape(-1, 1)).flatten()
    
    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    results = {
        'sentiment': {
            'rmse': float(np.sqrt(mean_squared_error(sentiment_true_original, sentiment_preds_original))),
            'mae': float(mean_absolute_error(sentiment_true_original, sentiment_preds_original)),
            'r2': float(r2_score(sentiment_true_original, sentiment_preds_original)),
            'correlation': float(np.corrcoef(sentiment_true_original, sentiment_preds_original)[0, 1])
        },
        'course': {
            'rmse': float(np.sqrt(mean_squared_error(course_true_original, course_preds_original))),
            'mae': float(mean_absolute_error(course_true_original, course_preds_original)),
            'r2': float(r2_score(course_true_original, course_preds_original)),
            'correlation': float(np.corrcoef(course_true_original, course_preds_original)[0, 1])
        }
    }
    
    # çµæœè¡¨ç¤º
    print("\næ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®çµæœ:")
    print(f"  RMSE: {results['sentiment']['rmse']:.4f}")
    print(f"  MAE: {results['sentiment']['mae']:.4f}")
    print(f"  RÂ²: {results['sentiment']['r2']:.4f}")
    print(f"  ç›¸é–¢ä¿‚æ•°: {results['sentiment']['correlation']:.4f}")
    
    print("\næˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®çµæœ:")
    print(f"  RMSE: {results['course']['rmse']:.4f}")
    print(f"  MAE: {results['course']['mae']:.4f}")
    print(f"  RÂ²: {results['course']['r2']:.4f}")
    print(f"  ç›¸é–¢ä¿‚æ•°: {results['course']['correlation']:.4f}")
    
    return results, sentiment_preds_original, sentiment_true_original, course_preds_original, course_true_original


def save_results(model, history, results, timestamp):
    """çµæœã®ä¿å­˜"""
    print("\n" + "="*60)
    print("ğŸ’¾ çµæœã®ä¿å­˜")
    print("="*60)
    
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ï¼‰
    output_dir = os.path.join('02_ãƒ¢ãƒ‡ãƒ«', 'æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«')
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    model_path = os.path.join(output_dir, 'best_class_level_multitask_model.pth')
    torch.save(model.state_dict(), model_path)
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_path}")
    
    # è¨­å®šã®ä¿å­˜
    config = {
        'model_type': 'ClassLevelMultitaskModel',
        'base_model': BASE_MODEL,
        'max_length': MAX_LENGTH,
        'batch_size': BATCH_SIZE,
        'learning_rate': LEARNING_RATE,
        'num_epochs': NUM_EPOCHS,
        'alpha': ALPHA,
        'beta': BETA,
        'data_level': 'class_level',
        'data_size': 3268
    }
    
    config_path = os.path.join(output_dir, 'model_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"è¨­å®šã‚’ä¿å­˜: {config_path}")
    
    # çµæœã®ä¿å­˜ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ï¼‰
    results_dir = os.path.join('03_åˆ†æçµæœ', 'æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’')
    os.makedirs(results_dir, exist_ok=True)
    
    results_data = {
        'timestamp': timestamp,
        'data_level': 'class_level',
        'data_size': 3268,
        'results': results,
        'training_history': history
    }
    
    results_path = os.path.join(results_dir, f'class_level_multitask_results_{timestamp}.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    print(f"çµæœã‚’ä¿å­˜: {results_path}")


def create_visualizations(history, sentiment_preds, sentiment_true, course_preds, course_true, timestamp):
    """å¯è¦–åŒ–ã®ä½œæˆ"""
    print("\n" + "="*60)
    print("ğŸ“Š å¯è¦–åŒ–ã®ä½œæˆ")
    print("="*60)
    
    # 1. å­¦ç¿’æ›²ç·š
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Loss
    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')
    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training and Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # RÂ²ã‚¹ã‚³ã‚¢
    axes[0, 1].plot(history['val_sentiment_r2'], label='Sentiment R2', marker='o')
    axes[0, 1].plot(history['val_course_r2'], label='Course R2', marker='s')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('R2 Score')
    axes[0, 1].set_title('R2 Score Progress')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # ç›¸é–¢ä¿‚æ•°
    axes[1, 0].plot(history['val_sentiment_corr'], label='Sentiment Correlation', marker='o')
    axes[1, 0].plot(history['val_course_corr'], label='Course Correlation', marker='s')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Correlation')
    axes[1, 0].set_title('Correlation Progress')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # ã‚¿ã‚¹ã‚¯åˆ¥æå¤±
    axes[1, 1].plot(history['train_sentiment_loss'], label='Train Sentiment Loss', marker='o')
    axes[1, 1].plot(history['train_course_loss'], label='Train Course Loss', marker='s')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].set_title('Task-wise Training Loss')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    results_dir = os.path.join('03_åˆ†æçµæœ', 'æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’')
    os.makedirs(results_dir, exist_ok=True)
    plt.savefig(os.path.join(results_dir, f'training_curves_{timestamp}.png'), dpi=300, bbox_inches='tight')
    print(f"å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    plt.close()
    
    # 2. äºˆæ¸¬vsçœŸå€¤ã®æ•£å¸ƒå›³
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
    axes[0].scatter(sentiment_true, sentiment_preds, alpha=0.6, s=20)
    axes[0].plot([sentiment_true.min(), sentiment_true.max()], 
                 [sentiment_true.min(), sentiment_true.max()], 
                 'r--', label='Perfect Prediction')
    axes[0].set_xlabel('True Sentiment Score')
    axes[0].set_ylabel('Predicted Sentiment Score')
    axes[0].set_title(f'Sentiment Score Prediction (RÂ²={r2_score(sentiment_true, sentiment_preds):.4f})')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢
    axes[1].scatter(course_true, course_preds, alpha=0.6, s=20)
    axes[1].plot([course_true.min(), course_true.max()], 
                 [course_true.min(), course_true.max()], 
                 'r--', label='Perfect Prediction')
    axes[1].set_xlabel('True Course Evaluation Score')
    axes[1].set_ylabel('Predicted Course Evaluation Score')
    axes[1].set_title(f'Course Score Prediction (RÂ²={r2_score(course_true, course_preds):.4f})')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, f'prediction_scatter_{timestamp}.png'), dpi=300, bbox_inches='tight')
    print(f"æ•£å¸ƒå›³ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    plt.close()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        # å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é©åˆ‡ã«è¨­å®š
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)  # 00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.chdir(project_root)
        print(f"ğŸ“ å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
        
        print("\n" + "="*60)
        print("ğŸ¯ æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’")
        print("="*60)
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # PyTorchã¨DirectMLã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º
        print(f"\nğŸ§  PyTorch version: {torch.__version__}")
        try:
            import torch_directml as dml
            print(f"ğŸ§© DirectML available: {dml.is_available()}")
            if torch.__version__.startswith("2.4"):
                print("âš ï¸ PyTorch 2.4.x ã¯ DirectML ã¨äº’æ›æ€§ã®å•é¡ŒãŒã‚ã‚Šã¾ã™")
                print("   æ¨å¥¨: PyTorch 2.2.2 + torch-directml 0.2.3.dev240715")
        except ImportError:
            print("â„¹ï¸ DirectML ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        print("\nğŸ”§ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–...")
        try:
            tokenizer = BertJapaneseTokenizer.from_pretrained(BASE_MODEL)
            print("âœ… æ—¥æœ¬èªBERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            # é•·æ–‡ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚’è‡ªå‰ã§è¡Œã†ãŸã‚ã€è­¦å‘ŠæŠ‘åˆ¶ç”¨ã«ãƒ¢ãƒ‡ãƒ«é•·ã‚’æ‹¡å¼µ
            try:
                tokenizer.model_max_length = 10**6
            except Exception:
                pass
        except Exception as e:
            print(f"âš ï¸  æ—¥æœ¬èªBERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            print("ğŸ”§ ä»£æ›¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™...")
            try:
                # ä»£æ›¿ã¨ã—ã¦AutoTokenizerã‚’ä½¿ç”¨
                from transformers import AutoTokenizer
                tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
                print("âœ… ä»£æ›¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            except Exception as e2:
                print(f"âŒ ä»£æ›¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚‚å¤±æ•—: {e2}")
                print("ğŸ’¡ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
                print("   pip install fugashi ipadic unidic-lite")
                raise e2
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        texts, sentiment_scores, course_scores = load_data()
        
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        train_loader, val_loader, test_loader, sentiment_scaler, course_scaler = prepare_data(
            texts, sentiment_scores, course_scores, tokenizer
        )
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        print("\nğŸ”§ ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–...")
        print(f"ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {BASE_MODEL}")
        
        try:
            model = ClassLevelMultitaskModel(BASE_MODEL)
            print(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            
            print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ä¸­...")
            model = model.to(device)
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«GPUç§»å‹•å®Œäº†")
            if WARMUP_FORWARD:
                # äº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›forwardã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«/åˆæœŸåŒ–å¾…ã¡ã‚’å…ˆã«æ¶ˆåŒ–ï¼‰
                try:
                    model.eval()
                    with torch.no_grad():
                        dummy_ids = torch.zeros((1, 1, CHUNK_LEN), dtype=torch.long, device=device)
                        dummy_mask = torch.zeros((1, 1, CHUNK_LEN), dtype=torch.long, device=device)
                        dummy_cmask = torch.tensor([[1]], dtype=torch.bool, device=device)
                        _ = model(dummy_ids, dummy_mask, dummy_cmask)
                    model.train()
                    print("ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—forwardå®Œäº†")
                except Exception as _e:
                    print(f"â„¹ï¸ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—forwardå¤±æ•—: {_e}")

            # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§VRAMç¯€ç´„
            if USE_GRADIENT_CHECKPOINTING:
                try:
                    model.bert.gradient_checkpointing_enable()
                    print("ğŸ§  Gradient Checkpointing æœ‰åŠ¹åŒ–")
                except Exception as _e:
                    print(f"â„¹ï¸ Gradient Checkpointing ç„¡åŠ¹ï¼ˆéå¯¾å¿œï¼‰: {_e}")
            
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
            print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # å­¦ç¿’
        model, history = train_model(model, train_loader, val_loader, NUM_EPOCHS)
        
        # è©•ä¾¡
        results, sentiment_preds, sentiment_true, course_preds, course_true = evaluate_model(
            model, test_loader, sentiment_scaler, course_scaler
        )
        
        # çµæœã®ä¿å­˜
        save_results(model, history, results, timestamp)
        
        # å¯è¦–åŒ–
        create_visualizations(history, sentiment_preds, sentiment_true, 
                            course_preds, course_true, timestamp)
        
        print("\n" + "="*60)
        print("âœ… æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("="*60)
        print(f"çµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
        print("\nğŸ“Š æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼")
        print("="*60)
        print("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬:")
        print(f"  RÂ²: {results['sentiment']['r2']:.4f}")
        print(f"  ç›¸é–¢ä¿‚æ•°: {results['sentiment']['correlation']:.4f}")
        print(f"  RMSE: {results['sentiment']['rmse']:.4f}")
        
        print("\næˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬:")
        print(f"  RÂ²: {results['course']['r2']:.4f}")
        print(f"  ç›¸é–¢ä¿‚æ•°: {results['course']['correlation']:.4f}")
        print(f"  RMSE: {results['course']['rmse']:.4f}")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

