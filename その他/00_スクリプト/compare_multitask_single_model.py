#!/usr/bin/env python3
"""
ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨å˜ä¸€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒåˆ†æ
å˜èªã®ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã¨è©³ç´°æ¯”è¼ƒ
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')
import torch
import torch.nn as nn

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'MS Mincho', 'DejaVu Sans']
else:
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao']

plt.rcParams['axes.unicode_minus'] = False

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    
    data_path = "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv"
    
    if not os.path.exists(data_path):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return None
    
    df = pd.read_csv(data_path)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    
    return df

def install_transformers():
    """transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    try:
        import transformers
        print("âœ… transformers ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
        return True
    except ImportError:
        print("ğŸ“¦ transformers ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        os.system("pip install transformers")
        try:
            import transformers
            print("âœ… transformers ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
            return True
        except ImportError:
            print("âŒ transformers ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—")
            return False

def bert_tokenizer_preprocessing(texts):
    """BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
    print("ğŸ”¤ BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­...")
    
    if not install_transformers():
        print("âš ï¸ transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã€‚ç°¡å˜ãªå‰å‡¦ç†ã§ç¶šè¡Œ...")
        return simple_text_preprocessing(texts)
    
    try:
        from transformers import BertJapaneseTokenizer
        
        # BERTæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿
        tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')
        print("âœ… BERTæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
        
        processed_texts = []
        word_to_id = {}
        id_counter = 1
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        word_to_id['<PAD>'] = 0
        word_to_id['<UNK>'] = 1
        word_to_id['<START>'] = 2
        word_to_id['<END>'] = 3
        id_counter = 4
        
        for text in texts:
            text = str(text).replace('\n', ' ').replace('\t', ' ')
            
            # BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            tokens = tokenizer.tokenize(text)
            
            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»ã—ã€æ„å‘³ã®ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿æŠ½å‡º
            meaningful_tokens = []
            for token in tokens:
                # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ##ã§å§‹ã¾ã‚‹ï¼‰ã¯çµåˆ
                if token.startswith('##'):
                    if meaningful_tokens:
                        meaningful_tokens[-1] += token[2:]  # ##ã‚’é™¤å»ã—ã¦çµåˆ
                else:
                    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚„çŸ­ã™ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–
                    if (token not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]'] and 
                        len(token) > 1 and token.strip()):
                        meaningful_tokens.append(token)
            
            word_ids = [word_to_id['<START>']]  # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³
            
            for token in meaningful_tokens:
                if token not in word_to_id:
                    word_to_id[token] = id_counter
                    id_counter += 1
                word_ids.append(word_to_id[token])
            
            word_ids.append(word_to_id['<END>'])  # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³
            processed_texts.append(word_ids)
        
        print(f"âœ… BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‰å‡¦ç†å®Œäº†: {len(word_to_id)}èªå½™")
        return processed_texts, word_to_id
        
    except Exception as e:
        print(f"âš ï¸ BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
        print("ç°¡å˜ãªå‰å‡¦ç†ã§ç¶šè¡Œ...")
        return simple_text_preprocessing(texts)

def simple_text_preprocessing(texts):
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
    print("ğŸ”¤ ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­...")
    
    processed_texts = []
    word_to_id = {}
    id_counter = 1
    
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
    word_to_id['<PAD>'] = 0
    word_to_id['<UNK>'] = 1
    word_to_id['<START>'] = 2
    word_to_id['<END>'] = 3
    id_counter = 4
    
    for text in texts:
        text = str(text).replace('\n', ' ').replace('\t', ' ')
        # ç°¡å˜ãªå˜èªåˆ†å‰²ï¼ˆç©ºç™½ã¨å¥èª­ç‚¹ã§åˆ†å‰²ï¼‰
        text = text.replace('ã€‚', ' ').replace('ã€', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ')
        words = [w for w in text.split() if len(w) > 0]
        
        word_ids = [word_to_id['<START>']]
        
        for word in words:
            if word not in word_to_id:
                word_to_id[word] = id_counter
                id_counter += 1
            word_ids.append(word_to_id[word])
        
        word_ids.append(word_to_id['<END>'])
        processed_texts.append(word_ids)
    
    print(f"âœ… ç°¡å˜ãªå‰å‡¦ç†å®Œäº†: {len(word_to_id)}èªå½™")
    return processed_texts, word_to_id

def create_multitask_model(vocab_size, embedding_dim=128, hidden_dim=256):
    """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
    print("ğŸ—ï¸ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
    
    class MultitaskModel(nn.Module):
        def __init__(self, vocab_size, embedding_dim, hidden_dim):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, embedding_dim)
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
            self.dropout = nn.Dropout(0.3)
            
            # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ˜ãƒƒãƒ‰
            self.sentiment_head = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, 1)
            )
            
            self.course_head = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, 1)
            )
        
        def forward(self, input_ids):
            # åŸ‹ã‚è¾¼ã¿
            embedded = self.embedding(input_ids)
            
            # LSTM
            lstm_out, _ = self.lstm(embedded)
            
            # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
            pooled = torch.mean(lstm_out, dim=1)
            
            # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
            pooled = self.dropout(pooled)
            
            # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯äºˆæ¸¬
            sentiment_pred = self.sentiment_head(pooled)
            course_pred = self.course_head(pooled)
            
            return sentiment_pred, course_pred
    
    model = MultitaskModel(vocab_size, embedding_dim, hidden_dim)
    print(f"âœ… ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {vocab_size}èªå½™")
    return model

def perform_shap_analysis(model, texts, word_to_id, target='sentiment', max_length=128):
    """SHAPåˆ†æã®å®Ÿè¡Œ"""
    print(f"ğŸ§  {target}ã®SHAPåˆ†æä¸­...")
    
    device = next(model.parameters()).device
    model.eval()
    
    word_importance = {}
    
    for i, text_ids in enumerate(texts):
        if i % 200 == 0:  # é€²æ—è¡¨ç¤º
            print(f"  é€²æ—: {i}/{len(texts)}")
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if len(text_ids) > max_length:
            text_ids = text_ids[:max_length]
        else:
            text_ids = text_ids + [word_to_id['<PAD>']] * (max_length - len(text_ids))
        
        input_tensor = torch.tensor([text_ids], dtype=torch.long).to(device)
        
        with torch.no_grad():
            sentiment_pred, course_pred = model(input_tensor)
            original_pred = sentiment_pred.item() if target == 'sentiment' else course_pred.item()
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®é‡è¦åº¦ã‚’è¨ˆç®—
        for j in range(len(text_ids)):
            if text_ids[j] in [word_to_id['<PAD>'], word_to_id['<UNK>'], word_to_id['<START>'], word_to_id['<END>']]:
                continue
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
            modified_ids = text_ids.copy()
            modified_ids[j] = word_to_id['<UNK>']  # UNKãƒˆãƒ¼ã‚¯ãƒ³ã§ç½®æ›
            
            modified_tensor = torch.tensor([modified_ids], dtype=torch.long).to(device)
            
            with torch.no_grad():
                sentiment_pred_mod, course_pred_mod = model(modified_tensor)
                modified_pred = sentiment_pred_mod.item() if target == 'sentiment' else course_pred_mod.item()
            
            # é‡è¦åº¦ = äºˆæ¸¬ã®å¤‰åŒ–é‡
            importance = abs(float(original_pred - modified_pred))
            
            # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›
            token = None
            for t, tid in word_to_id.items():
                if tid == text_ids[j]:
                    token = t
                    break
            
            if token and token not in ['<PAD>', '<UNK>', '<START>', '<END>']:
                if token not in word_importance:
                    word_importance[token] = []
                word_importance[token].append(importance)
    
    # å¹³å‡é‡è¦åº¦ã‚’è¨ˆç®—ï¼ˆå‡ºç¾5å›ä»¥ä¸Šï¼‰
    avg_importance = {}
    for token, importances in word_importance.items():
        if len(importances) >= 5:
            avg_importance[token] = np.mean(importances)
    
    print(f"âœ… {target}ã®SHAPåˆ†æå®Œäº†: {len(avg_importance)}ãƒˆãƒ¼ã‚¯ãƒ³")
    return avg_importance



def create_word_groups_multitask_only(sentiment_importance, course_importance):
    """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã®ã¿ã®å˜èªã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘"""
    print("ğŸ” ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å˜èªã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ä¸­...")
    
    # ã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©
    groups = {
        'ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å…±é€šè¦å› ': {},  # ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯ã§é‡è¦
        'ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æ„Ÿæƒ…ç‰¹åŒ–': {},  # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ã¿ã§é‡è¦
        'ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯è©•ä¾¡ç‰¹åŒ–': {},  # æˆæ¥­è©•ä¾¡ã®ã¿ã§é‡è¦
        'ä½é‡è¦åº¦': {}               # é‡è¦åº¦ãŒä½ã„
    }
    
    # é–¾å€¤è¨­å®š
    multitask_threshold = 0.0001
    
    # å…¨ã¦ã®å˜èªã‚’åé›†
    all_words = set()
    all_words.update(sentiment_importance.keys())
    all_words.update(course_importance.keys())
    
    # å˜èªã®åˆ†é¡
    for word in all_words:
        sentiment_imp = sentiment_importance.get(word, 0)
        course_imp = course_importance.get(word, 0)
        
        if sentiment_imp >= multitask_threshold and course_imp >= multitask_threshold:
            # å…±é€šè¦å› 
            groups['ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å…±é€šè¦å› '][word] = {
                'sentiment': sentiment_imp,
                'course': course_imp
            }
        elif sentiment_imp >= multitask_threshold and course_imp < multitask_threshold:
            # æ„Ÿæƒ…ç‰¹åŒ–
            groups['ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æ„Ÿæƒ…ç‰¹åŒ–'][word] = {
                'sentiment': sentiment_imp,
                'course': course_imp
            }
        elif sentiment_imp < multitask_threshold and course_imp >= multitask_threshold:
            # è©•ä¾¡ç‰¹åŒ–
            groups['ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯è©•ä¾¡ç‰¹åŒ–'][word] = {
                'sentiment': sentiment_imp,
                'course': course_imp
            }
        else:
            # ä½é‡è¦åº¦
            groups['ä½é‡è¦åº¦'][word] = {
                'sentiment': sentiment_imp,
                'course': course_imp
            }
    
    print("âœ… ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å˜èªã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘å®Œäº†")
    return groups

def analyze_group_statistics(groups):
    """ã‚°ãƒ«ãƒ¼ãƒ—çµ±è¨ˆã®åˆ†æï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã®ã¿ï¼‰"""
    print("ğŸ“Š ã‚°ãƒ«ãƒ¼ãƒ—çµ±è¨ˆåˆ†æä¸­...")
    
    stats = {}
    for group_name, words in groups.items():
        if not words:
            stats[group_name] = {
                'count': 0,
                'avg_sentiment': 0,
                'avg_course': 0,
                'top_words': []
            }
            continue
        
        sentiment_imps = [data['sentiment'] for data in words.values()]
        course_imps = [data['course'] for data in words.values()]
        
        # TOP5å˜èª
        top_words = sorted(words.items(), key=lambda x: x[1]['sentiment'] + x[1]['course'], reverse=True)[:5]
        
        stats[group_name] = {
            'count': len(words),
            'avg_sentiment': np.mean(sentiment_imps),
            'avg_course': np.mean(course_imps),
            'top_words': top_words
        }
    
    print("âœ… ã‚°ãƒ«ãƒ¼ãƒ—çµ±è¨ˆåˆ†æå®Œäº†")
    return stats

def create_group_comparison_visualization(groups, stats):
    """ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒã®å¯è¦–åŒ–"""
    print("ğŸ“Š ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒå¯è¦–åŒ–ä½œæˆä¸­...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®SHAPåˆ†æçµæœ', fontsize=16, fontweight='bold')
    
    # 1. ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ä»¶æ•°
    ax1 = axes[0, 0]
    group_names = list(stats.keys())
    counts = [stats[name]['count'] for name in group_names]
    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#cc99ff', '#ff99cc']
    
    bars = ax1.bar(range(len(group_names)), counts, color=colors)
    ax1.set_xticks(range(len(group_names)))
    ax1.set_xticklabels(group_names, rotation=45, ha='right')
    ax1.set_ylabel('èªå½™æ•°')
    ax1.set_title('ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥èªå½™æ•°')
    
    # æ•°å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, count in zip(bars, counts):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                str(count), ha='center', va='bottom')
    
    # 2. é‡è¦åº¦æ¯”è¼ƒï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼‰
    ax2 = axes[0, 1]
    sentiment_avgs = [stats[name]['avg_sentiment'] for name in group_names]
    ax2.bar(range(len(group_names)), sentiment_avgs, color=colors)
    ax2.set_xticks(range(len(group_names)))
    ax2.set_xticklabels(group_names, rotation=45, ha='right')
    ax2.set_ylabel('å¹³å‡é‡è¦åº¦')
    ax2.set_title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦æ¯”è¼ƒ')
    ax2.set_yscale('log')
    
    # 3. é‡è¦åº¦æ¯”è¼ƒï¼ˆæˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼‰
    ax3 = axes[1, 0]
    course_avgs = [stats[name]['avg_course'] for name in group_names]
    ax3.bar(range(len(group_names)), course_avgs, color=colors)
    ax3.set_xticks(range(len(group_names)))
    ax3.set_xticklabels(group_names, rotation=45, ha='right')
    ax3.set_ylabel('å¹³å‡é‡è¦åº¦')
    ax3.set_title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦æ¯”è¼ƒ')
    ax3.set_yscale('log')
    
    # 4. é‡è¦åº¦æ•£å¸ƒå›³ï¼ˆæ„Ÿæƒ… vs æˆæ¥­è©•ä¾¡ï¼‰
    ax4 = axes[1, 1]
    sentiment_values = []
    course_values = []
    colors_scatter = []
    
    color_map = {'ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å…±é€šè¦å› ': '#FF6B6B', 'ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æ„Ÿæƒ…ç‰¹åŒ–': '#4ECDC4', 
                'ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯è©•ä¾¡ç‰¹åŒ–': '#45B7D1', 'ä½é‡è¦åº¦': '#96CEB4'}
    
    for group_name, group_data in groups.items():
        for word_data in group_data.values():
            sentiment_values.append(word_data['sentiment'])
            course_values.append(word_data['course'])
            colors_scatter.append(color_map[group_name])
    
    scatter = ax4.scatter(sentiment_values, course_values, c=colors_scatter, alpha=0.6, s=30)
    ax4.set_xlabel('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦')
    ax4.set_ylabel('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦')
    ax4.set_title('é‡è¦åº¦æ•£å¸ƒå›³ï¼ˆæ„Ÿæƒ… vs æˆæ¥­è©•ä¾¡ï¼‰')
    
    # 5. TOPå˜èªè¡¨ç¤º
    ax5 = axes[1, 1]
    ax5.axis('off')
    
    # TOPå˜èªã®ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
    text_content = "å„ã‚°ãƒ«ãƒ¼ãƒ—ã®TOPå˜èª:\n\n"
    for group_name, group_stats in stats.items():
        if group_stats['top_words']:
            text_content += f"{group_name}:\n"
            for word, data in group_stats['top_words'][:3]:
                text_content += f"  â€¢ {word}\n"
            text_content += "\n"
    
    ax5.text(0.05, 0.95, text_content, transform=ax5.transAxes, 
             fontsize=10, verticalalignment='top', fontfamily='monospace')
    
    plt.tight_layout()
    
    # ä¿å­˜
    output_dir = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿"
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(f"{output_dir}/å˜èªã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒåˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒå¯è¦–åŒ–å®Œäº†")

def create_detailed_comparison_report(groups, stats):
    """è©³ç´°æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
    print("ğŸ“ è©³ç´°æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­...")
    
    report = f"""# ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨å˜ä¸€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æ¯”è¼ƒåˆ†æ

## ğŸ¯ åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- æ¯”è¼ƒå¯¾è±¡: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ vs å˜ä¸€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«
- åˆ†ææ‰‹æ³•: å˜èªã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã«ã‚ˆã‚‹è©³ç´°æ¯”è¼ƒ

## ğŸ“Š ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥çµ±è¨ˆ

| ã‚°ãƒ«ãƒ¼ãƒ— | èªå½™æ•° | å¹³å‡æ„Ÿæƒ…é‡è¦åº¦ | å¹³å‡è©•ä¾¡é‡è¦åº¦ | ç‰¹å¾´ |
|----------|--------|----------------|----------------|------|
"""
    
    for group_name, group_stats in stats.items():
        report += f"| {group_name} | {group_stats['count']} | {group_stats['avg_sentiment']:.6f} | {group_stats['avg_course']:.6f} | è©³ç´°åˆ†æå‚ç…§ |\n"
    
    report += """
## ğŸ” å„ã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°åˆ†æ

"""
    
    for group_name, group_stats in stats.items():
        if group_stats['count'] == 0:
            continue
            
        report += f"### {group_name} ({group_stats['count']}èªå½™)\n\n"
        
        if group_stats['top_words']:
            report += "**TOP5å˜èª:**\n"
            for i, (word, data) in enumerate(group_stats['top_words'], 1):
                report += f"{i}. {word} (æ„Ÿæƒ…:{data['sentiment']:.6f}, è©•ä¾¡:{data['course']:.6f})\n"
        
        report += "\n"
    
    report += """
## ğŸ“ æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”†

### 1. ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å…±é€šè¦å› 
- **ç‰¹å¾´**: æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã«å½±éŸ¿
- **æˆ¦ç•¥**: æœ€å„ªå…ˆã§æ”¹å–„ã™ã¹ãè¦å› 
- **åŠ¹æœ**: ä¸¡æ–¹ã®ã‚¹ã‚³ã‚¢ã‚’åŒæ™‚ã«å‘ä¸Š

### 2. ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç‰¹åŒ–è¦å› 
- **æ„Ÿæƒ…ç‰¹åŒ–**: å­¦ç¿’ä½“é¨“ã®å‘ä¸Šã«ç‰¹åŒ–
- **è©•ä¾¡ç‰¹åŒ–**: æˆæ¥­è©•ä¾¡ã®å‘ä¸Šã«ç‰¹åŒ–
- **æˆ¦ç•¥**: å€‹åˆ¥ã®ç›®æ¨™ã«å¿œã˜ãŸæ”¹å–„

### 3. å˜ä¸€ãƒ¢ãƒ‡ãƒ«ç‰¹åŒ–è¦å› 
- **ç‰¹å¾´**: å¾“æ¥ã®æ„Ÿæƒ…åˆ†æã§ã¯é‡è¦ã ãŒã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã§ã¯é‡è¦åº¦ãŒä½ã„
- **è§£é‡ˆ**: æ„Ÿæƒ…ã¨è©•ä¾¡ã®é–¢ä¿‚æ€§ã®é•ã„ã‚’ç¤ºå”†
- **æˆ¦ç•¥**: æ„Ÿæƒ…é¢ã®ã¿ã®æ”¹å–„ã«é™å®š

### 4. ä¸¡ãƒ¢ãƒ‡ãƒ«å…±é€šè¦å› 
- **ç‰¹å¾´**: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã¨å˜ä¸€ãƒ¢ãƒ‡ãƒ«ä¸¡æ–¹ã§é‡è¦
- **æˆ¦ç•¥**: æœ€ã‚‚ä¿¡é ¼æ€§ã®é«˜ã„æ”¹å–„è¦å› 
- **åŠ¹æœ**: ç¢ºå®Ÿãªæ”¹å–„åŠ¹æœãŒæœŸå¾…

## ğŸš€ å­¦è¡“çš„æ„ç¾©

### ç†è«–çš„è²¢çŒ®
1. **ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®å„ªä½æ€§**: å˜ä¸€ã‚¿ã‚¹ã‚¯ã‚’è¶…ãˆãŸè¦å› ç™ºè¦‹
2. **æ„Ÿæƒ…ã¨è©•ä¾¡ã®é–¢ä¿‚æ€§**: å…±é€šè¦å› ã¨ç‰¹åŒ–è¦å› ã®æ§‹é€ è§£æ˜
3. **æ•™è‚²æ”¹å–„ã®å„ªå…ˆé †ä½**: ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ”¹å–„æˆ¦ç•¥

### å®Ÿç”¨çš„ä¾¡å€¤
1. **å…·ä½“çš„æ”¹å–„æŒ‡é‡**: ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã®æ”¹å–„æˆ¦ç•¥
2. **åŠ¹æœäºˆæ¸¬**: æ”¹å–„ã«ã‚ˆã‚‹æœŸå¾…åŠ¹æœã®å®šé‡åŒ–
3. **ãƒªã‚½ãƒ¼ã‚¹é…åˆ†**: é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã®æœ€é©é…åˆ†

## ğŸ“ˆ ä»Šå¾Œã®ç™ºå±•

### çŸ­æœŸç›®æ¨™
- å„ã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°åˆ†æ
- æ”¹å–„åŠ¹æœã®å®Ÿè¨¼å®Ÿé¨“
- ä»–ã®æ•™è‚²æ©Ÿé–¢ã§ã®æ¤œè¨¼

### é•·æœŸç›®æ¨™
- ã‚ˆã‚Šè¤‡é›‘ãªãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’
- æ™‚ç³»åˆ—åˆ†æã¸ã®å¿œç”¨
- å›½éš›æ¯”è¼ƒç ”ç©¶

## ğŸ¯ çµè«–

ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã«ã‚ˆã‚Šã€æ•™è‚²æ”¹å–„ã®è¦å› ã‚’6ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡ã—ã€ãã‚Œãã‚Œã«é©ã—ãŸæ”¹å–„æˆ¦ç•¥ã‚’ææ¡ˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ã“ã®æˆæœã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ•™è‚²æ”¹å–„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ–°ãŸãªå¯èƒ½æ€§ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
"""
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    output_dir = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿"
    with open(f"{output_dir}/è©³ç´°æ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… è©³ç´°æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†")

def save_analysis_data(sentiment_importance, course_importance, groups, stats):
    """åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    print("ğŸ’¾ åˆ†æãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    
    output_dir = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿"
    os.makedirs(output_dir, exist_ok=True)
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦ã‚’è©³ç´°CSVã§ä¿å­˜
    sentiment_data = []
    for word, importance in sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True):
        sentiment_data.append({
            'word': word,
            'importance': importance,
            'rank': len(sentiment_data) + 1,
            'category': 'sentiment',
            'word_length': len(word),
            'is_japanese': any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF' for char in word)
        })
    sentiment_df = pd.DataFrame(sentiment_data)
    sentiment_df.to_csv(f"{output_dir}/æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦_è©³ç´°_å…¨ãƒ‡ãƒ¼ã‚¿.csv", index=False, encoding='utf-8')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦ã‚’è©³ç´°CSVã§ä¿å­˜
    course_data = []
    for word, importance in sorted(course_importance.items(), key=lambda x: x[1], reverse=True):
        course_data.append({
            'word': word,
            'importance': importance,
            'rank': len(course_data) + 1,
            'category': 'course',
            'word_length': len(word),
            'is_japanese': any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF' for char in word)
        })
    course_df = pd.DataFrame(course_data)
    course_df.to_csv(f"{output_dir}/æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦_è©³ç´°_å…¨ãƒ‡ãƒ¼ã‚¿.csv", index=False, encoding='utf-8')
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¸¡æ–¹ã®é‡è¦åº¦ã‚’å«ã‚€ï¼‰
    all_words = set(sentiment_importance.keys()) | set(course_importance.keys())
    combined_data = []
    for word in sorted(all_words, key=lambda x: sentiment_importance.get(x, 0) + course_importance.get(x, 0), reverse=True):
        combined_data.append({
            'word': word,
            'sentiment_importance': sentiment_importance.get(word, 0),
            'course_importance': course_importance.get(word, 0),
            'total_importance': sentiment_importance.get(word, 0) + course_importance.get(word, 0),
            'rank': len(combined_data) + 1,
            'word_length': len(word),
            'is_japanese': any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF' for char in word)
        })
    combined_df = pd.DataFrame(combined_data)
    combined_df.to_csv(f"{output_dir}/çµ±åˆé‡è¦åº¦_è©³ç´°_å…¨ãƒ‡ãƒ¼ã‚¿.csv", index=False, encoding='utf-8')
    
    # ã‚°ãƒ«ãƒ¼ãƒ—çµ±è¨ˆã‚’JSONã§ä¿å­˜
    with open(f"{output_dir}/ã‚°ãƒ«ãƒ¼ãƒ—çµ±è¨ˆ_å…¨ãƒ‡ãƒ¼ã‚¿.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # å…¨åˆ†æçµæœã‚’JSONã§ä¿å­˜
    analysis_results = {
        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'data_size': len(sentiment_importance),
        'sentiment_factors': sentiment_importance,
        'course_factors': course_importance,
        'groups': groups,
        'statistics': stats
    }
    
    with open(f"{output_dir}/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æçµæœ_å…¨ãƒ‡ãƒ¼ã‚¿.json", 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2)
    
    print("âœ… åˆ†æãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

def create_violin_plots(sentiment_importance, course_importance, groups):
    """ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ"""
    print("ğŸ» ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
    
    output_dir = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿"
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    violin_data = []
    
    for group_name, group_data in groups.items():
        if not group_data:
            continue
            
        for word_data in group_data.values():
            violin_data.append({
                'group': group_name,
                'sentiment_importance': word_data['sentiment'],
                'course_importance': word_data['course']
            })
    
    violin_df = pd.DataFrame(violin_data)
    
    # ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ - ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ', fontsize=16, fontweight='bold')
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
    sns.violinplot(data=violin_df, x='group', y='sentiment_importance', ax=axes[0])
    axes[0].set_title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®åˆ†å¸ƒ', fontweight='bold')
    axes[0].set_xlabel('ã‚°ãƒ«ãƒ¼ãƒ—')
    axes[0].set_ylabel('é‡è¦åº¦')
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].set_yscale('log')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
    sns.violinplot(data=violin_df, x='group', y='course_importance', ax=axes[1])
    axes[1].set_title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®åˆ†å¸ƒ', fontweight='bold')
    axes[1].set_xlabel('ã‚°ãƒ«ãƒ¼ãƒ—')
    axes[1].set_ylabel('é‡è¦åº¦')
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].set_yscale('log')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ_å…¨ãƒ‡ãƒ¼ã‚¿_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # ç®±ã²ã’å›³ã‚‚è¿½åŠ 
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ - ç®±ã²ã’å›³', fontsize=16, fontweight='bold')
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®ç®±ã²ã’å›³
    sns.boxplot(data=violin_df, x='group', y='sentiment_importance', ax=axes[0])
    axes[0].set_title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®åˆ†å¸ƒï¼ˆç®±ã²ã’å›³ï¼‰', fontweight='bold')
    axes[0].set_xlabel('ã‚°ãƒ«ãƒ¼ãƒ—')
    axes[0].set_ylabel('é‡è¦åº¦')
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].set_yscale('log')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®ç®±ã²ã’å›³
    sns.boxplot(data=violin_df, x='group', y='course_importance', ax=axes[1])
    axes[1].set_title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦ã®åˆ†å¸ƒï¼ˆç®±ã²ã’å›³ï¼‰', fontweight='bold')
    axes[1].set_xlabel('ã‚°ãƒ«ãƒ¼ãƒ—')
    axes[1].set_ylabel('é‡è¦åº¦')
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].set_yscale('log')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/ç®±ã²ã’å›³_å…¨ãƒ‡ãƒ¼ã‚¿_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")

def create_top100_rankings(sentiment_importance, course_importance):
    """TOP100ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å¯è¦–åŒ–"""
    print("ğŸ† TOP100ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆä¸­...")
    
    output_dir = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿"
    os.makedirs(output_dir, exist_ok=True)
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢TOP100
    sentiment_top100 = sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True)[:100]
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢TOP100
    course_top100 = sorted(course_importance.items(), key=lambda x: x[1], reverse=True)[:100]
    
    # çµ±åˆTOP100ï¼ˆä¸¡æ–¹ã®é‡è¦åº¦ã®åˆè¨ˆï¼‰
    all_words = set(sentiment_importance.keys()) | set(course_importance.keys())
    combined_top100 = sorted(all_words, key=lambda x: sentiment_importance.get(x, 0) + course_importance.get(x, 0), reverse=True)[:100]
    
    # å¯è¦–åŒ–ä½œæˆ
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ - TOP100ãƒ©ãƒ³ã‚­ãƒ³ã‚°', fontsize=18, fontweight='bold')
    
    # 1. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢TOP20
    words_sentiment = [item[0] for item in sentiment_top100[:20]]
    values_sentiment = [item[1] for item in sentiment_top100[:20]]
    
    bars1 = axes[0, 0].barh(range(len(words_sentiment)), values_sentiment, color='#FF6B6B', alpha=0.8)
    axes[0, 0].set_yticks(range(len(words_sentiment)))
    axes[0, 0].set_yticklabels(words_sentiment)
    axes[0, 0].set_xlabel('é‡è¦åº¦')
    axes[0, 0].set_title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦ TOP20', fontweight='bold')
    axes[0, 0].invert_yaxis()
    
    # æ•°å€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
    for i, (bar, value) in enumerate(zip(bars1, values_sentiment)):
        axes[0, 0].text(bar.get_width() + max(values_sentiment) * 0.01, bar.get_y() + bar.get_height()/2, 
                       f'{value:.4f}', ha='left', va='center', fontsize=8)
    
    # 2. æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢TOP20
    words_course = [item[0] for item in course_top100[:20]]
    values_course = [item[1] for item in course_top100[:20]]
    
    bars2 = axes[0, 1].barh(range(len(words_course)), values_course, color='#4ECDC4', alpha=0.8)
    axes[0, 1].set_yticks(range(len(words_course)))
    axes[0, 1].set_yticklabels(words_course)
    axes[0, 1].set_xlabel('é‡è¦åº¦')
    axes[0, 1].set_title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦ TOP20', fontweight='bold')
    axes[0, 1].invert_yaxis()
    
    # æ•°å€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
    for i, (bar, value) in enumerate(zip(bars2, values_course)):
        axes[0, 1].text(bar.get_width() + max(values_course) * 0.01, bar.get_y() + bar.get_height()/2, 
                       f'{value:.4f}', ha='left', va='center', fontsize=8)
    
    # 3. çµ±åˆé‡è¦åº¦TOP20
    words_combined = combined_top100[:20]
    values_combined = [sentiment_importance.get(word, 0) + course_importance.get(word, 0) for word in words_combined]
    
    bars3 = axes[1, 0].barh(range(len(words_combined)), values_combined, color='#45B7D1', alpha=0.8)
    axes[1, 0].set_yticks(range(len(words_combined)))
    axes[1, 0].set_yticklabels(words_combined)
    axes[1, 0].set_xlabel('çµ±åˆé‡è¦åº¦')
    axes[1, 0].set_title('çµ±åˆé‡è¦åº¦ TOP20', fontweight='bold')
    axes[1, 0].invert_yaxis()
    
    # æ•°å€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
    for i, (bar, value) in enumerate(zip(bars3, values_combined)):
        axes[1, 0].text(bar.get_width() + max(values_combined) * 0.01, bar.get_y() + bar.get_height()/2, 
                       f'{value:.4f}', ha='left', va='center', fontsize=8)
    
    # 4. TOP100ã®åˆ†å¸ƒæ¯”è¼ƒ
    sentiment_values = [item[1] for item in sentiment_top100]
    course_values = [item[1] for item in course_top100]
    
    axes[1, 1].plot(range(1, 101), sentiment_values, 'o-', color='#FF6B6B', alpha=0.7, label='æ„Ÿæƒ…ã‚¹ã‚³ã‚¢', markersize=3)
    axes[1, 1].plot(range(1, 101), course_values, 's-', color='#4ECDC4', alpha=0.7, label='æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢', markersize=3)
    axes[1, 1].set_xlabel('ãƒ©ãƒ³ã‚­ãƒ³ã‚°')
    axes[1, 1].set_ylabel('é‡è¦åº¦')
    axes[1, 1].set_title('TOP100é‡è¦åº¦åˆ†å¸ƒæ¯”è¼ƒ', fontweight='bold')
    axes[1, 1].legend()
    axes[1, 1].set_yscale('log')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/TOP100ãƒ©ãƒ³ã‚­ãƒ³ã‚°_å…¨ãƒ‡ãƒ¼ã‚¿_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # TOP100ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ä¿å­˜
    sentiment_top100_df = pd.DataFrame([
        {'rank': i+1, 'word': word, 'importance': importance, 'category': 'sentiment'}
        for i, (word, importance) in enumerate(sentiment_top100)
    ])
    sentiment_top100_df.to_csv(f"{output_dir}/æ„Ÿæƒ…ã‚¹ã‚³ã‚¢TOP100_å…¨ãƒ‡ãƒ¼ã‚¿.csv", index=False, encoding='utf-8')
    
    course_top100_df = pd.DataFrame([
        {'rank': i+1, 'word': word, 'importance': importance, 'category': 'course'}
        for i, (word, importance) in enumerate(course_top100)
    ])
    course_top100_df.to_csv(f"{output_dir}/æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢TOP100_å…¨ãƒ‡ãƒ¼ã‚¿.csv", index=False, encoding='utf-8')
    
    combined_top100_df = pd.DataFrame([
        {'rank': i+1, 'word': word, 
         'sentiment_importance': sentiment_importance.get(word, 0),
         'course_importance': course_importance.get(word, 0),
         'total_importance': sentiment_importance.get(word, 0) + course_importance.get(word, 0)}
        for i, word in enumerate(combined_top100)
    ])
    combined_top100_df.to_csv(f"{output_dir}/çµ±åˆé‡è¦åº¦TOP100_å…¨ãƒ‡ãƒ¼ã‚¿.csv", index=False, encoding='utf-8')
    
    print("âœ… TOP100ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®SHAPåˆ†æ")
    print("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è¦å› åˆ†æ")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    df = load_data()
    if df is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
        return
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    texts = df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].dropna().tolist()  # å…¨ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œ
    print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ: {len(texts)}ä»¶ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")
    
    # BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã‚‹å‰å‡¦ç†
    processed_texts, word_to_id = bert_tokenizer_preprocessing(texts)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨SHAPåˆ†æ
    print("\nğŸ”¬ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®SHAPåˆ†æé–‹å§‹...")
    multitask_model = create_multitask_model(len(word_to_id))
    multitask_model.to(device)
    multitask_model.eval()
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®SHAPåˆ†æ
    sentiment_importance = perform_shap_analysis(multitask_model, processed_texts, word_to_id, target='sentiment')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®SHAPåˆ†æ
    course_importance = perform_shap_analysis(multitask_model, processed_texts, word_to_id, target='course')
    
    # å˜èªã®ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã®ã¿ï¼‰
    groups = create_word_groups_multitask_only(sentiment_importance, course_importance)
    
    # ã‚°ãƒ«ãƒ¼ãƒ—çµ±è¨ˆã®åˆ†æ
    stats = analyze_group_statistics(groups)
    
    # ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒã®å¯è¦–åŒ–
    create_group_comparison_visualization(groups, stats)
    
    # è©³ç´°æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
    create_detailed_comparison_report(groups, stats)
    
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    save_analysis_data(sentiment_importance, course_importance, groups, stats)
    
    # ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ
    create_violin_plots(sentiment_importance, course_importance, groups)
    
    # TOP100ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ä½œæˆ
    create_top100_rankings(sentiment_importance, course_importance)
    
    print("\nğŸ‰ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®SHAPåˆ†æå®Œäº†ï¼")
    print("ğŸ“ çµæœã¯ 03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()
