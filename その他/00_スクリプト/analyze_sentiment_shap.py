"""
SHAPåˆ†æ: å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«1ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼‰
æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿200ä»¶ã§æ„Ÿæƒ…åˆ¤å®šã«å¯„ä¸ã™ã‚‹é‡è¦èªã‚’ç‰¹å®š
"""

import pandas as pd
import numpy as np
import torch
from transformers import BertForSequenceClassification, BertJapaneseTokenizer
import shap
import matplotlib.pyplot as plt
import json
import os
from datetime import datetime
from collections import defaultdict
from tqdm import tqdm

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
MODEL_PATH = r"C:\Users\takahashi.DESKTOP-U0T5SUB\Downloads\BERT\git_excluded\finetuned_bert_model_20250718_step2_fixed_classweights_variant1_positiveé‡ç‚¹å¼·åŒ–"
print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_PATH}")

tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.to(device)
model.eval()

# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
VAL_DATA_PATH = r"../01_ãƒ‡ãƒ¼ã‚¿\è‡ªç”±è¨˜è¿°â†’æ„Ÿæƒ…ã‚¹ã‚³ã‚¢\finetuning_val_20250710_220621.csv"
print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {VAL_DATA_PATH}")
val_df = pd.read_csv(VAL_DATA_PATH)

print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(val_df)}")
print(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:\n{val_df['label'].value_counts()}")

# ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ0, 1, 2 â†’ ãƒã‚¬ãƒ†ã‚£ãƒ–, ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«, ãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰
label_names = {0: "ãƒã‚¬ãƒ†ã‚£ãƒ–", 1: "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", 2: "ãƒã‚¸ãƒ†ã‚£ãƒ–"}

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
RUN_SAMPLE_5000 = False  # è¿½åŠ æ¤œè¨¼: 8ä¸‡ä»¶ã‹ã‚‰5,000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆé›†è¨ˆã®ã¿ï¼‰
SAMPLE_SIZE = 5000
RAW_TEXT_PATH = r"../01_ãƒ‡ãƒ¼ã‚¿\å…ƒãƒ‡ãƒ¼ã‚¿\(CSVç”¨)å‰å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿_free_text_only.csv"  # ID,è‡ªç”±è¨˜è¿°

# äºˆæ¸¬é–¢æ•°ï¼ˆSHAPç”¨ï¼‰
def predict_proba(texts):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’è¿”ã™"""
    # SHAPã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿å‹ã‚’å‡¦ç†
    if isinstance(texts, str):
        texts = [texts]
    elif isinstance(texts, np.ndarray):
        texts = texts.tolist()
    elif not isinstance(texts, list):
        # ãã®ä»–ã®å‹ã®å ´åˆã€ãƒªã‚¹ãƒˆã«å¤‰æ›ã‚’è©¦ã¿ã‚‹
        try:
            texts = list(texts)
        except:
            texts = [str(texts)]
    
    # ç©ºæ–‡å­—åˆ—ã‚„ç„¡åŠ¹ãªå…¥åŠ›ã‚’å‡¦ç†
    texts = [str(t) if t else "" for t in texts]
    
    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
    
    return probs.cpu().numpy()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆå°è¦æ¨¡ï¼‰
print("\n" + "="*60)
print("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: æœ€åˆã®5ä»¶ã§å‹•ä½œç¢ºèª")
print("="*60)

test_texts = val_df['text'].head(5).tolist()
print(f"\nãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(test_texts)}")

# äºˆæ¸¬ã®ãƒ†ã‚¹ãƒˆ
test_probs = predict_proba(test_texts)
print(f"äºˆæ¸¬ç¢ºç‡ã®å½¢çŠ¶: {test_probs.shape}")
print(f"æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã®äºˆæ¸¬: {test_probs[0]}")
print(f"äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {label_names[test_probs[0].argmax()]}")

# SHAPåˆ†æã®æº–å‚™
print("\n" + "="*60)
print("SHAP Explainerã®ä½œæˆ")
print("="*60)

# Partition Explainerã‚’ä½¿ç”¨ï¼ˆã‚ˆã‚Šå®‰å®šï¼‰
print("Explainerã‚’åˆæœŸåŒ–ä¸­...")
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¹ã‚«ãƒ¼ã§ã¯ãªãã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨
masker = shap.maskers.Text(tokenizer)
explainer = shap.Explainer(predict_proba, masker, algorithm="partition")

# å°è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆæœ€åˆã®10ä»¶ï¼‰
print("\n" + "="*60)
print("å°è¦æ¨¡SHAPåˆ†æï¼ˆ10ä»¶ï¼‰")
print("="*60)

small_texts = val_df['text'].head(10).tolist()
print("SHAPå€¤ã‚’è¨ˆç®—ä¸­...")
small_shap_values = explainer(small_texts)

print(f"SHAPå€¤ã®å½¢çŠ¶: {small_shap_values.shape}")
print("âœ“ å°è¦æ¨¡ãƒ†ã‚¹ãƒˆæˆåŠŸï¼")

# æœ¬ç•ªå®Ÿè¡Œï¼ˆå…¨200ä»¶ï¼‰
print("\n" + "="*60)
print("æœ¬ç•ªSHAPåˆ†æï¼ˆå…¨200ä»¶ï¼‰")
print("="*60)

all_texts = val_df['text'].tolist()
all_labels = val_df['label'].astype(int).tolist()

print(f"åˆ†æå¯¾è±¡: {len(all_texts)}ä»¶")
print("SHAPå€¤ã‚’è¨ˆç®—ä¸­ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...")

# ãƒãƒƒãƒå‡¦ç†ã§SHAPåˆ†æï¼ˆãƒ¡ãƒ¢ãƒªå¯¾ç­–ï¼‰
batch_size = 20
all_shap_values = []

for i in tqdm(range(0, len(all_texts), batch_size), desc="SHAPåˆ†æ"):
    batch_texts = all_texts[i:i+batch_size]
    batch_shap = explainer(batch_texts)
    all_shap_values.append(batch_shap)

print("âœ“ SHAPåˆ†æå®Œäº†ï¼")

# çµæœã®é›†è¨ˆ
print("\n" + "="*60)
print("çµæœã®é›†è¨ˆ")
print("="*60)

def merge_wordpieces(tokens, shap_vals_pos):
    """WordPieceã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆ##ï¼‰ã‚’å‰ã®èªã«çµåˆã—ã¦é›†ç´„ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: (merged_tokens, merged_shap_vals)
    """
    merged_tokens = []
    merged_vals = []
    current = ''
    current_val = 0.0
    for tok, val in zip(tokens, shap_vals_pos):
        t = str(tok)
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
        if t in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
            continue
        if t.startswith('##'):
            # é€£çµï¼ˆæ¥é ­ã®##ã‚’é™¤å»ã—ã¦å‰èªã«è¿½åŠ ï¼‰
            current += t[2:]
            current_val += float(val)
        else:
            # ç›´å‰ã®èªã‚’ç¢ºå®š
            if current:
                merged_tokens.append(current)
                merged_vals.append(current_val)
            current = t
            current_val = float(val)
    if current:
        merged_tokens.append(current)
        merged_vals.append(current_val)
    return merged_tokens, merged_vals

# ã‚¯ãƒ©ã‚¹åˆ¥ã«SHAPå€¤ã‚’é›†è¨ˆã—ã¤ã¤ã€èªå˜ä½ã¸ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰çµåˆ
print("\nå˜èªã”ã¨ã®é‡è¦åº¦ã‚’é›†è¨ˆä¸­ï¼ˆã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰çµ±åˆã‚ã‚Šï¼‰...")
word_importance = defaultdict(lambda: {'shap_values': [], 'count': 0, 'class_dist': {0: 0, 1: 0, 2: 0}})

for idx, (text, label) in enumerate(zip(all_texts, all_labels)):
    batch_idx = idx // batch_size
    within_idx = idx % batch_size
    if within_idx >= len(all_shap_values[batch_idx]):
        continue
    sv = all_shap_values[batch_idx][within_idx]
    tokens = sv.data
    shap_vals = sv.values
    # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚¯ãƒ©ã‚¹ï¼ˆindex=2ï¼‰ã®SHAPå€¤ã‚’ä½¿ç”¨
    if len(shap_vals.shape) > 1:
        shap_vals_pos = shap_vals[:, 2]
    else:
        shap_vals_pos = shap_vals
    merged_tokens, merged_vals = merge_wordpieces(tokens, shap_vals_pos)
    for m_tok, m_val in zip(merged_tokens, merged_vals):
        if not m_tok:
            continue
        word_importance[m_tok]['shap_values'].append(float(m_val))
        word_importance[m_tok]['count'] += 1
        word_importance[m_tok]['class_dist'][label] += 1

# å¹³å‡SHAPå€¤ã‚’è¨ˆç®—
word_stats = {}
for word, data in word_importance.items():
    if data['count'] >= 3:  # 3å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹å˜èªã®ã¿
        word_stats[word] = {
            'mean_shap': np.mean(data['shap_values']),
            'abs_mean_shap': np.mean(np.abs(data['shap_values'])),
            'std_shap': np.std(data['shap_values']),
            'count': data['count'],
            'class_dist': data['class_dist']
        }

# DataFrameåŒ–
df_importance = pd.DataFrame(word_stats).T
df_importance = df_importance.sort_values('mean_shap', ascending=False)

print(f"\nåˆ†æå¯¾è±¡å˜èªæ•°: {len(df_importance)}")

# çµæœã®è¡¨ç¤º
print("\n" + "="*60)
print("ãƒã‚¸ãƒ†ã‚£ãƒ–åˆ¤å®šã«æœ€ã‚‚å¯„ä¸ã™ã‚‹å˜èª TOP20")
print("="*60)
print(df_importance.head(20)[['mean_shap', 'abs_mean_shap', 'count']].to_string())

print("\n" + "="*60)
print("ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¤å®šã«æœ€ã‚‚å¯„ä¸ã™ã‚‹å˜èª TOP20")
print("="*60)
print(df_importance.tail(20)[['mean_shap', 'abs_mean_shap', 'count']].to_string())

# çµæœã‚’ä¿å­˜
print("\n" + "="*60)
print("çµæœã‚’ä¿å­˜ä¸­")
print("="*60)

output_dir = "../03_åˆ†æçµæœ/SHAPåˆ†æ/æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿200ä»¶"
os.makedirs(output_dir, exist_ok=True)
os.makedirs(f"{output_dir}/å¯è¦–åŒ–", exist_ok=True)
# å€‹åˆ¥äº‹ä¾‹ã®å‡ºåŠ›è¨­å®šï¼ˆä¸è¦ãªã‚‰ Falseï¼‰
SAVE_INDIVIDUAL = False

# 1. é‡è¦èªã®JSONã¨ã—ã¦ä¿å­˜
importance_data = {
    "analysis_date": datetime.now().strftime("%Y%m%d_%H%M%S"),
    "dataset_size": len(all_texts),
    "model_path": MODEL_PATH,
    "top_positive_words": df_importance.head(20).to_dict('index'),
    "top_negative_words": df_importance.tail(20).to_dict('index'),
    "all_words": df_importance.to_dict('index')
}

with open(f"{output_dir}/global_importance.json", 'w', encoding='utf-8') as f:
    json.dump(importance_data, f, ensure_ascii=False, indent=2)

print(f"âœ“ é‡è¦èªãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_dir}/global_importance.json")

# 2. CSVã¨ã—ã¦ã‚‚ä¿å­˜ï¼ˆExcelã§é–‹ã‘ã‚‹ï¼‰
df_importance.to_csv(f"{output_dir}/word_importance.csv", encoding='utf-8-sig')
print(f"âœ“ CSVä¿å­˜: {output_dir}/word_importance.csv")

# 3. å¯è¦–åŒ–: ãƒã‚¸ãƒ†ã‚£ãƒ–TOP20
fig, ax = plt.subplots(figsize=(10, 8))
top20_pos = df_importance.head(20)
ax.barh(range(len(top20_pos)), top20_pos['mean_shap'], color='green', alpha=0.7)
ax.set_yticks(range(len(top20_pos)))
ax.set_yticklabels(top20_pos.index)
ax.set_xlabel('å¹³å‡SHAPå€¤ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–æ–¹å‘ï¼‰')
ax.set_title('ãƒã‚¸ãƒ†ã‚£ãƒ–åˆ¤å®šã«æœ€ã‚‚å¯„ä¸ã™ã‚‹å˜èª TOP20')
ax.invert_yaxis()
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig(f"{output_dir}/å¯è¦–åŒ–/top20_positive.png", dpi=300, bbox_inches='tight')
print(f"âœ“ ã‚°ãƒ©ãƒ•ä¿å­˜: {output_dir}/å¯è¦–åŒ–/top20_positive.png")
plt.close()

# 4. å¯è¦–åŒ–: ãƒã‚¬ãƒ†ã‚£ãƒ–TOP20
fig, ax = plt.subplots(figsize=(10, 8))
top20_neg = df_importance.tail(20).sort_values('mean_shap')
ax.barh(range(len(top20_neg)), top20_neg['mean_shap'], color='red', alpha=0.7)
ax.set_yticks(range(len(top20_neg)))
ax.set_yticklabels(top20_neg.index)
ax.set_xlabel('å¹³å‡SHAPå€¤ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–æ–¹å‘ï¼‰')
ax.set_title('ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¤å®šã«æœ€ã‚‚å¯„ä¸ã™ã‚‹å˜èª TOP20')
ax.invert_yaxis()
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig(f"{output_dir}/å¯è¦–åŒ–/top20_negative.png", dpi=300, bbox_inches='tight')
print(f"âœ“ ã‚°ãƒ©ãƒ•ä¿å­˜: {output_dir}/å¯è¦–åŒ–/top20_negative.png")
plt.close()

# 5. å€‹åˆ¥äº‹ä¾‹ã®å¯è¦–åŒ–ï¼ˆå„ã‚¯ãƒ©ã‚¹2ä»¶ãšã¤ï¼‰
if SAVE_INDIVIDUAL:
    print("\nå€‹åˆ¥äº‹ä¾‹ã‚’å¯è¦–åŒ–ä¸­...")
    os.makedirs(f"{output_dir}/å€‹åˆ¥äº‹ä¾‹", exist_ok=True)

    # ãƒã‚¸ãƒ†ã‚£ãƒ–äº‹ä¾‹
    pos_indices = val_df[val_df['label'] == 2].head(2).index.tolist()
    for i, idx in enumerate(pos_indices):
        text_idx = list(val_df.index).index(idx)
        batch_idx = text_idx // batch_size
        within_idx = text_idx % batch_size
        
        if within_idx < len(all_shap_values[batch_idx]):
            shap.plots.text(all_shap_values[batch_idx][within_idx], display=False)
            plt.savefig(f"{output_dir}/å€‹åˆ¥äº‹ä¾‹/positive_example_{i+1}.png", dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ“ ãƒã‚¸ãƒ†ã‚£ãƒ–äº‹ä¾‹{i+1}ã‚’ä¿å­˜")

    # ãƒã‚¬ãƒ†ã‚£ãƒ–äº‹ä¾‹
    neg_indices = val_df[val_df['label'] == 0].head(2).index.tolist()
    for i, idx in enumerate(neg_indices):
        text_idx = list(val_df.index).index(idx)
        batch_idx = text_idx // batch_size
        within_idx = text_idx % batch_size
        
        if within_idx < len(all_shap_values[batch_idx]):
            shap.plots.text(all_shap_values[batch_idx][within_idx], display=False)
            plt.savefig(f"{output_dir}/å€‹åˆ¥äº‹ä¾‹/negative_example_{i+1}.png", dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ“ ãƒã‚¬ãƒ†ã‚£ãƒ–äº‹ä¾‹{i+1}ã‚’ä¿å­˜")

    # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«äº‹ä¾‹
    neu_indices = val_df[val_df['label'] == 1].head(2).index.tolist()
    for i, idx in enumerate(neu_indices):
        text_idx = list(val_df.index).index(idx)
        batch_idx = text_idx // batch_size
        within_idx = text_idx % batch_size
        
        if within_idx < len(all_shap_values[batch_idx]):
            shap.plots.text(all_shap_values[batch_idx][within_idx], display=False)
            plt.savefig(f"{output_dir}/å€‹åˆ¥äº‹ä¾‹/neutral_example_{i+1}.png", dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ“ ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«äº‹ä¾‹{i+1}ã‚’ä¿å­˜")

# ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
print("\n" + "="*60)
print("ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­")
print("="*60)

summary_report = f"""# SHAPåˆ†æã‚µãƒãƒªãƒ¼ï¼ˆæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿200ä»¶ï¼‰

**åˆ†ææ—¥æ™‚:** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  
**å¯¾è±¡ãƒ‡ãƒ¼ã‚¿:** æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿200ä»¶ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ï¼‰  
**ãƒ¢ãƒ‡ãƒ«:** å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«1ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼‰

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦

- **ç·ã‚µãƒ³ãƒ—ãƒ«æ•°:** {len(val_df)}ä»¶
- **ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:**
  - ãƒã‚¬ãƒ†ã‚£ãƒ–: {(val_df['label']==0).sum()}ä»¶
  - ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«: {(val_df['label']==1).sum()}ä»¶
  - ãƒã‚¸ãƒ†ã‚£ãƒ–: {(val_df['label']==2).sum()}ä»¶

---

## ğŸ” ãƒã‚¸ãƒ†ã‚£ãƒ–åˆ¤å®šã«å¯„ä¸ã™ã‚‹é‡è¦èª TOP20

| é †ä½ | å˜èª | å¹³å‡SHAPå€¤ | å‡ºç¾å›æ•° |
|------|------|-----------|---------|
{chr(10).join([f"| {i+1} | {word} | {row['mean_shap']:.4f} | {int(row['count'])} |" for i, (word, row) in enumerate(df_importance.head(20).iterrows())])}

---

## ğŸ”» ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¤å®šã«å¯„ä¸ã™ã‚‹é‡è¦èª TOP20

| é †ä½ | å˜èª | å¹³å‡SHAPå€¤ | å‡ºç¾å›æ•° |
|------|------|-----------|---------|
{chr(10).join([f"| {i+1} | {word} | {row['mean_shap']:.4f} | {int(row['count'])} |" for i, (word, row) in enumerate(df_importance.tail(20).sort_values('mean_shap').iterrows())])}

---

## ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«

- `global_importance.json` - å…¨å˜èªã®é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿
- `word_importance.csv` - Excelç”¨CSV
- `å¯è¦–åŒ–/top20_positive.png` - ãƒã‚¸ãƒ†ã‚£ãƒ–TOP20ã‚°ãƒ©ãƒ•
- `å¯è¦–åŒ–/top20_negative.png` - ãƒã‚¬ãƒ†ã‚£ãƒ–TOP20ã‚°ãƒ©ãƒ•
- `å€‹åˆ¥äº‹ä¾‹/*.png` - å€‹åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®SHAPå¯è¦–åŒ–

---

## ğŸ’¡ ä¸»è¦ãªç™ºè¦‹

### æº€è¶³åº¦ã‚’é«˜ã‚ã‚‹è¦å› ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰
{chr(10).join([f"- **{word}**: {row['mean_shap']:.4f}ï¼ˆ{int(row['count'])}å›å‡ºç¾ï¼‰" for word, row in df_importance.head(5).iterrows()])}

### ä¸æº€ã®åŸå› ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
{chr(10).join([f"- **{word}**: {row['mean_shap']:.4f}ï¼ˆ{int(row['count'])}å›å‡ºç¾ï¼‰" for word, row in df_importance.tail(5).sort_values('mean_shap').iterrows()])}

---

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:**
- ã‚¯ãƒ©ã‚¹åˆ¥ã®è©³ç´°åˆ†æ
- èª¤åˆ†é¡äº‹ä¾‹ã®åˆ†æ
- æ•™å“¡ã¸ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä½œæˆ
"""

with open(f"{output_dir}/SHAPåˆ†æã‚µãƒãƒªãƒ¼.md", 'w', encoding='utf-8') as f:
    f.write(summary_report)

print(f"âœ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_dir}/SHAPåˆ†æã‚µãƒãƒªãƒ¼.md")

print("\n" + "="*60)
print("SHAPåˆ†æå®Œäº†ï¼")
print("="*60)
print(f"\nçµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
print(f"  {output_dir}/")
print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
print("  1. ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’ç¢ºèª")
print("  2. SHAPåˆ†æã‚µãƒãƒªãƒ¼.mdã‚’ç¢ºèª")
print("  3. å¿…è¦ã«å¿œã˜ã¦è¿½åŠ åˆ†æï¼ˆ5,000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")

# è¿½åŠ æ¤œè¨¼: 8ä¸‡ä»¶ã‹ã‚‰5,000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é›†è¨ˆã®ã¿å®Ÿæ–½
if RUN_SAMPLE_5000:
    print("\n" + "="*60)
    print("è¿½åŠ æ¤œè¨¼: 8ä¸‡ä»¶ã‹ã‚‰5,000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§SHAPé›†è¨ˆï¼ˆå¯è¦–åŒ–ãªã—ï¼‰")
    print("="*60)
    try:
        raw_df = pd.read_csv(RAW_TEXT_PATH)
        # åˆ—åæ¨å®š: å…ˆé ­åˆ—ãŒIDã€2åˆ—ç›®ãŒè‡ªç”±è¨˜è¿°ã‚’æƒ³å®š
        if 'è‡ªç”±è¨˜è¿°' in raw_df.columns:
            texts_all = raw_df['è‡ªç”±è¨˜è¿°'].dropna().astype(str)
        elif 'text' in raw_df.columns:
            texts_all = raw_df['text'].dropna().astype(str)
        else:
            # 2åˆ—ç›®ã‚’è‡ªç”±è¨˜è¿°ã¨ä»®å®š
            texts_all = raw_df.iloc[:, 1].dropna().astype(str)

        n = min(SAMPLE_SIZE, len(texts_all))
        sample_texts = texts_all.sample(n=n, random_state=42).tolist()
        print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {len(sample_texts)} / ç·ä»¶æ•°: {len(texts_all)}")

        # ã‚¹ãƒˆãƒªãƒ¼ãƒ é›†è¨ˆï¼ˆèªå˜ä½çµ±åˆï¼‰
        word_importance_sample = defaultdict(lambda: {'shap_values': [], 'count': 0})
        batch_size_sample = 64
        for i in tqdm(range(0, len(sample_texts), batch_size_sample), desc="SHAPé›†è¨ˆ(5k)"):
            bt = sample_texts[i:i+batch_size_sample]
            sv_batch = explainer(bt)
            for sv in sv_batch:
                tokens = sv.data
                vals = sv.values
                if len(vals.shape) > 1:
                    vals_pos = vals[:, 2]
                else:
                    vals_pos = vals
                m_toks, m_vals = merge_wordpieces(tokens, vals_pos)
                for t, v in zip(m_toks, m_vals):
                    if not t:
                        continue
                    word_importance_sample[t]['shap_values'].append(float(v))
                    word_importance_sample[t]['count'] += 1

        # DataFrameåŒ–
        word_stats_sample = {
            w: {
                'mean_shap': float(np.mean(d['shap_values'])),
                'abs_mean_shap': float(np.mean(np.abs(d['shap_values']))),
                'std_shap': float(np.std(d['shap_values'])),
                'count': int(d['count'])
            }
            for w, d in word_importance_sample.items() if d['count'] >= 5
        }
        df_sample = pd.DataFrame(word_stats_sample).T.sort_values('mean_shap', ascending=False)

        out_dir_sample = "../03_åˆ†æçµæœ/SHAPåˆ†æ/ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°5000ä»¶"
        os.makedirs(out_dir_sample, exist_ok=True)
        df_sample.to_csv(f"{out_dir_sample}/word_importance_sample5000.csv", encoding='utf-8-sig')
        with open(f"{out_dir_sample}/global_importance_sample5000.json", 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_date': datetime.now().strftime('%Y%m%d_%H%M%S'),
                'dataset_size': len(sample_texts),
                'model_path': MODEL_PATH,
                'top_positive_words': df_sample.head(50).to_dict('index'),
                'top_negative_words': df_sample.tail(50).to_dict('index')
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«é›†è¨ˆã‚’ä¿å­˜: {out_dir_sample}/word_importance_sample5000.csv")
        print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«é›†è¨ˆã‚’ä¿å­˜: {out_dir_sample}/global_importance_sample5000.json")
    except Exception as e:
        print(f"è¿½åŠ æ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼: {e}")

