#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é †åºå›å¸°ãƒ¢ãƒ‡ãƒ« æœ¬ç•ªç”¨SHAPåˆ†æ
ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æã¨åŒã˜å½¢å¼ã§å‡ºåŠ›

**ä½œæˆæ—¥**: 2025å¹´1æœˆ

å‡ºåŠ›å½¢å¼:
- word_importance_sentiment_production.csv (æ„Ÿæƒ…ã‚¹ã‚³ã‚¢)
- word_importance_course_production.csv (æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢)
- word_importance_expected_production.csv (æœŸå¾…å€¤E[y])
- word_importance_p1_production.csv (P1: ä½è©•ä¾¡ç¢ºç‡)
- word_importance_p2_production.csv (P2: ä¸­ä½è©•ä¾¡ç¢ºç‡)
- word_importance_p3_production.csv (P3: ä¸­é«˜è©•ä¾¡ç¢ºç‡)
- word_importance_p4_production.csv (P4: é«˜è©•ä¾¡ç¢ºç‡)
- analysis_summary_production.json
- ordinal_shap_analysis_summary_production.md
- factor_categories_production.json
- å¯è¦–åŒ–PNGãƒ•ã‚¡ã‚¤ãƒ«
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

os.environ['TORCH_DISABLE_SAFETENSORS_WARNING'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import json
from datetime import datetime
import shap

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(BASE_DIR, "02_ãƒ¢ãƒ‡ãƒ«", "æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«", "class_level_ordinal_llp_20260114_101852.pth")
DATA_DIR = os.path.join(BASE_DIR, "01_ãƒ‡ãƒ¼ã‚¿", "ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿")
OUTPUT_DIR = os.path.join(BASE_DIR, "03_åˆ†æçµæœ", "é †åºå›å¸°SHAPåˆ†æ_P2P4")

# CSV: å›ç­”åˆ†å¸ƒä»˜ãã‚’å„ªå…ˆã€ç„¡ã‘ã‚Œã°æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_*.csv ã®æœ€æ–°ã‚’ä½¿ç”¨ï¼ˆSHAPã¯è‡ªç”±è¨˜è¿°ã¾ã¨ã‚ã®ã¿ä½¿ç”¨ï¼‰
def _get_csv_path():
    preferred = os.path.join(DATA_DIR, "æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv")
    if os.path.exists(preferred):
        return preferred
    import glob
    pattern = os.path.join(DATA_DIR, "æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_*.csv")
    candidates = glob.glob(pattern)
    if not candidates:
        return preferred  # ã‚¨ãƒ©ãƒ¼ã¯èª­ã¿è¾¼ã¿æ™‚ã«
    return max(candidates, key=os.path.getmtime)

CSV_PATH = _get_csv_path()
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼ˆGPUæœ€å„ªå…ˆï¼‰
def get_device():
    """GPUã‚’æœ€å„ªå…ˆã§é¸æŠï¼ˆCUDA â†’ DirectML â†’ CPUï¼‰"""
    # 1. CUDAã‚’è©¦ã™
    if torch.cuda.is_available():
        try:
            device = torch.device("cuda")
            _ = torch.tensor([1.0]).to(device)
            print(f"âœ… CUDAä½¿ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"   GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            return device
        except Exception as e:
            print(f"âš ï¸ CUDAã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. DirectMLã‚’è©¦ã™
    try:
        import torch_directml as dml
        if dml.is_available():
            device = dml.device()
            print(f"âœ… DirectMLä½¿ç”¨")
            return device
    except Exception:
        pass
    
    # 3. CPUï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
    print("âš ï¸ GPUãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CPUã§å®Ÿè¡Œã—ã¾ã™")
    return torch.device("cpu")

device = get_device()
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"PyTorch version: {torch.__version__}")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
print("="*60)
print("é †åºå›å¸°ãƒ¢ãƒ‡ãƒ« æœ¬ç•ªç”¨SHAPåˆ†æ")
print("="*60)

from train_class_level_ordinal_llp import CourseOrdinalLLPModel, BASE_MODEL
from transformers import BertJapaneseTokenizer

print("ğŸ“¥ é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
tokenizer = BertJapaneseTokenizer.from_pretrained(BASE_MODEL)
model = CourseOrdinalLLPModel(BASE_MODEL)
model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
model.to(device)
model.eval()
print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
print(f"   CSV: {CSV_PATH}")
df = pd.read_csv(CSV_PATH)
texts = df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].fillna("").astype(str).tolist()
print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(texts)}")

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¨å¥¨ã ãŒã€ç°¡æ˜“ç‰ˆã¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
SAMPLE_SIZE = 1000
if len(texts) > SAMPLE_SIZE:
    np.random.seed(42)
    sample_indices = np.random.choice(len(texts), SAMPLE_SIZE, replace=False)
    sample_texts = [texts[i] for i in sample_indices]
else:
    sample_texts = texts
print(f"åˆ†æã‚µãƒ³ãƒ—ãƒ«æ•°: {len(sample_texts)}")

# ======================== äºˆæ¸¬é–¢æ•° ========================

MAX_LENGTH = 192
BATCH_SIZE = 16

def predict_probs(list_of_texts):
    """P1ï½P4ã®ç¢ºç‡ã‚’äºˆæ¸¬"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    all_probs = []
    for i in range(0, len(list_of_texts), BATCH_SIZE):
        batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH_SIZE]]
        encoding = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
        with torch.no_grad():
            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            _, _, P, _, _ = model(input_ids, attention_mask, chunk_mask)
            all_probs.extend(P.cpu().numpy().tolist())
    return np.array(all_probs)

def predict_expected_value(list_of_texts):
    """æœŸå¾…å€¤ E[y] = 1*P1 + 2*P2 + 3*P3 + 4*P4"""
    probs = predict_probs(list_of_texts)
    expected = probs @ np.array([1, 2, 3, 4])
    return expected.reshape(-1, 1)

def predict_p1(list_of_texts):
    """P1ï¼ˆä½è©•ä¾¡ç¢ºç‡ï¼‰"""
    probs = predict_probs(list_of_texts)
    return probs[:, 0:1]

def predict_p2(list_of_texts):
    """P2ï¼ˆä¸­ä½è©•ä¾¡ç¢ºç‡ï¼‰"""
    probs = predict_probs(list_of_texts)
    return probs[:, 1:2]

def predict_p3(list_of_texts):
    """P3ï¼ˆä¸­é«˜è©•ä¾¡ç¢ºç‡ï¼‰"""
    probs = predict_probs(list_of_texts)
    return probs[:, 2:3]

def predict_p4(list_of_texts):
    """P4ï¼ˆé«˜è©•ä¾¡ç¢ºç‡ï¼‰"""
    probs = predict_probs(list_of_texts)
    return probs[:, 3:4]

def predict_sentiment(list_of_texts):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    pred = []
    for i in range(0, len(list_of_texts), BATCH_SIZE):
        batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH_SIZE]]
        encoding = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
        with torch.no_grad():
            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            _, _, _, y_sent, _ = model(input_ids, attention_mask, chunk_mask)
            pred.extend(y_sent.cpu().numpy().tolist())
    return np.array(pred).reshape(-1, 1)

def predict_course(list_of_texts):
    """æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆæœŸå¾…å€¤ã‹ã‚‰è¨ˆç®—ï¼‰"""
    # æœŸå¾…å€¤ E[y] = 1Ã—P1 + 2Ã—P2 + 3Ã—P3 + 4Ã—P4 ã‚’ä½¿ç”¨
    probs = predict_probs(list_of_texts)
    expected = probs @ np.array([1, 2, 3, 4])
    return expected.reshape(-1, 1)

# ======================== SHAPåˆ†æå®Ÿè¡Œ ========================

def merge_wordpieces(tokens, shap_vals_pos):
    """WordPieceã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆ##ï¼‰ã‚’å‰ã®èªã«çµåˆã—ã¦é›†ç´„ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: (merged_tokens, merged_shap_vals)
    ï¼ˆanalyze_sentiment_shap_5000.pyã¨åŒã˜å®Ÿè£…ã§æ•´åˆæ€§ã‚’ç¢ºä¿ï¼‰
    """
    merged_tokens = []
    merged_vals = []
    current = ''
    current_val = 0.0
    for tok, val in zip(tokens, shap_vals_pos):
        t = str(tok)
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
        if t in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
            continue
        if t.startswith('##'):
            # é€£çµï¼ˆæ¥é ­ã®##ã‚’é™¤å»ã—ã¦å‰èªã«è¿½åŠ ï¼‰
            current += t[2:]
            current_val += float(val)
        else:
            # ç›´å‰ã®èªã‚’ç¢ºå®š
            if current:
                merged_tokens.append(current)
                merged_vals.append(current_val)
            current = t
            current_val = float(val)
    if current:
        merged_tokens.append(current)
        merged_vals.append(current_val)
    return merged_tokens, merged_vals

def run_shap_analysis(predict_fn, texts, name, output_dir):
    """SHAPåˆ†æã‚’å®Ÿè¡Œï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜æ–¹æ³•ã§çµ±ä¸€ï¼‰
    
    ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼ˆanalyze_classlevel_multitask_shap_beeswarm.pyï¼‰ã¨åŒã˜è¨ˆç®—æ–¹æ³•ï¼š
    - importance = np.abs(shap_values.values).mean(axis=0)
    - WordPieceã®çµåˆã¯è¡Œã‚ãªã„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ã¾ã¾ï¼‰
    
    ãŸã ã—ã€ä¸è¦å‰‡ãªå½¢çŠ¶ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å‡¦ç†ã—ã¦é›†è¨ˆ
    """
    try:
        print(f"\nğŸ” SHAPåˆ†æå®Ÿè¡Œ: {name}")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(texts)}ä»¶")
        print(f"   âš ï¸ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜æ–¹æ³•ã§è¨ˆç®—ï¼ˆWordPieceçµåˆãªã—ï¼‰")
        
        explainer = shap.Explainer(predict_fn, tokenizer)
        shap_values = explainer(texts)
        
        # ä¸è¦å‰‡ãªå½¢çŠ¶ã«å¯¾å¿œï¼šå„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å‡¦ç†
        # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®SHAPå€¤ã‚’é›†è¨ˆï¼ˆWordPieceçµåˆãªã—ï¼‰
        token_importance_dict = defaultdict(lambda: {'shap_values': [], 'count': 0})
        
        # shap_valuesã¯Explanationã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã€å„ã‚µãƒ³ãƒ—ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
        if isinstance(shap_values, shap.Explanation):
            # å„ã‚µãƒ³ãƒ—ãƒ«ã‚’å€‹åˆ¥ã«å‡¦ç†
            for sv in shap_values:
                if hasattr(sv, 'values') and hasattr(sv, 'data'):
                    tokens = sv.data
                    vals = sv.values
                    
                    # å½¢çŠ¶ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å‡¦ç†
                    if hasattr(vals, 'ndim') and vals.ndim > 1:
                        # å›å¸°ã‚¿ã‚¹ã‚¯ã®å ´åˆã€valsã®å½¢çŠ¶ã¯(n_tokens, 1)ã¾ãŸã¯(n_tokens,)
                        if vals.shape[1] == 1:
                            vals_abs = np.abs(vals).flatten()  # (n_tokens, 1) -> (n_tokens,)
                        else:
                            vals_abs = np.abs(vals[:, 0])  # æœ€åˆã®å‡ºåŠ›ã‚’ä½¿ç”¨
                    else:
                        vals_abs = np.abs(vals)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨SHAPå€¤ã‚’å¯¾å¿œä»˜ã‘ï¼ˆWordPieceçµåˆãªã—ï¼‰
                    for token, val in zip(tokens, vals_abs):
                        if token and str(token).strip() and str(token) not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
                            token_importance_dict[str(token)]['shap_values'].append(float(val))
                            token_importance_dict[str(token)]['count'] += 1
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®å¹³å‡é‡è¦åº¦ã‚’è¨ˆç®—ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜æ–¹æ³•ï¼‰
        token_stats = {
            token: np.mean(data['shap_values'])
            for token, data in token_importance_dict.items()
            if data['count'] > 0
        }
        
        # DataFrameã«å¤‰æ›ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨åŒã˜å½¢å¼ï¼‰
        df_importance = pd.DataFrame({
            'word': list(token_stats.keys()),
            'importance': list(token_stats.values())
        }).sort_values('importance', ascending=False)
        
        # å³åº§ã«CSVä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
        csv_path = f"{output_dir}/word_importance_{name.lower().replace(' ', '_').replace('ï¼ˆ', '').replace('ï¼‰', '').replace('[', '').replace(']', '')}_production.csv"
        df_importance.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"âœ… {name} å®Œäº†: {len(df_importance)}èª")
        print(f"   ğŸ“ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}")
        
        return shap_values, df_importance
        
    except Exception as e:
        print(f"âŒ {name} ã®SHAPåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"   ğŸ’¡ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œï¼ˆç©ºã®DataFrameã‚’è¿”ã™ï¼‰
        return None, pd.DataFrame({'word': [], 'importance': []})

# SHAPåˆ†æå®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãã€å„åˆ†æå®Œäº†å¾Œã«å³åº§ã«ä¿å­˜ï¼‰
print("\n" + "="*60)
print("SHAPåˆ†æå®Ÿè¡Œä¸­...")
print("="*60)
print("âš ï¸  å„åˆ†æãŒå®Œäº†æ¬¡ç¬¬ã€çµæœã‚’å³åº§ã«ä¿å­˜ã—ã¾ã™")
print("   é€”ä¸­ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã€å®Œäº†ã—ãŸåˆ†æçµæœã¯ä¿å­˜ã•ã‚Œã¾ã™")
print("="*60)

shap_results = {}
completed_analyses = []  # å®Œäº†ã—ãŸåˆ†æã‚’è¨˜éŒ²

# å„åˆ†æã‚’å€‹åˆ¥ã«å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œï¼‰
# P2ã¨P4ã®ã¿å®Ÿè¡Œï¼ˆæ•™å¸«ã®æœ€é‡è¦èª²é¡Œï¼‰
analyses = [
    ("P2ï¼ˆä¸­ä½è©•ä¾¡ç¢ºç‡ï¼‰", predict_p2, "p2"),  # 2ç‚¹ã‚’æ¸›ã‚‰ã™è¦å› ï¼ˆæ•™å¸«ã®æœ€é‡è¦èª²é¡Œï¼‰
    ("P4ï¼ˆé«˜è©•ä¾¡ç¢ºç‡ï¼‰", predict_p4, "p4"),  # 4ç‚¹ã‚’å¢—ã‚„ã™è¦å› 
]

for name, predict_fn, key in analyses:
    try:
        shap_val, df_imp = run_shap_analysis(predict_fn, sample_texts, name, OUTPUT_DIR)
        # df_impãŒç©ºã§ãªã‘ã‚Œã°æˆåŠŸï¼ˆshap_valã¯Noneã§ã‚‚å•é¡Œãªã„ï¼‰
        if len(df_imp) > 0:
            shap_results[key] = {'shap': shap_val, 'df': df_imp}
            completed_analyses.append(key)
            print(f"âœ… {name} ã®åˆ†æã¨ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            print(f"âš ï¸  {name} ã®åˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼ˆã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ç©ºã®çµæœï¼‰")
    except Exception as e:
        print(f"âŒ {name} ã®åˆ†æã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"   æ¬¡ã®åˆ†æã‚’ç¶šè¡Œã—ã¾ã™...")
        continue

print(f"\nâœ… å®Œäº†ã—ãŸåˆ†æ: {len(completed_analyses)}/{len(analyses)}")
print(f"   å®Œäº†ãƒªã‚¹ãƒˆ: {', '.join(completed_analyses)}")

# ======================== çµæœä¿å­˜ ========================

print("\n" + "="*60)
print("çµæœä¿å­˜ä¸­...")
print("="*60)

# CSVä¿å­˜ï¼ˆæ—¢ã«å„åˆ†æã§ä¿å­˜æ¸ˆã¿ã€ã“ã“ã§ã¯TOP100ã‚’ä¿å­˜ï¼‰
print("\nğŸ“Š TOP100ã®ä¿å­˜ä¸­...")
for key, data in shap_results.items():
    if 'df' in data and len(data['df']) > 0:
        df = data['df']
        top100_path = f"{OUTPUT_DIR}/word_importance_{key}_top100_production.csv"
        df.head(100).to_csv(top100_path, index=False, encoding='utf-8')
        print(f"   âœ… {key} ã®TOP100ã‚’ä¿å­˜: {top100_path}")

print("âœ… CSVä¿å­˜å®Œäº†")

# ======================== ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ ========================

def categorize_factors(sentiment_df, course_df, threshold=0.0005):
    """è¦å› ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç‰ˆã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    # è¾æ›¸ã«å¤‰æ›
    sent_dict = dict(zip(sentiment_df['word'], sentiment_df['importance']))
    course_dict = dict(zip(course_df['word'], course_df['importance']))
    
    all_words = set(sentiment_df['word']) | set(course_df['word'])
    
    categories = {
        'strong_common': [],      # ä¸¡æ–¹ã§é«˜ã„
        'sentiment_leaning': [],  # æ„Ÿæƒ…å¯„ã‚Š
        'course_leaning': [],     # è©•ä¾¡å¯„ã‚Š
        'sentiment_specific': [], # æ„Ÿæƒ…ç‰¹åŒ–
        'course_specific': []     # è©•ä¾¡ç‰¹åŒ–
    }
    
    for word in all_words:
        sent_imp = sent_dict.get(word, 0)
        course_imp = course_dict.get(word, 0)
        
        if sent_imp >= threshold and course_imp >= threshold:
            categories['strong_common'].append((word, sent_imp, course_imp))
        elif sent_imp >= threshold and course_imp < threshold * 0.5:
            categories['sentiment_specific'].append((word, sent_imp, course_imp))
        elif course_imp >= threshold and sent_imp < threshold * 0.5:
            categories['course_specific'].append((word, sent_imp, course_imp))
        elif sent_imp >= threshold * 0.5 and course_imp >= threshold * 0.5:
            if sent_imp > course_imp * 1.5:
                categories['sentiment_leaning'].append((word, sent_imp, course_imp))
            elif course_imp > sent_imp * 1.5:
                categories['course_leaning'].append((word, sent_imp, course_imp))
    
    # ã‚½ãƒ¼ãƒˆ
    for cat in categories:
        categories[cat].sort(key=lambda x: x[1] + x[2], reverse=True)
    
    return categories

# ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆP2ã¨P4ã®æ¯”è¼ƒï¼‰
if 'p2' in shap_results and 'p4' in shap_results:
    df_p2 = shap_results['p2']['df']
    df_p4 = shap_results['p4']['df']
    # P2ã¨P4ã®æ¯”è¼ƒç”¨ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
    p2_dict = dict(zip(df_p2['word'], df_p2['importance']))
    p4_dict = dict(zip(df_p4['word'], df_p4['importance']))
    all_words = set(df_p2['word']) | set(df_p4['word'])
    threshold = 0.0005
    
    categories = {
        'strong_common': [],      # P2ã¨P4ã®ä¸¡æ–¹ã§é«˜ã„
        'p2_specific': [],        # P2ç‰¹åŒ–ï¼ˆ2ç‚¹ã‚’æ¸›ã‚‰ã™è¦å› ï¼‰
        'p4_specific': [],        # P4ç‰¹åŒ–ï¼ˆ4ç‚¹ã‚’å¢—ã‚„ã™è¦å› ï¼‰
        'p2_leaning': [],         # P2å¯„ã‚Š
        'p4_leaning': []          # P4å¯„ã‚Š
    }
    
    for word in all_words:
        p2_imp = p2_dict.get(word, 0)
        p4_imp = p4_dict.get(word, 0)
        
        if p2_imp >= threshold and p4_imp >= threshold:
            categories['strong_common'].append((word, p2_imp, p4_imp))
        elif p2_imp >= threshold and p4_imp < threshold * 0.5:
            categories['p2_specific'].append((word, p2_imp, p4_imp))
        elif p4_imp >= threshold and p2_imp < threshold * 0.5:
            categories['p4_specific'].append((word, p2_imp, p4_imp))
        elif p2_imp >= threshold * 0.5 and p4_imp >= threshold * 0.5:
            if p2_imp > p4_imp * 1.5:
                categories['p2_leaning'].append((word, p2_imp, p4_imp))
            elif p4_imp > p2_imp * 1.5:
                categories['p4_leaning'].append((word, p2_imp, p4_imp))
    
    # ã‚½ãƒ¼ãƒˆ
    for cat in categories:
        categories[cat].sort(key=lambda x: x[1] + x[2], reverse=True)
    print("âœ… P2ã¨P4ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡å®Œäº†")
elif 'sentiment' in shap_results and 'course' in shap_results:
    df_sent = shap_results['sentiment']['df']
    df_course = shap_results['course']['df']
    categories = categorize_factors(df_sent, df_course)
else:
    print("âš ï¸  P2ã¨P4ã€ã¾ãŸã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®åˆ†æãŒå®Œäº†ã—ã¦ã„ãªã„ãŸã‚ã€ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    categories = {
        'strong_common': [],
        'sentiment_leaning': [],
        'course_leaning': [],
        'sentiment_specific': [],
        'course_specific': []
    }

# JSONä¿å­˜
categories_json = {}
for category, items in categories.items():
    if 'p2' in shap_results and 'p4' in shap_results:
        # P2ã¨P4ã®æ¯”è¼ƒç”¨
        categories_json[category] = [
            {'word': word, 'p2_importance': p2_imp, 'p4_importance': p4_imp}
            for word, p2_imp, p4_imp in items
        ]
    else:
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®æ¯”è¼ƒç”¨
        categories_json[category] = [
            {'word': word, 'sentiment_importance': s_imp, 'course_importance': c_imp}
            for word, s_imp, c_imp in items
        ]

with open(f"{OUTPUT_DIR}/factor_categories_production.json", 'w', encoding='utf-8') as f:
    json.dump(categories_json, f, ensure_ascii=False, indent=2)

print("âœ… ã‚«ãƒ†ã‚´ãƒªåˆ†é¡å®Œäº†")

# ======================== å¯è¦–åŒ– ========================

print("\nğŸ“Š å¯è¦–åŒ–ä½œæˆä¸­...")

# å¯è¦–åŒ–ï¼ˆå®Œäº†ã—ãŸåˆ†æã®ã¿ï¼‰
# 1. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢TOP30
if 'sentiment' in shap_results:
    df_sent = shap_results['sentiment']['df']
    plt.figure(figsize=(12, 8))
    top30_sent = df_sent.head(30)
    plt.barh(range(len(top30_sent)), top30_sent['importance'].values[::-1])
    plt.yticks(range(len(top30_sent)), top30_sent['word'].values[::-1])
    plt.xlabel('é‡è¦åº¦')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ é‡è¦èªTOP30ï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/sentiment_top30_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… æ„Ÿæƒ…ã‚¹ã‚³ã‚¢TOP30ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")

# 2. æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢TOP30
if 'course' in shap_results:
    df_course = shap_results['course']['df']
    plt.figure(figsize=(12, 8))
    top30_course = df_course.head(30)
    plt.barh(range(len(top30_course)), top30_course['importance'].values[::-1])
    plt.yticks(range(len(top30_course)), top30_course['word'].values[::-1])
    plt.xlabel('é‡è¦åº¦')
    plt.title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ é‡è¦èªTOP30ï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/course_top30_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢TOP30ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")

# 3. æœŸå¾…å€¤TOP30
if 'expected' in shap_results:
    df_expected = shap_results['expected']['df']
    plt.figure(figsize=(12, 8))
    top30_expected = df_expected.head(30)
    plt.barh(range(len(top30_expected)), top30_expected['importance'].values[::-1])
    plt.yticks(range(len(top30_expected)), top30_expected['word'].values[::-1])
    plt.xlabel('é‡è¦åº¦')
    plt.title('æœŸå¾…å€¤E[y]äºˆæ¸¬ é‡è¦èªTOP30ï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/expected_top30_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… æœŸå¾…å€¤TOP30ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")

# 4-7. P1ï½P4ã®å¯è¦–åŒ–ï¼ˆå®Œäº†ã—ãŸåˆ†æã®ã¿ï¼‰
for key, label in [('p1', 'P1ï¼ˆä½è©•ä¾¡ç¢ºç‡ï¼‰'), ('p2', 'P2ï¼ˆä¸­ä½è©•ä¾¡ç¢ºç‡ï¼‰'), 
                   ('p3', 'P3ï¼ˆä¸­é«˜è©•ä¾¡ç¢ºç‡ï¼‰'), ('p4', 'P4ï¼ˆé«˜è©•ä¾¡ç¢ºç‡ï¼‰')]:
    if key in shap_results:
        df = shap_results[key]['df']
        plt.figure(figsize=(12, 8))
        top30 = df.head(30)
        plt.barh(range(len(top30)), top30['importance'].values[::-1])
        plt.yticks(range(len(top30)), top30['word'].values[::-1])
        plt.xlabel('é‡è¦åº¦')
        plt.title(f'{label}äºˆæ¸¬ é‡è¦èªTOP30ï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
        plt.tight_layout()
        plt.savefig(f"{OUTPUT_DIR}/{key}_top30_factors_production.png", dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… {label}TOP30ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")

# 8. P2ã¨P4ã®æ¯”è¼ƒã‚°ãƒ©ãƒ•ï¼ˆæ•™å¸«ã®æœ€é‡è¦èª²é¡Œï¼‰
if 'p2' in shap_results and 'p4' in shap_results:
    df_p2 = shap_results['p2']['df']
    df_p4 = shap_results['p4']['df']
    plt.figure(figsize=(14, 10))
    top20_p2 = df_p2.head(20)
    top20_p4 = df_p4.head(20)
    x_pos = np.arange(len(top20_p2))
    width = 0.35
    plt.barh(x_pos - width/2, top20_p2['importance'].values[::-1], width, label='P2ï¼ˆä¸­ä½è©•ä¾¡ï¼š2ç‚¹ã‚’æ¸›ã‚‰ã™è¦å› ï¼‰', color='#ff6b6b')
    plt.barh(x_pos + width/2, top20_p4['importance'].values[::-1], width, label='P4ï¼ˆé«˜è©•ä¾¡ï¼š4ç‚¹ã‚’å¢—ã‚„ã™è¦å› ï¼‰', color='#4ecdc4')
    plt.yticks(x_pos, top20_p2['word'].values[::-1])
    plt.xlabel('é‡è¦åº¦')
    plt.title('P2ã¨P4ã®é‡è¦èªæ¯”è¼ƒ TOP20ï¼ˆæ•™å¸«ã®æœ€é‡è¦èª²é¡Œï¼‰', fontsize=14)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/p2_p4_comparison_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… P2ã¨P4æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")
elif all(k in shap_results for k in ['p1', 'p2', 'p3', 'p4']):
    # P1ï½P4 æ¯”è¼ƒï¼ˆå…¨ç¢ºç‡åˆ†å¸ƒã€ã™ã¹ã¦å®Œäº†ã—ã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
    df_p1 = shap_results['p1']['df']
    df_p2 = shap_results['p2']['df']
    df_p3 = shap_results['p3']['df']
    df_p4 = shap_results['p4']['df']
    plt.figure(figsize=(14, 10))
    top20_p1 = df_p1.head(20)
    top20_p2 = df_p2.head(20)
    top20_p3 = df_p3.head(20)
    top20_p4 = df_p4.head(20)
    x_pos = np.arange(len(top20_p1))
    width = 0.2
    plt.barh(x_pos - width*1.5, top20_p1['importance'].values[::-1], width, label='P1ï¼ˆä½è©•ä¾¡ï¼‰')
    plt.barh(x_pos - width*0.5, top20_p2['importance'].values[::-1], width, label='P2ï¼ˆä¸­ä½è©•ä¾¡ï¼‰')
    plt.barh(x_pos + width*0.5, top20_p3['importance'].values[::-1], width, label='P3ï¼ˆä¸­é«˜è©•ä¾¡ï¼‰')
    plt.barh(x_pos + width*1.5, top20_p4['importance'].values[::-1], width, label='P4ï¼ˆé«˜è©•ä¾¡ï¼‰')
    plt.yticks(x_pos, top20_p1['word'].values[::-1])
    plt.xlabel('é‡è¦åº¦')
    plt.title('P1ï½P4ï¼ˆå…¨ç¢ºç‡åˆ†å¸ƒï¼‰é‡è¦èªæ¯”è¼ƒ TOP20', fontsize=14)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/p1_p2_p3_p4_comparison_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… P1ï½P4æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ")
else:
    print("âš ï¸  P2ã¨P4ã€ã¾ãŸã¯P1ï½P4ã®ã™ã¹ã¦ã®åˆ†æãŒå®Œäº†ã—ã¦ã„ãªã„ãŸã‚ã€æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

# 9. ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒãƒ£ãƒ¼ãƒˆ

# 5. ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒãƒ£ãƒ¼ãƒˆ
plt.figure(figsize=(10, 6))
cat_counts = {k: len(v) for k, v in categories.items()}
plt.bar(cat_counts.keys(), cat_counts.values())
plt.ylabel('è¦å› æ•°')
plt.title('è¦å› ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†å¸ƒï¼ˆé †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/factor_categories_chart_production.png", dpi=300, bbox_inches='tight')
plt.close()

print("âœ… å¯è¦–åŒ–å®Œäº†")

# ======================== ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ ========================

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# JSONã‚µãƒãƒªãƒ¼ï¼ˆå®Œäº†ã—ãŸåˆ†æã®ã¿ã‚’å«ã‚ã‚‹ï¼‰
summary = {
    "analysis_date": timestamp,
    "device_used": str(device),
    "pytorch_version": torch.__version__,
    "method": "é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«SHAPåˆ†æ",
    "sample_size": len(sample_texts),
    "completed_analyses": completed_analyses,
    "category_counts": {k: len(v) for k, v in categories.items()},
}

# å®Œäº†ã—ãŸåˆ†æã®æƒ…å ±ã‚’è¿½åŠ 
for key in ['sentiment', 'course', 'expected', 'p1', 'p2', 'p3', 'p4']:
    if key in shap_results:
        df = shap_results[key]['df']
        summary[f"total_words_{key}"] = len(df)
        summary[f"top_{key}_factors"] = dict(df.head(20).values) if len(df) > 0 else {}

# å…±é€šè¦å› ï¼ˆP2ã¨P4ã®æ¯”è¼ƒã€ã¾ãŸã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®æ¯”è¼ƒï¼‰
if 'p2' in shap_results and 'p4' in shap_results:
    df_p2 = shap_results['p2']['df']
    df_p4 = shap_results['p4']['df']
    summary["common_words_count"] = len(set(df_p2['word']) & set(df_p4['word']))
    summary["strong_common_factors"] = [
        {"word": word, "p2_importance": p2_imp, "p4_importance": p4_imp}
        for word, p2_imp, p4_imp in categories.get('strong_common', [])[:20]
    ]
elif 'sentiment' in shap_results and 'course' in shap_results:
    df_sent = shap_results['sentiment']['df']
    df_course = shap_results['course']['df']
    summary["common_words_count"] = len(set(df_sent['word']) & set(df_course['word']))
    summary["strong_common_factors"] = [
        {"word": word, "sentiment": s_imp, "course": c_imp}
        for word, s_imp, c_imp in categories.get('strong_common', [])[:20]
    ]

with open(f"{OUTPUT_DIR}/analysis_summary_production.json", 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ
report = f"""# é †åºå›å¸°ãƒ¢ãƒ‡ãƒ« SHAPåˆ†æçµæœã‚µãƒãƒªãƒ¼

**ä½œæˆæ—¥**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {timestamp}
- åˆ†æå¯¾è±¡: é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆCORALå‹ã€LLPæå¤±ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(sample_texts)}ä»¶
- ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}
- PyTorch version: {torch.__version__}

## åˆ†æçµæœã‚µãƒãƒªãƒ¼
- å®Œäº†ã—ãŸåˆ†æ: {len(completed_analyses)}/{len(analyses)}ä»¶
- å®Œäº†ãƒªã‚¹ãƒˆ: {', '.join(completed_analyses) if completed_analyses else 'ãªã—'}
"""
# å®Œäº†ã—ãŸåˆ†æã®è¦å› æ•°ã‚’è¿½åŠ 
for key, name in [('sentiment', 'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢'), ('course', 'æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'), ('expected', 'æœŸå¾…å€¤E[y]'), 
                  ('p1', 'P1ï¼ˆä½è©•ä¾¡ç¢ºç‡ï¼‰'), ('p2', 'P2ï¼ˆä¸­ä½è©•ä¾¡ç¢ºç‡ï¼‰'), 
                  ('p3', 'P3ï¼ˆä¸­é«˜è©•ä¾¡ç¢ºç‡ï¼‰'), ('p4', 'P4ï¼ˆé«˜è©•ä¾¡ç¢ºç‡ï¼‰')]:
    if key in shap_results:
        report += f"- {name}äºˆæ¸¬è¦å› æ•°: {len(shap_results[key]['df'])}å˜èª\n"

# å…±é€šè¦å› æ•°ï¼ˆP2ã¨P4ã®æ¯”è¼ƒã€ã¾ãŸã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®æ¯”è¼ƒï¼‰
if 'p2' in shap_results and 'p4' in shap_results:
    df_p2 = shap_results['p2']['df']
    df_p4 = shap_results['p4']['df']
    report += f"- å…±é€šè¦å› æ•°: {len(set(df_p2['word']) & set(df_p4['word']))}å˜èª\n"
    report += f"""- å¼·ã„å…±é€šè¦å› æ•°: {len(categories.get('strong_common', []))}å˜èª

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°ï¼ˆP2ã¨P4ã®æ¯”è¼ƒï¼‰

### å¼·ã„å…±é€šè¦å› ï¼ˆP2ã¨P4ã®ä¸¡æ–¹ã§é«˜ã„ï¼‰ ({len(categories.get('strong_common', []))}ä»¶)
| é †ä½ | å˜èª | P2é‡è¦åº¦ï¼ˆ2ç‚¹ã‚’æ¸›ã‚‰ã™ï¼‰ | P4é‡è¦åº¦ï¼ˆ4ç‚¹ã‚’å¢—ã‚„ã™ï¼‰ | ç·åˆé‡è¦åº¦ |
|------|------|----------------------|----------------------|------------|
"""
    for i, (word, p2_imp, p4_imp) in enumerate(categories.get('strong_common', [])[:20], 1):
        report += f"| {i} | {word} | {p2_imp:.6f} | {p4_imp:.6f} | {p2_imp + p4_imp:.6f} |\n"
    
    report += f"""
### P2ç‰¹åŒ–è¦å› ï¼ˆ2ç‚¹ã‚’æ¸›ã‚‰ã™è¦å› ï¼‰ ({len(categories.get('p2_specific', []))}ä»¶)
| é †ä½ | å˜èª | P2é‡è¦åº¦ | P4é‡è¦åº¦ |
|------|------|----------|----------|
"""
    for i, (word, p2_imp, p4_imp) in enumerate(categories.get('p2_specific', [])[:15], 1):
        report += f"| {i} | {word} | {p2_imp:.6f} | {p4_imp:.6f} |\n"
    
    report += f"""
### P4ç‰¹åŒ–è¦å› ï¼ˆ4ç‚¹ã‚’å¢—ã‚„ã™è¦å› ï¼‰ ({len(categories.get('p4_specific', []))}ä»¶)
| é †ä½ | å˜èª | P2é‡è¦åº¦ | P4é‡è¦åº¦ |
|------|------|----------|----------|
"""
    for i, (word, p2_imp, p4_imp) in enumerate(categories.get('p4_specific', [])[:15], 1):
        report += f"| {i} | {word} | {p2_imp:.6f} | {p4_imp:.6f} |\n"
elif 'sentiment' in shap_results and 'course' in shap_results:
    df_sent = shap_results['sentiment']['df']
    df_course = shap_results['course']['df']
    report += f"- å…±é€šè¦å› æ•°: {len(set(df_sent['word']) & set(df_course['word']))}å˜èª\n"
    report += f"""- å¼·ã„å…±é€šè¦å› æ•°: {len(categories.get('strong_common', []))}å˜èª

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°

### å¼·ã„å…±é€šè¦å›  ({len(categories.get('strong_common', []))}ä»¶)
| é †ä½ | å˜èª | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ | ç·åˆé‡è¦åº¦ |
|------|------|------------|------------|------------|
"""
    for i, (word, s_imp, c_imp) in enumerate(categories.get('strong_common', [])[:20], 1):
        report += f"| {i} | {word} | {s_imp:.6f} | {c_imp:.6f} | {s_imp + c_imp:.6f} |\n"
    
    report += f"""
### æ„Ÿæƒ…ç‰¹åŒ–è¦å›  ({len(categories.get('sentiment_specific', []))}ä»¶)
| é †ä½ | å˜èª | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ |
|------|------|------------|------------|
"""
    for i, (word, s_imp, c_imp) in enumerate(categories.get('sentiment_specific', [])[:15], 1):
        report += f"| {i} | {word} | {s_imp:.6f} | {c_imp:.6f} |\n"
    
    report += f"""
### è©•ä¾¡ç‰¹åŒ–è¦å›  ({len(categories.get('course_specific', []))}ä»¶)
| é †ä½ | å˜èª | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ |
|------|------|------------|------------|
"""
    for i, (word, s_imp, c_imp) in enumerate(categories.get('course_specific', [])[:15], 1):
        report += f"| {i} | {word} | {s_imp:.6f} | {c_imp:.6f} |\n"
else:
    report += "\n## ã‚«ãƒ†ã‚´ãƒªåˆ†é¡\n\nã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆP2ã¨P4ã€ã¾ãŸã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®åˆ†æãŒå¿…è¦ã§ã™ï¼‰ã€‚\n"

# å®Œäº†ã—ãŸåˆ†æã®TOP10ã‚’ãƒ¬ãƒãƒ¼ãƒˆã«è¿½åŠ 
for key, title in [('expected', 'æœŸå¾…å€¤E[y]'), ('p1', 'P1ï¼ˆä½è©•ä¾¡ç¢ºç‡ï¼‰'), 
                   ('p2', 'P2ï¼ˆä¸­ä½è©•ä¾¡ç¢ºç‡ï¼‰'), ('p3', 'P3ï¼ˆä¸­é«˜è©•ä¾¡ç¢ºç‡ï¼‰'), 
                   ('p4', 'P4ï¼ˆé«˜è©•ä¾¡ç¢ºç‡ï¼‰')]:
    if key in shap_results:
        df = shap_results[key]['df']
        report += f"""
## {title} TOP10é‡è¦èª
| é †ä½ | å˜èª | é‡è¦åº¦ |
|------|------|--------|
"""
        for i, row in df.head(10).iterrows():
            report += f"| {i+1} | {row['word']} | {row['importance']:.6f} |\n"

report += """
## ä¸»è¦ãªç™ºè¦‹

### 1. é †åºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ï¼ˆP2ã¨P4ã®åˆ†æï¼‰
- **P2ï¼ˆä¸­ä½è©•ä¾¡ç¢ºç‡ï¼‰ã¸ã®SHAPåˆ†æ**: 2ç‚¹ã‚’æ¸›ã‚‰ã™è¦å› ã‚’ç‰¹å®š
- **P4ï¼ˆé«˜è©•ä¾¡ç¢ºç‡ï¼‰ã¸ã®SHAPåˆ†æ**: 4ç‚¹ã‚’å¢—ã‚„ã™è¦å› ã‚’ç‰¹å®š
- å¹³å‡3ç‚¹ã®æ”¹å–„æˆ¦ç•¥ã¨ã—ã¦ã€2ç‚¹ã‚’æ¸›ã‚‰ã—ã€4ç‚¹ã‚’å¢—ã‚„ã™è¦å› ã‚’æ˜ç¢ºã«åˆ†é›¢å¯èƒ½

### 2. ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆP2ã¨P4ã®æ¯”è¼ƒï¼‰
- **å¼·ã„å…±é€šè¦å› **: P2ã¨P4ã®ä¸¡æ–¹ã§é«˜ã„è¦å› ï¼ˆ2ç‚¹ã‚’æ¸›ã‚‰ã—ã€4ç‚¹ã‚’å¢—ã‚„ã™ä¸¡æ–¹ã«å½±éŸ¿ï¼‰
- **P2ç‰¹åŒ–è¦å› **: 2ç‚¹ã‚’æ¸›ã‚‰ã™è¦å› ï¼ˆæ•™å¸«ã®æœ€é‡è¦èª²é¡Œï¼‰
- **P4ç‰¹åŒ–è¦å› **: 4ç‚¹ã‚’å¢—ã‚„ã™è¦å› 
- **P2å¯„ã‚Šè¦å› **: P2ã«ã‚ˆã‚Šå¼·ãå½±éŸ¿ã™ã‚‹è¦å› 
- **P4å¯„ã‚Šè¦å› **: P4ã«ã‚ˆã‚Šå¼·ãå½±éŸ¿ã™ã‚‹è¦å› 

### 3. æˆæ¥­æ”¹å–„ã¸ã®ç¤ºå”†ï¼ˆæ•™å¸«ã®è¦–ç‚¹ï¼šå¹³å‡3ç‚¹ã®æ”¹å–„æˆ¦ç•¥ï¼‰
1. **å„ªå…ˆåº¦1ï¼ˆæœ€é‡è¦ï¼‰**: P2ç‰¹åŒ–è¦å› ã®å›é¿ãƒ»æ”¹å–„ï¼ˆ2ç‚¹ã‚’æ¸›ã‚‰ã™ï¼‰
2. **å„ªå…ˆåº¦2**: P4ç‰¹åŒ–è¦å› ã¸ã®æŠ•è³‡ï¼ˆ4ç‚¹ã‚’å¢—ã‚„ã™ï¼‰
3. **å„ªå…ˆåº¦3**: å¼·ã„å…±é€šè¦å› ã‚’é‡è¦–ã—ãŸæˆæ¥­æ”¹å–„ï¼ˆ2ç‚¹ã‚’æ¸›ã‚‰ã—ã€4ç‚¹ã‚’å¢—ã‚„ã™ä¸¡æ–¹ã«åŠ¹æœï¼‰
4. **å„ªå…ˆåº¦4**: P2å¯„ã‚Šè¦å› ã¨P4å¯„ã‚Šè¦å› ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸæ”¹å–„

---
**åˆ†æå®Œäº†**: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S')

with open(f"{OUTPUT_DIR}/ordinal_shap_analysis_summary_production.md", 'w', encoding='utf-8') as f:
    f.write(report)

print("âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†")

print("\n" + "="*60)
print("âœ… å…¨ã¦ã®åˆ†æå®Œäº†ï¼")
print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {OUTPUT_DIR}")
print("="*60)



