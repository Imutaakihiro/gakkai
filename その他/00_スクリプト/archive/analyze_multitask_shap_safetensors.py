#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
safetensorså¯¾å¿œãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ
æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®è¦å› åˆ†æï¼ˆPyTorch 2.3.1å¯¾å¿œç‰ˆï¼‰
"""

import torch
import pandas as pd
import numpy as np
from transformers import BertJapaneseTokenizer, BertModel
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from tqdm import tqdm
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

print("="*60)
print("safetensorså¯¾å¿œãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ")
print("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®è¦å› åˆ†æï¼ˆPyTorch 2.3.1å¯¾å¿œç‰ˆï¼‰")
print("="*60)

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"PyTorch version: {torch.__version__}")

# ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class ClassLevelMultitaskModel(torch.nn.Module):
    """æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆsafetensorså¯¾å¿œï¼‰"""
    
    def __init__(self, model_name='koheiduck/bert-japanese-finetuned-sentiment', dropout_rate=0.3):
        super(ClassLevelMultitaskModel, self).__init__()
        
        # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆsafetensorså¯¾å¿œï¼‰
        try:
            self.bert = BertModel.from_pretrained(
                model_name,
                from_tf=False,
                from_flax=False,
                from_safetensors=True
            )
            print("âœ… safetensorså½¢å¼ã§BERTãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ safetensorsèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ é€šå¸¸å½¢å¼ã§èª­ã¿è¾¼ã¿ã¾ã™...")
            self.bert = BertModel.from_pretrained(model_name)
        
        self.dropout = torch.nn.Dropout(dropout_rate)
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.sentiment_head = torch.nn.Sequential(
            torch.nn.Linear(768, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(256, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(64, 1)
        )
        
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.course_head = torch.nn.Sequential(
            torch.nn.Linear(768, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(256, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(64, 1)
        )
    
    def forward(self, input_ids, attention_mask):
        """ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹"""
        # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        
        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        sentiment_pred = self.sentiment_head(pooled_output)
        course_pred = self.course_head(pooled_output)
        
        return sentiment_pred, course_pred

def stratified_sampling(df, n_samples=200):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒã‚’è€ƒæ…®ã—ãŸå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
    print(f"ğŸ“Š å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹: {len(df)}ä»¶ã‹ã‚‰{n_samples}ä»¶ã‚’æŠ½å‡º")
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã§5åˆ†å‰²
    df['sentiment_bin'] = pd.qcut(df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'], q=5, labels=False, duplicates='drop')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã§5åˆ†å‰²  
    df['course_bin'] = pd.qcut(df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'], q=5, labels=False, duplicates='drop')
    
    # å„å±¤ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sampled_df = df.groupby(['sentiment_bin', 'course_bin']).apply(
        lambda x: x.sample(min(len(x), max(1, n_samples//25)), random_state=42)
    ).reset_index(drop=True)
    
    print(f"âœ… å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(sampled_df)}ä»¶ã‚’æŠ½å‡º")
    return sampled_df

def predict_sentiment_only(texts):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ã¿ã‚’äºˆæ¸¬ã™ã‚‹é–¢æ•°"""
    with torch.no_grad():
        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        sentiment_pred, _ = model(inputs['input_ids'], inputs['attention_mask'])
        return sentiment_pred.cpu().numpy()

def predict_course_only(texts):
    """æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿ã‚’äºˆæ¸¬ã™ã‚‹é–¢æ•°"""
    with torch.no_grad():
        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        _, course_pred = model(inputs['input_ids'], inputs['attention_mask'])
        return course_pred.cpu().numpy()

def classify_factors(sentiment_shap_dict, course_shap_dict):
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŸºæº–ã§è¦å› ã‚’5ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
    print("ğŸ” è¦å› ã®åˆ†é¡é–‹å§‹...")
    
    # ä¸Šä½ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã®é–¾å€¤è¨ˆç®—
    sentiment_values = list(sentiment_shap_dict.values())
    course_values = list(course_shap_dict.values())
    
    sentiment_top20 = np.percentile(sentiment_values, 80)
    sentiment_top10 = np.percentile(sentiment_values, 90)
    sentiment_top30 = np.percentile(sentiment_values, 70)
    
    course_top20 = np.percentile(course_values, 80)
    course_top10 = np.percentile(course_values, 90)
    course_top30 = np.percentile(course_values, 70)
    
    categories = {
        'strong_common': [],
        'sentiment_leaning': [],
        'course_leaning': [],
        'sentiment_specific': [],
        'course_specific': []
    }
    
    for word in set(sentiment_shap_dict.keys()) | set(course_shap_dict.keys()):
        s_shap = sentiment_shap_dict.get(word, 0)
        c_shap = course_shap_dict.get(word, 0)
        
        # å¼·ã„å…±é€šè¦å› 
        if s_shap >= sentiment_top20 and c_shap >= course_top20:
            if (s_shap > 0 and c_shap > 0) or (s_shap < 0 and c_shap < 0):
                categories['strong_common'].append((word, s_shap, c_shap))
        
        # æ„Ÿæƒ…å¯„ã‚Šè¦å› 
        elif s_shap >= sentiment_top10 and c_shap >= course_top30:
            categories['sentiment_leaning'].append((word, s_shap, c_shap))
        
        # è©•ä¾¡å¯„ã‚Šè¦å› 
        elif c_shap >= course_top10 and s_shap >= sentiment_top30:
            categories['course_leaning'].append((word, s_shap, c_shap))
        
        # æ„Ÿæƒ…ç‰¹åŒ–è¦å› 
        elif s_shap >= sentiment_top20 and c_shap < course_top30:
            categories['sentiment_specific'].append((word, s_shap, c_shap))
        
        # è©•ä¾¡ç‰¹åŒ–è¦å› 
        elif c_shap >= course_top20 and s_shap < sentiment_top30:
            categories['course_specific'].append((word, s_shap, c_shap))
    
    # å„ã‚«ãƒ†ã‚´ãƒªã‚’SHAPå€¤ã§ã‚½ãƒ¼ãƒˆ
    for category in categories:
        categories[category].sort(key=lambda x: abs(x[1]) + abs(x[2]), reverse=True)
    
    print("âœ… è¦å› ã®åˆ†é¡å®Œäº†")
    return categories

def create_visualizations(sentiment_shap_dict, course_shap_dict, categories, output_dir):
    """å¯è¦–åŒ–ã®ä½œæˆ"""
    print("ğŸ“Š å¯è¦–åŒ–ã®ä½œæˆé–‹å§‹...")
    
    # 1. å€‹åˆ¥ã‚¿ã‚¹ã‚¯åˆ†æ
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20
    sentiment_top20 = sorted(sentiment_shap_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    plt.figure(figsize=(12, 8))
    words, values = zip(*sentiment_top20)
    plt.barh(range(len(words)), values)
    plt.yticks(range(len(words)), words)
    plt.xlabel('SHAPå€¤')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (safetensorså¯¾å¿œ)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/sentiment_top20_factors_safetensors.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20
    course_top20 = sorted(course_shap_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    plt.figure(figsize=(12, 8))
    words, values = zip(*course_top20)
    plt.barh(range(len(words)), values)
    plt.yticks(range(len(words)), words)
    plt.xlabel('SHAPå€¤')
    plt.title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (safetensorså¯¾å¿œ)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/course_top20_factors_safetensors.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. æ¯”è¼ƒåˆ†æ
    # 2ã‚¿ã‚¹ã‚¯ã®SHAPå€¤æ•£å¸ƒå›³
    plt.figure(figsize=(10, 8))
    common_words = set(sentiment_shap_dict.keys()) & set(course_shap_dict.keys())
    x_values = [sentiment_shap_dict[word] for word in common_words]
    y_values = [course_shap_dict[word] for word in common_words]
    plt.scatter(x_values, y_values, alpha=0.6)
    plt.xlabel('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬SHAPå€¤')
    plt.ylabel('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬SHAPå€¤')
    plt.title('2ã‚¿ã‚¹ã‚¯ã®SHAPå€¤æ•£å¸ƒå›³ (safetensorså¯¾å¿œ)')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_comparison_scatter_safetensors.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. çµ±åˆåˆ†æ
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°
    category_counts = {cat: len(items) for cat, items in categories.items()}
    plt.figure(figsize=(10, 6))
    categories_names = ['å¼·ã„å…±é€šè¦å› ', 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 'è©•ä¾¡å¯„ã‚Šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ', 'è©•ä¾¡ç‰¹åŒ–è¦å› ']
    counts = list(category_counts.values())
    plt.bar(categories_names, counts)
    plt.title('ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•° (safetensorså¯¾å¿œ)')
    plt.ylabel('è¦å› æ•°')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_categories_chart_safetensors.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… å¯è¦–åŒ–ã®ä½œæˆå®Œäº†")

def save_results(sentiment_shap_dict, course_shap_dict, categories, output_dir):
    """çµæœã®ä¿å­˜"""
    print("ğŸ’¾ çµæœã®ä¿å­˜é–‹å§‹...")
    
    # CSVå½¢å¼ã§ä¿å­˜
    sentiment_df = pd.DataFrame(list(sentiment_shap_dict.items()), columns=['word', 'shap_value'])
    sentiment_df = sentiment_df.sort_values('shap_value', ascending=False)
    sentiment_df.to_csv(f"{output_dir}/word_importance_sentiment_safetensors.csv", index=False, encoding='utf-8')
    
    course_df = pd.DataFrame(list(course_shap_dict.items()), columns=['word', 'shap_value'])
    course_df = course_df.sort_values('shap_value', ascending=False)
    course_df.to_csv(f"{output_dir}/word_importance_course_safetensors.csv", index=False, encoding='utf-8')
    
    # JSONå½¢å¼ã§ä¿å­˜
    categories_json = {}
    for category, items in categories.items():
        categories_json[category] = [
            {'word': word, 'sentiment_shap': s_shap, 'course_shap': c_shap}
            for word, s_shap, c_shap in items
        ]
    
    with open(f"{output_dir}/factor_categories_safetensors.json", 'w', encoding='utf-8') as f:
        json.dump(categories_json, f, ensure_ascii=False, indent=2)
    
    # åˆ†æã‚µãƒãƒªãƒ¼
    summary = {
        'analysis_date': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'device_used': str(device),
        'pytorch_version': torch.__version__,
        'safetensors_enabled': True,
        'total_words_sentiment': len(sentiment_shap_dict),
        'total_words_course': len(course_shap_dict),
        'common_words': len(set(sentiment_shap_dict.keys()) & set(course_shap_dict.keys())),
        'category_counts': {cat: len(items) for cat, items in categories.items()},
        'top_sentiment_factors': dict(list(sentiment_shap_dict.items())[:10]),
        'top_course_factors': dict(list(course_shap_dict.items())[:10])
    }
    
    with open(f"{output_dir}/analysis_summary_safetensors.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("âœ… çµæœã®ä¿å­˜å®Œäº†")

def create_summary_report(categories, output_dir):
    """åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
    print("ğŸ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆé–‹å§‹...")
    
    report = f"""# safetensorså¯¾å¿œãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æçµæœã‚µãƒãƒªãƒ¼

## åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- åˆ†æå¯¾è±¡: æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆsafetensorså¯¾å¿œï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: 200ä»¶ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}
- PyTorch version: {torch.__version__}
- safetensorså¯¾å¿œ: æœ‰åŠ¹

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°
"""
    
    category_names = {
        'strong_common': 'å¼·ã„å…±é€šè¦å› ',
        'sentiment_leaning': 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 
        'course_leaning': 'è©•ä¾¡å¯„ã‚Šè¦å› ',
        'sentiment_specific': 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ',
        'course_specific': 'è©•ä¾¡ç‰¹åŒ–è¦å› '
    }
    
    for category, items in categories.items():
        report += f"\n### {category_names[category]} ({len(items)}ä»¶)\n"
        if items:
            report += "| é †ä½ | å˜èª | æ„Ÿæƒ…SHAP | è©•ä¾¡SHAP |\n"
            report += "|------|------|----------|----------|\n"
            for i, (word, s_shap, c_shap) in enumerate(items[:10], 1):
                report += f"| {i} | {word} | {s_shap:.4f} | {c_shap:.4f} |\n"
        else:
            report += "è©²å½“ã™ã‚‹è¦å› ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    
    report += f"""
## ä¸»è¦ãªç™ºè¦‹

### 1. å¼·ã„å…±é€šè¦å› 
ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯ã§é«˜ã„å¯„ä¸ã‚’ç¤ºã™è¦å› ãŒ{len(categories['strong_common'])}ä»¶ç™ºè¦‹ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã‚‰ã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã«å½±éŸ¿ã™ã‚‹çœŸã®è¦å› ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

### 2. ã‚¿ã‚¹ã‚¯ç‰¹åŒ–è¦å› 
- æ„Ÿæƒ…ç‰¹åŒ–è¦å› : {len(categories['sentiment_specific'])}ä»¶
- è©•ä¾¡ç‰¹åŒ–è¦å› : {len(categories['course_specific'])}ä»¶

ã“ã‚Œã‚‰ã®è¦å› ã¯ã€ãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ã«ç‰¹æœ‰ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã§ã™ã€‚

### 3. safetensorså¯¾å¿œã®åŠ¹æœ
- PyTorch 2.3.1ã®ã¾ã¾å®‰å…¨ã«å‹•ä½œ
- è„†å¼±æ€§å•é¡Œã‚’å›é¿
- DirectMLã¨ã®äº’æ›æ€§ã‚’ç¶­æŒ

### 4. æˆæ¥­æ”¹å–„ã¸ã®ç¤ºå”†
å…±é€šè¦å› ã‚’é‡è¦–ã—ãŸæˆæ¥­æ”¹å–„ã«ã‚ˆã‚Šã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

## ä»Šå¾Œã®èª²é¡Œ
1. å…±é€šè¦å› ã®å› æœé–¢ä¿‚ã®æ¤œè¨¼
2. å®Ÿé¨“çš„æˆæ¥­æ”¹å–„ã®å®Ÿæ–½
3. æ”¹å–„åŠ¹æœã®å®šé‡çš„æ¸¬å®š
4. safetensorså½¢å¼ã®ã•ã‚‰ãªã‚‹æ´»ç”¨
"""
    
    with open(f"{output_dir}/multitask_shap_analysis_summary_safetensors.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆsafetensorså¯¾å¿œï¼‰"""
    print("ğŸš€ safetensorså¯¾å¿œãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æã‚’é–‹å§‹...")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = "../03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_safetensors"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    print("\n=== Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ===")
    data_path = "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv"
    df = pd.read_csv(data_path)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    
    # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sampled_df = stratified_sampling(df, n_samples=200)
    
    # 2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    print("\n=== Phase 2: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆsafetensorså¯¾å¿œï¼‰ ===")
    model_path = "../02_ãƒ¢ãƒ‡ãƒ«/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    tokenizer = BertJapaneseTokenizer.from_pretrained(model_path)
    
    # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model = ClassLevelMultitaskModel()
    model.load_state_dict(torch.load(f"{model_path}/best_multitask_model.pth", map_location=device))
    model.to(device)
    model.eval()
    print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    # 3. SHAPåˆ†æå®Ÿè¡Œ
    print("\n=== Phase 3: SHAPåˆ†æå®Ÿè¡Œï¼ˆsafetensorså¯¾å¿œï¼‰ ===")
    texts_sample = sampled_df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].tolist()
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ
    print("ğŸ§  æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ...")
    explainer_sentiment = shap.Explainer(predict_sentiment_only, tokenizer)
    shap_values_sentiment = explainer_sentiment(texts_sample[:50])  # 50ä»¶ã§ãƒ†ã‚¹ãƒˆ
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ
    print("ğŸ“Š æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ...")
    explainer_course = shap.Explainer(predict_course_only, tokenizer)
    shap_values_course = explainer_course(texts_sample[:50])  # 50ä»¶ã§ãƒ†ã‚¹ãƒˆ
    
    # 4. è¦å› åˆ†æã¨åˆ†é¡
    print("\n=== Phase 4: è¦å› åˆ†æã¨åˆ†é¡ ===")
    
    # å˜èªãƒ¬ãƒ™ãƒ«SHAPå€¤ã®é›†ç´„
    sentiment_shap_dict = {}
    course_shap_dict = {}
    
    # å‡ºç¾å›æ•°3å›ä»¥ä¸Šã®å˜èªã®ã¿ã‚’åˆ†æå¯¾è±¡
    word_counts = defaultdict(int)
    for text in texts_sample[:50]:
        tokens = tokenizer.tokenize(text)
        for token in tokens:
            word_counts[token] += 1
    
    # SHAPå€¤ã®é›†ç´„
    for i in range(len(shap_values_sentiment.values)):
        tokens = tokenizer.tokenize(texts_sample[i])
        for j, token in enumerate(tokens):
            if word_counts[token] >= 3:  # å‡ºç¾3å›ä»¥ä¸Š
                if token not in sentiment_shap_dict:
                    sentiment_shap_dict[token] = []
                    course_shap_dict[token] = []
                sentiment_shap_dict[token].append(shap_values_sentiment.values[i][j])
                course_shap_dict[token].append(shap_values_course.values[i][j])
    
    # å¹³å‡SHAPå€¤ã‚’è¨ˆç®—
    sentiment_shap_dict = {word: np.mean(values) for word, values in sentiment_shap_dict.items()}
    course_shap_dict = {word: np.mean(values) for word, values in course_shap_dict.items()}
    
    # è¦å› ã®åˆ†é¡
    categories = classify_factors(sentiment_shap_dict, course_shap_dict)
    
    # 5. çµæœã®ä¿å­˜ã¨å¯è¦–åŒ–
    print("\n=== Phase 5: çµæœã®ä¿å­˜ã¨å¯è¦–åŒ–ï¼ˆsafetensorså¯¾å¿œï¼‰ ===")
    save_results(sentiment_shap_dict, course_shap_dict, categories, output_dir)
    create_visualizations(sentiment_shap_dict, course_shap_dict, categories, output_dir)
    create_summary_report(categories, output_dir)
    
    print("\nğŸ‰ safetensorså¯¾å¿œãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print(f"ğŸ”’ PyTorch 2.3.1ã®ã¾ã¾å®‰å…¨ã«å‹•ä½œã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
