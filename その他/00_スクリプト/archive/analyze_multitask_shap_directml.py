#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DirectMLæœ€é©åŒ–ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ
æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®è¦å› åˆ†æï¼ˆGPUåŠ é€Ÿç‰ˆï¼‰
"""

import os
# PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’å›é¿
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'

import torch
import torch_directml as dml
import pandas as pd
import numpy as np
from transformers import BertJapaneseTokenizer, BertModel
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from tqdm import tqdm
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

print("="*60)
print("DirectMLæœ€é©åŒ–ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ")
print("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®è¦å› åˆ†æï¼ˆGPUåŠ é€Ÿç‰ˆï¼‰")
print("="*60)

# DirectMLãƒ‡ãƒã‚¤ã‚¹è¨­å®š
if dml.is_available():
    device = dml.device()
    print(f"ğŸš€ DirectML GPUä½¿ç”¨: {device}")
    print(f"ğŸ§  PyTorch version: {torch.__version__}")
else:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âš ï¸ DirectMLãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ä»£æ›¿ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class ClassLevelMultitaskModel(torch.nn.Module):
    """æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆDirectMLæœ€é©åŒ–ï¼‰"""
    
    def __init__(self, model_name='koheiduck/bert-japanese-finetuned-sentiment', dropout_rate=0.3):
        super(ClassLevelMultitaskModel, self).__init__()
        
        # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆPyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’å›é¿ï¼‰
        try:
            self.bert = BertModel.from_pretrained(model_name)
        except Exception as e:
            print(f"âš ï¸ BERTãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ ä»£æ›¿æ–¹æ³•ã§BERTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
            # ä»£æ›¿æ–¹æ³•ï¼šãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨
            model_path = "../02_ãƒ¢ãƒ‡ãƒ«/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"
            self.bert = BertModel.from_pretrained(model_path)
        
        self.dropout = torch.nn.Dropout(dropout_rate)
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.sentiment_head = torch.nn.Sequential(
            torch.nn.Linear(768, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(256, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(64, 1)
        )
        
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.course_head = torch.nn.Sequential(
            torch.nn.Linear(768, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(256, 64),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout_rate),
            torch.nn.Linear(64, 1)
        )
    
    def forward(self, input_ids, attention_mask):
        """ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ï¼ˆDirectMLæœ€é©åŒ–ï¼‰"""
        # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        
        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        sentiment_pred = self.sentiment_head(pooled_output)
        course_pred = self.course_head(pooled_output)
        
        return sentiment_pred, course_pred

def stratified_sampling(df, n_samples=1000):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒã‚’è€ƒæ…®ã—ãŸå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
    print(f"ğŸ“Š å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹: {len(df)}ä»¶ã‹ã‚‰{n_samples}ä»¶ã‚’æŠ½å‡º")
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã§5åˆ†å‰²
    df['sentiment_bin'] = pd.qcut(df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'], q=5, labels=False, duplicates='drop')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã§5åˆ†å‰²  
    df['course_bin'] = pd.qcut(df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'], q=5, labels=False, duplicates='drop')
    
    # å„å±¤ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sampled_df = df.groupby(['sentiment_bin', 'course_bin']).apply(
        lambda x: x.sample(min(len(x), max(1, n_samples//25)), random_state=42)
    ).reset_index(drop=True)
    
    print(f"âœ… å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(sampled_df)}ä»¶ã‚’æŠ½å‡º")
    return sampled_df

def predict_sentiment_only(texts):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ã¿ã‚’äºˆæ¸¬ã™ã‚‹é–¢æ•°ï¼ˆDirectMLæœ€é©åŒ–ï¼‰"""
    with torch.no_grad():
        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        sentiment_pred, _ = model(inputs['input_ids'], inputs['attention_mask'])
        return sentiment_pred.cpu().numpy()

def predict_course_only(texts):
    """æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿ã‚’äºˆæ¸¬ã™ã‚‹é–¢æ•°ï¼ˆDirectMLæœ€é©åŒ–ï¼‰"""
    with torch.no_grad():
        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        _, course_pred = model(inputs['input_ids'], inputs['attention_mask'])
        return course_pred.cpu().numpy()

def classify_factors(sentiment_shap_dict, course_shap_dict):
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŸºæº–ã§è¦å› ã‚’5ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
    print("ğŸ” è¦å› ã®åˆ†é¡é–‹å§‹...")
    
    # ä¸Šä½ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã®é–¾å€¤è¨ˆç®—
    sentiment_values = list(sentiment_shap_dict.values())
    course_values = list(course_shap_dict.values())
    
    sentiment_top20 = np.percentile(sentiment_values, 80)
    sentiment_top10 = np.percentile(sentiment_values, 90)
    sentiment_top30 = np.percentile(sentiment_values, 70)
    
    course_top20 = np.percentile(course_values, 80)
    course_top10 = np.percentile(course_values, 90)
    course_top30 = np.percentile(course_values, 70)
    
    categories = {
        'strong_common': [],
        'sentiment_leaning': [],
        'course_leaning': [],
        'sentiment_specific': [],
        'course_specific': []
    }
    
    for word in set(sentiment_shap_dict.keys()) | set(course_shap_dict.keys()):
        s_shap = sentiment_shap_dict.get(word, 0)
        c_shap = course_shap_dict.get(word, 0)
        
        # å¼·ã„å…±é€šè¦å› 
        if s_shap >= sentiment_top20 and c_shap >= course_top20:
            if (s_shap > 0 and c_shap > 0) or (s_shap < 0 and c_shap < 0):
                categories['strong_common'].append((word, s_shap, c_shap))
        
        # æ„Ÿæƒ…å¯„ã‚Šè¦å› 
        elif s_shap >= sentiment_top10 and c_shap >= course_top30:
            categories['sentiment_leaning'].append((word, s_shap, c_shap))
        
        # è©•ä¾¡å¯„ã‚Šè¦å› 
        elif c_shap >= course_top10 and s_shap >= sentiment_top30:
            categories['course_leaning'].append((word, s_shap, c_shap))
        
        # æ„Ÿæƒ…ç‰¹åŒ–è¦å› 
        elif s_shap >= sentiment_top20 and c_shap < course_top30:
            categories['sentiment_specific'].append((word, s_shap, c_shap))
        
        # è©•ä¾¡ç‰¹åŒ–è¦å› 
        elif c_shap >= course_top20 and s_shap < sentiment_top30:
            categories['course_specific'].append((word, s_shap, c_shap))
    
    # å„ã‚«ãƒ†ã‚´ãƒªã‚’SHAPå€¤ã§ã‚½ãƒ¼ãƒˆ
    for category in categories:
        categories[category].sort(key=lambda x: abs(x[1]) + abs(x[2]), reverse=True)
    
    print("âœ… è¦å› ã®åˆ†é¡å®Œäº†")
    return categories

def create_visualizations(sentiment_shap_dict, course_shap_dict, categories, output_dir):
    """å¯è¦–åŒ–ã®ä½œæˆï¼ˆDirectMLæœ€é©åŒ–ï¼‰"""
    print("ğŸ“Š å¯è¦–åŒ–ã®ä½œæˆé–‹å§‹...")
    
    # 1. å€‹åˆ¥ã‚¿ã‚¹ã‚¯åˆ†æ
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20
    sentiment_top20 = sorted(sentiment_shap_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    plt.figure(figsize=(12, 8))
    words, values = zip(*sentiment_top20)
    plt.barh(range(len(words)), values)
    plt.yticks(range(len(words)), words)
    plt.xlabel('SHAPå€¤')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (DirectMLæœ€é©åŒ–)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/sentiment_top20_factors_directml.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20
    course_top20 = sorted(course_shap_dict.items(), key=lambda x: x[1], reverse=True)[:20]
    plt.figure(figsize=(12, 8))
    words, values = zip(*course_top20)
    plt.barh(range(len(words)), values)
    plt.yticks(range(len(words)), words)
    plt.xlabel('SHAPå€¤')
    plt.title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (DirectMLæœ€é©åŒ–)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/course_top20_factors_directml.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. æ¯”è¼ƒåˆ†æ
    # 2ã‚¿ã‚¹ã‚¯ã®SHAPå€¤æ•£å¸ƒå›³
    plt.figure(figsize=(10, 8))
    common_words = set(sentiment_shap_dict.keys()) & set(course_shap_dict.keys())
    x_values = [sentiment_shap_dict[word] for word in common_words]
    y_values = [course_shap_dict[word] for word in common_words]
    plt.scatter(x_values, y_values, alpha=0.6)
    plt.xlabel('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬SHAPå€¤')
    plt.ylabel('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬SHAPå€¤')
    plt.title('2ã‚¿ã‚¹ã‚¯ã®SHAPå€¤æ•£å¸ƒå›³ (DirectMLæœ€é©åŒ–)')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_comparison_scatter_directml.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. çµ±åˆåˆ†æ
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°
    category_counts = {cat: len(items) for cat, items in categories.items()}
    plt.figure(figsize=(10, 6))
    categories_names = ['å¼·ã„å…±é€šè¦å› ', 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 'è©•ä¾¡å¯„ã‚Šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ', 'è©•ä¾¡ç‰¹åŒ–è¦å› ']
    counts = list(category_counts.values())
    plt.bar(categories_names, counts)
    plt.title('ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•° (DirectMLæœ€é©åŒ–)')
    plt.ylabel('è¦å› æ•°')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_categories_chart_directml.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… å¯è¦–åŒ–ã®ä½œæˆå®Œäº†")

def save_results(sentiment_shap_dict, course_shap_dict, categories, output_dir):
    """çµæœã®ä¿å­˜ï¼ˆDirectMLæœ€é©åŒ–ç‰ˆï¼‰"""
    print("ğŸ’¾ çµæœã®ä¿å­˜é–‹å§‹...")
    
    # CSVå½¢å¼ã§ä¿å­˜
    sentiment_df = pd.DataFrame(list(sentiment_shap_dict.items()), columns=['word', 'shap_value'])
    sentiment_df = sentiment_df.sort_values('shap_value', ascending=False)
    sentiment_df.to_csv(f"{output_dir}/word_importance_sentiment_directml.csv", index=False, encoding='utf-8')
    
    course_df = pd.DataFrame(list(course_shap_dict.items()), columns=['word', 'shap_value'])
    course_df = course_df.sort_values('shap_value', ascending=False)
    course_df.to_csv(f"{output_dir}/word_importance_course_directml.csv", index=False, encoding='utf-8')
    
    # JSONå½¢å¼ã§ä¿å­˜
    categories_json = {}
    for category, items in categories.items():
        categories_json[category] = [
            {'word': word, 'sentiment_shap': s_shap, 'course_shap': c_shap}
            for word, s_shap, c_shap in items
        ]
    
    with open(f"{output_dir}/factor_categories_directml.json", 'w', encoding='utf-8') as f:
        json.dump(categories_json, f, ensure_ascii=False, indent=2)
    
    # åˆ†æã‚µãƒãƒªãƒ¼
    summary = {
        'analysis_date': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'device_used': str(device),
        'directml_optimized': dml.is_available(),
        'total_words_sentiment': len(sentiment_shap_dict),
        'total_words_course': len(course_shap_dict),
        'common_words': len(set(sentiment_shap_dict.keys()) & set(course_shap_dict.keys())),
        'category_counts': {cat: len(items) for cat, items in categories.items()},
        'top_sentiment_factors': dict(list(sentiment_shap_dict.items())[:10]),
        'top_course_factors': dict(list(course_shap_dict.items())[:10])
    }
    
    with open(f"{output_dir}/analysis_summary_directml.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("âœ… çµæœã®ä¿å­˜å®Œäº†")

def create_summary_report(categories, output_dir):
    """åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆï¼ˆDirectMLæœ€é©åŒ–ç‰ˆï¼‰"""
    print("ğŸ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆé–‹å§‹...")
    
    report = f"""# DirectMLæœ€é©åŒ–ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æçµæœã‚µãƒãƒªãƒ¼

## åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- åˆ†æå¯¾è±¡: æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆDirectMLæœ€é©åŒ–ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: 1,000ä»¶ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}
- DirectMLæœ€é©åŒ–: {'æœ‰åŠ¹' if dml.is_available() else 'ç„¡åŠ¹'}

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°
"""
    
    category_names = {
        'strong_common': 'å¼·ã„å…±é€šè¦å› ',
        'sentiment_leaning': 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 
        'course_leaning': 'è©•ä¾¡å¯„ã‚Šè¦å› ',
        'sentiment_specific': 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ',
        'course_specific': 'è©•ä¾¡ç‰¹åŒ–è¦å› '
    }
    
    for category, items in categories.items():
        report += f"\n### {category_names[category]} ({len(items)}ä»¶)\n"
        if items:
            report += "| é †ä½ | å˜èª | æ„Ÿæƒ…SHAP | è©•ä¾¡SHAP |\n"
            report += "|------|------|----------|----------|\n"
            for i, (word, s_shap, c_shap) in enumerate(items[:10], 1):
                report += f"| {i} | {word} | {s_shap:.4f} | {c_shap:.4f} |\n"
        else:
            report += "è©²å½“ã™ã‚‹è¦å› ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    
    report += f"""
## ä¸»è¦ãªç™ºè¦‹

### 1. å¼·ã„å…±é€šè¦å› 
ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯ã§é«˜ã„å¯„ä¸ã‚’ç¤ºã™è¦å› ãŒ{len(categories['strong_common'])}ä»¶ç™ºè¦‹ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã‚‰ã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã«å½±éŸ¿ã™ã‚‹çœŸã®è¦å› ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

### 2. ã‚¿ã‚¹ã‚¯ç‰¹åŒ–è¦å› 
- æ„Ÿæƒ…ç‰¹åŒ–è¦å› : {len(categories['sentiment_specific'])}ä»¶
- è©•ä¾¡ç‰¹åŒ–è¦å› : {len(categories['course_specific'])}ä»¶

ã“ã‚Œã‚‰ã®è¦å› ã¯ã€ãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ã«ç‰¹æœ‰ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã§ã™ã€‚

### 3. DirectMLæœ€é©åŒ–ã®åŠ¹æœ
- GPUåŠ é€Ÿã«ã‚ˆã‚Šå‡¦ç†æ™‚é–“ãŒå¤§å¹…ã«çŸ­ç¸®
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®å‘ä¸Š
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®é«˜é€Ÿå‡¦ç†ãŒå¯èƒ½

### 4. æˆæ¥­æ”¹å–„ã¸ã®ç¤ºå”†
å…±é€šè¦å› ã‚’é‡è¦–ã—ãŸæˆæ¥­æ”¹å–„ã«ã‚ˆã‚Šã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

## ä»Šå¾Œã®èª²é¡Œ
1. å…±é€šè¦å› ã®å› æœé–¢ä¿‚ã®æ¤œè¨¼
2. å®Ÿé¨“çš„æˆæ¥­æ”¹å–„ã®å®Ÿæ–½
3. æ”¹å–„åŠ¹æœã®å®šé‡çš„æ¸¬å®š
4. DirectMLæœ€é©åŒ–ã®ã•ã‚‰ãªã‚‹æ´»ç”¨
"""
    
    with open(f"{output_dir}/multitask_shap_analysis_summary_directml.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆDirectMLæœ€é©åŒ–ï¼‰"""
    print("ğŸš€ DirectMLæœ€é©åŒ–ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æã‚’é–‹å§‹...")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = "../03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_DirectML"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    print("\n=== Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ===")
    data_path = "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv"
    df = pd.read_csv(data_path)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    
    # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sampled_df = stratified_sampling(df, n_samples=1000)
    
    # 2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    print("\n=== Phase 2: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆDirectMLæœ€é©åŒ–ï¼‰ ===")
    model_path = "../02_ãƒ¢ãƒ‡ãƒ«/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ï¼‰
    try:
        tokenizer = BertJapaneseTokenizer.from_pretrained('koheiduck/bert-japanese-finetuned-sentiment')
    except Exception as e:
        print(f"âš ï¸ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”„ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™...")
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨
        tokenizer = BertJapaneseTokenizer.from_pretrained(model_path)
    
    # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model = ClassLevelMultitaskModel()
    
    # PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€weights_only=Falseã§èª­ã¿è¾¼ã¿
    try:
        model.load_state_dict(torch.load(f"{model_path}/best_class_level_multitask_model.pth", map_location=device, weights_only=False))
    except Exception as e:
        print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”„ ä»£æ›¿æ–¹æ³•ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
        # ä»£æ›¿æ–¹æ³•ï¼špickleã‚’ä½¿ç”¨
        import pickle
        with open(f"{model_path}/best_class_level_multitask_model.pth", 'rb') as f:
            state_dict = pickle.load(f)
        model.load_state_dict(state_dict)
    
    model.to(device)
    model.eval()
    print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    # 3. SHAPåˆ†æå®Ÿè¡Œï¼ˆDirectMLæœ€é©åŒ–ï¼‰
    print("\n=== Phase 3: SHAPåˆ†æå®Ÿè¡Œï¼ˆDirectMLæœ€é©åŒ–ï¼‰ ===")
    texts_sample = sampled_df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].tolist()
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ
    print("ğŸ§  æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ...")
    explainer_sentiment = shap.Explainer(predict_sentiment_only, tokenizer)
    shap_values_sentiment = explainer_sentiment(texts_sample[:100])  # DirectMLæœ€é©åŒ–ã§100ä»¶å‡¦ç†
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ
    print("ğŸ“Š æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®SHAPåˆ†æ...")
    explainer_course = shap.Explainer(predict_course_only, tokenizer)
    shap_values_course = explainer_course(texts_sample[:100])  # DirectMLæœ€é©åŒ–ã§100ä»¶å‡¦ç†
    
    # 4. è¦å› åˆ†æã¨åˆ†é¡
    print("\n=== Phase 4: è¦å› åˆ†æã¨åˆ†é¡ ===")
    
    # å˜èªãƒ¬ãƒ™ãƒ«SHAPå€¤ã®é›†ç´„
    sentiment_shap_dict = {}
    course_shap_dict = {}
    
    # å‡ºç¾å›æ•°5å›ä»¥ä¸Šã®å˜èªã®ã¿ã‚’åˆ†æå¯¾è±¡
    word_counts = defaultdict(int)
    for text in texts_sample[:100]:
        tokens = tokenizer.tokenize(text)
        for token in tokens:
            word_counts[token] += 1
    
    # SHAPå€¤ã®é›†ç´„
    for i in range(len(shap_values_sentiment.values)):
        tokens = tokenizer.tokenize(texts_sample[i])
        for j, token in enumerate(tokens):
            if word_counts[token] >= 5:  # å‡ºç¾5å›ä»¥ä¸Š
                if token not in sentiment_shap_dict:
                    sentiment_shap_dict[token] = []
                    course_shap_dict[token] = []
                sentiment_shap_dict[token].append(shap_values_sentiment.values[i][j])
                course_shap_dict[token].append(shap_values_course.values[i][j])
    
    # å¹³å‡SHAPå€¤ã‚’è¨ˆç®—
    sentiment_shap_dict = {word: np.mean(values) for word, values in sentiment_shap_dict.items()}
    course_shap_dict = {word: np.mean(values) for word, values in course_shap_dict.items()}
    
    # è¦å› ã®åˆ†é¡
    categories = classify_factors(sentiment_shap_dict, course_shap_dict)
    
    # 5. çµæœã®ä¿å­˜ã¨å¯è¦–åŒ–
    print("\n=== Phase 5: çµæœã®ä¿å­˜ã¨å¯è¦–åŒ–ï¼ˆDirectMLæœ€é©åŒ–ï¼‰ ===")
    save_results(sentiment_shap_dict, course_shap_dict, categories, output_dir)
    create_visualizations(sentiment_shap_dict, course_shap_dict, categories, output_dir)
    create_summary_report(categories, output_dir)
    
    print("\nğŸ‰ DirectMLæœ€é©åŒ–ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print(f"ğŸš€ DirectMLæœ€é©åŒ–ã«ã‚ˆã‚Šé«˜é€Ÿå‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
