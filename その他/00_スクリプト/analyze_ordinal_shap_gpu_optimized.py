#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é †åºå›å¸°ãƒ¢ãƒ‡ãƒ« GPUæœ€å„ªå…ˆSHAPåˆ†æ

**ä½œæˆæ—¥**: 2025å¹´1æœˆ

æ–¹é‡:
- GPUä½¿ç”¨ã‚’æœ€å„ªå…ˆ
- 100%ã®GPUä½¿ç”¨ç‡ã§ãªãã¦ã‚‚ã€GPUã§å‹•ä½œã™ã‚‹ã“ã¨ã‚’å„ªå…ˆ
- ãƒãƒƒãƒå‡¦ç†ã‚’æœ€é©åŒ–ã—ã¦GPUè² è·ã‚’æœ€å¤§åŒ–
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

import torch
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# SHAPã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import shap
except ImportError as e:
    print(f"âŒ SHAPã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ python safe_fix_for_shap.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

# GPUæœ€å„ªå…ˆè¨­å®š
print("="*60)
print("GPUæœ€å„ªå…ˆSHAPåˆ†æ")
print("="*60)

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼ˆGPUå„ªå…ˆï¼‰
def get_device_gpu_priority():
    """GPUã‚’æœ€å„ªå…ˆã§é¸æŠ"""
    # 1. CUDAã‚’è©¦ã™
    if torch.cuda.is_available():
        try:
            device = torch.device("cuda")
            _ = torch.tensor([1.0]).to(device)
            print(f"âœ… CUDAä½¿ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"   GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            return device
        except Exception as e:
            print(f"âš ï¸ CUDAã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. DirectMLã‚’è©¦ã™
    try:
        import torch_directml as dml
        if dml.is_available():
            device = dml.device()
            print(f"âœ… DirectMLä½¿ç”¨")
            return device
    except Exception:
        pass
    
    # 3. CPUï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
    print("âš ï¸ GPUãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CPUã§å®Ÿè¡Œã—ã¾ã™")
    return torch.device("cpu")

device = get_device_gpu_priority()
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
from train_class_level_ordinal_llp import CourseOrdinalLLPModel, BASE_MODEL
from transformers import BertJapaneseTokenizer

print("\nğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
tokenizer = BertJapaneseTokenizer.from_pretrained(BASE_MODEL)
model = CourseOrdinalLLPModel(BASE_MODEL)

MODEL_PATH = os.path.join("..", "02_ãƒ¢ãƒ‡ãƒ«", "æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«", "class_level_ordinal_llp_20251030_162353.pth")
if not os.path.exists(MODEL_PATH):
    MODEL_PATH = os.path.join("02_ãƒ¢ãƒ‡ãƒ«", "æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«", "class_level_ordinal_llp_20251030_162353.pth")

model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
model.to(device)  # GPUã«ç§»å‹•
model.eval()

# GPUãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
if device.type == 'cuda':
    torch.cuda.empty_cache()
    print(f"âœ… GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Œäº†")

print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
CSV_PATH = os.path.join("..", "01_ãƒ‡ãƒ¼ã‚¿", "ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿", "æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv")
if not os.path.exists(CSV_PATH):
    CSV_PATH = os.path.join("01_ãƒ‡ãƒ¼ã‚¿", "ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿", "æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv")

print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
df = pd.read_csv(CSV_PATH)
texts = df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].fillna("").astype(str).tolist()

# ã‚µãƒ³ãƒ—ãƒ«æ•°
MAX_SAMPLES = 100
BATCH_SIZE = 64  # GPUä½¿ç”¨ç‡å‘ä¸Šã®ãŸã‚å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
MAX_LENGTH = 192

if len(texts) > MAX_SAMPLES:
    np.random.seed(42)
    sample_indices = np.random.choice(len(texts), MAX_SAMPLES, replace=False)
    sample_texts = [texts[i] for i in sample_indices]
else:
    sample_texts = texts

print(f"åˆ†æã‚µãƒ³ãƒ—ãƒ«æ•°: {len(sample_texts)}")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}")

# ======================== GPUæœ€é©åŒ–äºˆæ¸¬é–¢æ•° ========================

def predict_sentiment_gpu(list_of_texts):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ï¼ˆGPUæœ€é©åŒ–ç‰ˆï¼‰"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    
    # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    
    pred = []
    model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰
    
    with torch.no_grad():
        for i in range(0, len(list_of_texts), BATCH_SIZE):
            batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH_SIZE]]
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆCPUï¼‰
            encoding = tokenizer(
                batch, 
                padding=True, 
                truncation=True, 
                max_length=MAX_LENGTH, 
                return_tensors="pt"
            )
            
            # GPUã«ä¸€åº¦ã«è»¢é€ï¼ˆéåŒæœŸï¼‰
            input_ids = encoding['input_ids'].to(device, non_blocking=True)
            attention_mask = encoding['attention_mask'].to(device, non_blocking=True)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            
            # GPUã§æ¨è«–
            out = model(input_ids, attention_mask, chunk_mask)
            y_sent_pred = out[3]  # GPUä¸Šã§ä¿æŒ
            
            # CPUã«è»¢é€ï¼ˆæœ€å°é™ï¼‰
            pred.extend(y_sent_pred.cpu().numpy().tolist())
    
    return np.array(pred).reshape(-1, 1)

def predict_course_gpu(list_of_texts):
    """æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ï¼ˆGPUæœ€é©åŒ–ç‰ˆï¼‰"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    
    pred = []
    model.eval()
    
    with torch.no_grad():
        for i in range(0, len(list_of_texts), BATCH_SIZE):
            batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH_SIZE]]
            
            encoding = tokenizer(
                batch, 
                padding=True, 
                truncation=True, 
                max_length=MAX_LENGTH, 
                return_tensors="pt"
            )
            
            input_ids = encoding['input_ids'].to(device, non_blocking=True)
            attention_mask = encoding['attention_mask'].to(device, non_blocking=True)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            
            out = model(input_ids, attention_mask, chunk_mask)
            y_course_pred = out[4]
            
            pred.extend(y_course_pred.cpu().numpy().tolist())
    
    return np.array(pred).reshape(-1, 1)

# ======================== SHAPåˆ†æå®Ÿè¡Œ ========================

OUTPUT_DIR = os.path.join("..", "03_åˆ†æçµæœ", "é †åºå›å¸°SHAP_GPUæœ€é©åŒ–")
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("\n" + "="*60)
print("SHAPåˆ†æå®Ÿè¡Œï¼ˆGPUæœ€å„ªå…ˆï¼‰")
print("="*60)

# GPUä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
if device.type == 'cuda':
    print(f"\nğŸ“Š GPUä½¿ç”¨çŠ¶æ³:")
    print(f"   ãƒ‡ãƒã‚¤ã‚¹: {torch.cuda.get_device_name(0)}")
    print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
    print(f"   æœ€å¤§ãƒ¡ãƒ¢ãƒª: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB")

# 1. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAPåˆ†æ
print("\nğŸ” æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAPåˆ†æå®Ÿè¡Œä¸­...")
print("   ï¼ˆGPUã§æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ï¼‰")

try:
    explainer_sent = shap.Explainer(
        predict_sentiment_gpu, 
        tokenizer,
        max_evals=50  # è¨ˆç®—é‡ã‚’èª¿æ•´
    )
    
    print("   SHAPå€¤è¨ˆç®—ä¸­ï¼ˆGPUæ¨è«–ã‚’ä½¿ç”¨ï¼‰...")
    shap_values_sent = explainer_sent(sample_texts, max_evals=50)
    
    # é‡è¦åº¦ã‚’é›†è¨ˆ
    importance = np.abs(shap_values_sent.values).mean(axis=0)
    words = shap_values_sent.feature_names if hasattr(shap_values_sent, "feature_names") else list(range(len(importance)))
    
    df_sent = pd.DataFrame({
        'word': words,
        'importance': importance.flatten() if importance.ndim > 1 else importance
    }).sort_values('importance', ascending=False)
    
    df_sent.to_csv(f"{OUTPUT_DIR}/word_importance_sentiment_gpu.csv", index=False, encoding='utf-8')
    print(f"âœ… æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAPåˆ†æå®Œäº†: {len(df_sent)}èª")
    
except Exception as e:
    print(f"âŒ æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAPåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()

# 2. æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢SHAPåˆ†æ
print("\nğŸ” æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢SHAPåˆ†æå®Ÿè¡Œä¸­...")
print("   ï¼ˆGPUã§æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ï¼‰")

try:
    explainer_course = shap.Explainer(
        predict_course_gpu,
        tokenizer,
        max_evals=50
    )
    
    print("   SHAPå€¤è¨ˆç®—ä¸­ï¼ˆGPUæ¨è«–ã‚’ä½¿ç”¨ï¼‰...")
    shap_values_course = explainer_course(sample_texts, max_evals=50)
    
    importance = np.abs(shap_values_course.values).mean(axis=0)
    words = shap_values_course.feature_names if hasattr(shap_values_course, "feature_names") else list(range(len(importance)))
    
    df_course = pd.DataFrame({
        'word': words,
        'importance': importance.flatten() if importance.ndim > 1 else importance
    }).sort_values('importance', ascending=False)
    
    df_course.to_csv(f"{OUTPUT_DIR}/word_importance_course_gpu.csv", index=False, encoding='utf-8')
    print(f"âœ… æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢SHAPåˆ†æå®Œäº†: {len(df_course)}èª")
    
except Exception as e:
    print(f"âŒ æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢SHAPåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()

# GPUä½¿ç”¨çŠ¶æ³ã®æœ€çµ‚ç¢ºèª
if device.type == 'cuda':
    print(f"\nğŸ“Š æœ€çµ‚GPUä½¿ç”¨çŠ¶æ³:")
    print(f"   æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB")
    torch.cuda.reset_peak_memory_stats()

print("\n" + "="*60)
print("âœ… GPUæœ€å„ªå…ˆSHAPåˆ†æå®Œäº†ï¼")
print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {OUTPUT_DIR}")
print("="*60)



