#!/usr/bin/env python3
"""
PyTorch/CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ
RTX 5070 Tiå¯¾å¿œã®ãŸã‚ã®ç’°å¢ƒãƒã‚§ãƒƒã‚¯
"""

import torch
import sys
import os

def main():
    print("=" * 60)
    print("ğŸ” PyTorch/CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª")
    print("=" * 60)
    
    # åŸºæœ¬æƒ…å ±
    print(f"Python version: {sys.version}")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA version (built): {torch.version.cuda}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"  Compute capability: {torch.cuda.get_device_capability(i)}")
            print(f"  Total memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.1f} GB")
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ RTX 5070 Ti æ¨å¥¨è¨­å®š")
    print("=" * 60)
    
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
    torch_version = torch.__version__
    cuda_version = torch.version.cuda
    
    print(f"ç¾åœ¨ã®è¨­å®š:")
    print(f"  PyTorch: {torch_version}")
    print(f"  CUDA: {cuda_version}")
    
    # æ¨å¥¨è¨­å®šã®åˆ¤å®š
    recommendations = []
    
    if torch_version.startswith("1.") or torch_version.startswith("2.0") or torch_version.startswith("2.1"):
        recommendations.append("âš ï¸ PyTorch 2.2+ ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å¥¨")
        recommendations.append("   pip install torch==2.3.1 --index-url https://download.pytorch.org/whl/cu121")
    
    if cuda_version and cuda_version.startswith("11."):
        recommendations.append("âš ï¸ CUDA 12.1+ ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å¥¨")
        recommendations.append("   NVIDIAãƒ‰ãƒ©ã‚¤ãƒã‚’æœ€æ–°ç‰ˆã«æ›´æ–°")
    
    if not recommendations:
        print("âœ… ç¾åœ¨ã®è¨­å®šã¯RTX 5070 Tiã«é©ã—ã¦ã„ã¾ã™")
    else:
        print("ğŸ”§ æ¨å¥¨ã•ã‚Œã‚‹æ”¹å–„:")
        for rec in recommendations:
            print(f"  {rec}")
    
    print("\n" + "=" * 60)
    print("ğŸ§ª ç°¡å˜ãªGPUãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    if torch.cuda.is_available():
        try:
            device = torch.device("cuda:0")
            print(f"Testing on {device}...")
            
            # ç°¡å˜ãªè¡Œåˆ—ç©ãƒ†ã‚¹ãƒˆ
            a = torch.randn(1000, 1000, device=device)
            b = torch.randn(1000, 1000, device=device)
            c = torch.matmul(a, b)
            
            print("âœ… GPUè¡Œåˆ—ç©ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            cached = torch.cuda.memory_reserved(0) / (1024**3)
            print(f"GPU memory allocated: {allocated:.2f} GB")
            print(f"GPU memory cached: {cached:.2f} GB")
            
        except Exception as e:
            print(f"âŒ GPUãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
    else:
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    print("\n" + "=" * 60)
    print("å®Œäº†")
    print("=" * 60)

if __name__ == "__main__":
    main()
