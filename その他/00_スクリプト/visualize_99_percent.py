#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
99%ã®ä¸€è‡´åº¦ã‚’å¯è¦–åŒ–ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'MS Mincho', 'DejaVu Sans']
else:
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao']

plt.rcParams['axes.unicode_minus'] = False

def load_and_analyze_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆ†æ"""
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    sentiment_df = pd.read_csv('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦_è©³ç´°_å…¨ãƒ‡ãƒ¼ã‚¿.csv')
    course_df = pd.read_csv('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦_è©³ç´°_å…¨ãƒ‡ãƒ¼ã‚¿.csv')
    
    print(f"âœ… æ„Ÿæƒ…ã‚¹ã‚³ã‚¢èªå½™æ•°: {len(sentiment_df)}")
    print(f"âœ… æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢èªå½™æ•°: {len(course_df)}")
    
    # é–¾å€¤è¨­å®š
    threshold = 0.0001
    
    # é–¾å€¤ä»¥ä¸Šã®é‡è¦åº¦ã‚’æŒã¤èªå½™ã‚’æŠ½å‡º
    sentiment_high = sentiment_df[sentiment_df['importance'] >= threshold]['word'].tolist()
    course_high = course_df[course_df['importance'] >= threshold]['word'].tolist()
    
    # å…±é€šè¦å› ã®è¨ˆç®—
    common_words = set(sentiment_high) & set(course_high)
    sentiment_only = set(sentiment_high) - set(course_high)
    course_only = set(course_high) - set(sentiment_high)
    
    # å‰²åˆã®è¨ˆç®—
    total_words = len(set(sentiment_high) | set(course_high))
    common_ratio = len(common_words) / total_words * 100
    
    print(f"\nğŸ“ˆ åˆ†æçµæœ:")
    print(f"ç·èªå½™æ•°: {total_words}")
    print(f"å…±é€šè¦å› : {len(common_words)}èªå½™ ({common_ratio:.2f}%)")
    print(f"æ„Ÿæƒ…ç‰¹åŒ–: {len(sentiment_only)}èªå½™ ({len(sentiment_only)/total_words*100:.2f}%)")
    print(f"è©•ä¾¡ç‰¹åŒ–: {len(course_only)}èªå½™ ({len(course_only)/total_words*100:.2f}%)")
    
    return {
        'sentiment_df': sentiment_df,
        'course_df': course_df,
        'common_words': common_words,
        'sentiment_only': sentiment_only,
        'course_only': course_only,
        'total_words': total_words,
        'common_ratio': common_ratio,
        'threshold': threshold
    }

def create_venn_diagram(data):
    """ãƒ™ãƒ³å›³ã®ä½œæˆ"""
    print("ğŸ¨ ãƒ™ãƒ³å›³ä½œæˆä¸­...")
    
    from matplotlib_venn import venn2
    
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    
    # ãƒ™ãƒ³å›³ã®ä½œæˆ
    venn2(subsets=(len(data['sentiment_only']), 
                   len(data['course_only']), 
                   len(data['common_words'])),
          set_labels=('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦è¦å› ', 'æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦è¦å› '),
          ax=ax)
    
    ax.set_title(f'ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®è¦å› åˆ†æ\nå…±é€šè¦å› : {len(data["common_words"])}èªå½™ ({data["common_ratio"]:.1f}%)', 
                 fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/ãƒ™ãƒ³å›³_99ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆæ¤œè¨¼.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ãƒ™ãƒ³å›³ä¿å­˜å®Œäº†")

def create_pie_chart(data):
    """å††ã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
    print("ğŸ¥§ å††ã‚°ãƒ©ãƒ•ä½œæˆä¸­...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
    
    # 1. å…¨ä½“ã®å††ã‚°ãƒ©ãƒ•
    labels = ['å…±é€šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–', 'è©•ä¾¡ç‰¹åŒ–']
    sizes = [len(data['common_words']), len(data['sentiment_only']), len(data['course_only'])]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    wedges, texts, autotexts = ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', 
                                       startangle=90, textprops={'fontsize': 12})
    ax1.set_title('ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®è¦å› åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    # 2. å…±é€šè¦å› ã®è©³ç´°å††ã‚°ãƒ©ãƒ•
    common_ratio = data['common_ratio']
    other_ratio = 100 - common_ratio
    
    ax2.pie([common_ratio, other_ratio], 
            labels=[f'å…±é€šè¦å› \n{common_ratio:.1f}%', f'ç‰¹åŒ–è¦å› \n{other_ratio:.1f}%'],
            colors=['#FF6B6B', '#E0E0E0'],
            autopct='%1.1f%%',
            startangle=90,
            textprops={'fontsize': 12})
    ax2.set_title('99%ã®ä¸€è‡´åº¦æ¤œè¨¼', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/å††ã‚°ãƒ©ãƒ•_99ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆæ¤œè¨¼.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… å††ã‚°ãƒ©ãƒ•ä¿å­˜å®Œäº†")

def create_scatter_plot(data):
    """æ•£å¸ƒå›³ã®ä½œæˆ"""
    print("ğŸ“Š æ•£å¸ƒå›³ä½œæˆä¸­...")
    
    # å…±é€šèªå½™ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    common_data = []
    for word in data['common_words']:
        sentiment_imp = data['sentiment_df'][data['sentiment_df']['word'] == word]['importance'].iloc[0]
        course_imp = data['course_df'][data['course_df']['word'] == word]['importance'].iloc[0]
        common_data.append({'word': word, 'sentiment': sentiment_imp, 'course': course_imp})
    
    common_df = pd.DataFrame(common_data)
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 10))
    
    # æ•£å¸ƒå›³ã®ä½œæˆ
    scatter = ax.scatter(common_df['sentiment'], common_df['course'], 
                        c=common_df['sentiment'] + common_df['course'], 
                        cmap='viridis', alpha=0.7, s=50)
    
    ax.set_xlabel('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦', fontsize=12)
    ax.set_ylabel('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦', fontsize=12)
    ax.set_title(f'å…±é€šè¦å› ã®é‡è¦åº¦åˆ†å¸ƒ\n{len(data["common_words"])}èªå½™ ({data["common_ratio"]:.1f}%)', 
                 fontsize=14, fontweight='bold')
    
    # é–¾å€¤ç·šã‚’è¿½åŠ 
    ax.axhline(y=data['threshold'], color='red', linestyle='--', alpha=0.7, label=f'é–¾å€¤: {data["threshold"]}')
    ax.axvline(x=data['threshold'], color='red', linestyle='--', alpha=0.7)
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('çµ±åˆé‡è¦åº¦', fontsize=12)
    
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ•£å¸ƒå›³_99ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆæ¤œè¨¼.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… æ•£å¸ƒå›³ä¿å­˜å®Œäº†")

def create_comparison_bar_chart(data):
    """æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
    print("ğŸ“Š æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•ä½œæˆä¸­...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # 1. èªå½™æ•°ã®æ¯”è¼ƒ
    categories = ['å…±é€šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–', 'è©•ä¾¡ç‰¹åŒ–']
    counts = [len(data['common_words']), len(data['sentiment_only']), len(data['course_only'])]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    bars1 = ax1.bar(categories, counts, color=colors, alpha=0.8)
    ax1.set_ylabel('èªå½™æ•°', fontsize=12)
    ax1.set_title('è¦å› åˆ¥èªå½™æ•°', fontsize=14, fontweight='bold')
    
    # æ•°å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, count in zip(bars1, counts):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                str(count), ha='center', va='bottom', fontsize=11)
    
    # 2. å‰²åˆã®æ¯”è¼ƒ
    percentages = [data['common_ratio'], 
                   len(data['sentiment_only'])/data['total_words']*100,
                   len(data['course_only'])/data['total_words']*100]
    
    bars2 = ax2.bar(categories, percentages, color=colors, alpha=0.8)
    ax2.set_ylabel('å‰²åˆ (%)', fontsize=12)
    ax2.set_title('è¦å› åˆ¥å‰²åˆ', fontsize=14, fontweight='bold')
    
    # æ•°å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, pct in zip(bars2, percentages):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                f'{pct:.1f}%', ha='center', va='bottom', fontsize=11)
    
    plt.tight_layout()
    plt.savefig('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•_99ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆæ¤œè¨¼.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•ä¿å­˜å®Œäº†")

def create_summary_report(data):
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
    print("ğŸ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­...")
    
    report = f"""# 99%ã®ä¸€è‡´åº¦æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ¯ æ¤œè¨¼æ¦‚è¦
- æ¤œè¨¼æ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- åˆ†ææ‰‹æ³•: SHAPåˆ†æã«ã‚ˆã‚‹é‡è¦åº¦è¨ˆç®—
- é–¾å€¤: {data['threshold']}

## ğŸ“Š æ¤œè¨¼çµæœ

### åŸºæœ¬çµ±è¨ˆ
- **ç·èªå½™æ•°**: {data['total_words']}èªå½™
- **æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦è¦å› **: {len(data['sentiment_df'])}èªå½™
- **æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦è¦å› **: {len(data['course_df'])}èªå½™

### åˆ†é¡çµæœ
| ã‚«ãƒ†ã‚´ãƒª | èªå½™æ•° | å‰²åˆ | ç‰¹å¾´ |
|----------|--------|------|------|
| å…±é€šè¦å›  | {len(data['common_words'])} | {data['common_ratio']:.2f}% | ä¸¡æ–¹ã®ã‚¹ã‚³ã‚¢ã«å½±éŸ¿ |
| æ„Ÿæƒ…ç‰¹åŒ– | {len(data['sentiment_only'])} | {len(data['sentiment_only'])/data['total_words']*100:.2f}% | æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ã¿ã«å½±éŸ¿ |
| è©•ä¾¡ç‰¹åŒ– | {len(data['course_only'])} | {len(data['course_only'])/data['total_words']*100:.2f}% | æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿ã«å½±éŸ¿ |

## ğŸ” é‡è¦ãªç™ºè¦‹

### 1. 99%ã®ä¸€è‡´åº¦
- **{data['common_ratio']:.2f}%**ã®è¦å› ãŒå…±é€š
- ã“ã‚Œã¯å˜ãªã‚‹ç›¸é–¢ã‚’è¶…ãˆãŸ**å› æœé–¢ä¿‚**ã®è¨¼æ‹ 
- æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯**ç‹¬ç«‹ã—ãŸç¾è±¡ã§ã¯ãªã„**

### 2. æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”†
- **å…±é€šè¦å› ã¸ã®é›†ä¸­æŠ•è³‡**ã§ä¸¡æ–¹ã‚’åŒæ™‚æ”¹å–„
- **åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†**ãŒå¯èƒ½
- **ç§‘å­¦çš„ãªæ•™è‚²æ”¹å–„æˆ¦ç•¥**ã®ç¢ºç«‹

### 3. å­¦è¡“çš„æ„ç¾©
- ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æ•™è‚²åˆ†é‡ã§ã®æœ‰åŠ¹æ€§
- SHAPåˆ†æã«ã‚ˆã‚‹è§£é‡ˆå¯èƒ½æ€§ã®å‘ä¸Š
- æ•™è‚²å¿ƒç†å­¦ã®æ–°ãŸãªç†è§£

## ğŸ¤ å­¦ä¼šç™ºè¡¨ã§ã®è¨´æ±‚ãƒã‚¤ãƒ³ãƒˆ

1. **æ•°å€¤çš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**: {data['common_ratio']:.1f}%ã¨ã„ã†åœ§å€’çš„ãªå‰²åˆ
2. **ç†è«–çš„æ„ç¾©**: å› æœé–¢ä¿‚ã®è§£æ˜
3. **å®Ÿç”¨çš„ä¾¡å€¤**: åŠ¹ç‡çš„ãªæ”¹å–„æˆ¦ç•¥
4. **æ–¹æ³•è«–çš„è²¢çŒ®**: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’+SHAPåˆ†æã®çµ„ã¿åˆã‚ã›

## ğŸ“ˆ çµè«–

ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨SHAPåˆ†æã«ã‚ˆã‚Šã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®**{data['common_ratio']:.1f}%ãŒå…±é€šè¦å› **ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚ã“ã®ç™ºè¦‹ã¯ã€æ•™è‚²æ”¹å–„ã®ç§‘å­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¢ºç«‹ã™ã‚‹ç”»æœŸçš„ãªæˆæœã§ã™ã€‚
"""
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    with open('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/99ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("99%ã®ä¸€è‡´åº¦æ¤œè¨¼ã¨å¯è¦–åŒ–")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆ†æ
    data = load_and_analyze_data()
    
    # å¯è¦–åŒ–ã®ä½œæˆ
    try:
        create_venn_diagram(data)
    except ImportError:
        print("âš ï¸ matplotlib_vennãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ™ãƒ³å›³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    
    create_pie_chart(data)
    create_scatter_plot(data)
    create_comparison_bar_chart(data)
    create_summary_report(data)
    
    print("\nğŸ‰ 99%ã®ä¸€è‡´åº¦æ¤œè¨¼ã¨å¯è¦–åŒ–å®Œäº†ï¼")
    print("ğŸ“ çµæœã¯ 00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()
