# SHAP分析の具体例とイメージ（日本語モデル）

## 📝 具体的な流れ

### ステップ1: 入力文

```
入力: "先生の説明がわかりやすく、楽しく学べた"
```

---

### ステップ2: トークン化（BERTのWordPiece）

```
トークン化結果:
["先生", "の", "説明", "が", "わかり", "##やすく", "、", "楽しく", "学", "##べ", "##た"]

説明:
- "わかりやすく" → "わかり" + "##やすく"（##は前に続く印）
- "学べた" → "学" + "##べ" + "##た"
```

---

### ステップ3: モデルの予測

```
感情分析モデルの出力:
- ネガティブ: 5%
- ニュートラル: 10%
- ポジティブ: 85% ← 予測

結論: この文はポジティブ！
```

---

### ステップ4: SHAP分析（トークンごとの寄与度）

```
トークン         SHAP値     意味
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
先生             +0.02      ポジティブに少し寄与
の               0.00       影響なし
説明             -0.03      ネガティブに少し寄与（意外！）
が               0.00       影響なし
わかり           +0.25      ポジティブに大きく寄与
##やすく         +0.20      ポジティブに大きく寄与
、               0.00       影響なし
楽しく           +0.38      ポジティブに最大寄与！
学                +0.08      ポジティブに少し寄与
##べ             +0.10      ポジティブに少し寄与
##た             +0.04      ポジティブに少し寄与
```

---

### ステップ5: サブワードを語に統合（ユーザーが見やすく）

```
語               SHAP値     視覚化
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
楽しく           +0.38      🟢🟢🟢🟢 ← 最重要！
わかりやすく     +0.45      🟢🟢🟢🟢 ← (0.25+0.20で合算)
学べた           +0.22      🟢🟢    ← (0.08+0.10+0.04で合算)
先生             +0.02      🟢
説明             -0.03      🔴      ← マイナス（他の文脈では「説明が下手」と使われるため）
```

**解釈:**
- 「楽しく」「わかりやすく」がポジティブ判定に最も貢献
- 「学べた」も貢献
- 「説明」は若干マイナス（他の文で「説明が悪い」と使われるから）

---

## 🎨 実際の可視化イメージ

### 個別文の可視化（SHAP text plot）

```
先生の説明がわかりやすく、楽しく学べた
                ▲▲▲▲▲▲▲  ▲▲▲▲
                重要      最重要

予測: ポジティブ（85%）
```

または色付き：

```
先生の[説明]がわかりやすく、楽しく学べた
      🔴   🟢🟢🟢🟢🟢🟢  🟢🟢🟢🟢
```

---

### 全体の重要語ランキング（複数の文を分析）

```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ポジティブ判定に寄与する重要語 TOP10
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. わかりやすい/わかりやすく    +0.42（平均SHAP値）
2. 楽しい/楽しく                +0.38
3. 面白い                        +0.35
4. 丁寧                          +0.28
5. 興味深い                      +0.26
6. 良い/よい                    +0.24
7. 充実                          +0.22
8. 役立つ                        +0.20
9. 熱心                          +0.18
10. 満足                         +0.17
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ネガティブ判定に寄与する重要語 TOP10
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. つまらない                    -0.45（平均SHAP値）
2. 難しい/難しすぎる            -0.42
3. わかりにくい                  -0.40
4. 退屈                          -0.38
5. 興味がない                    -0.35
6. 大変                          -0.30
7. ついていけない                -0.28
8. 不満                          -0.26
9. 適当                          -0.24
10. 役に立たない                 -0.22
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

---

## 📊 卒論で使える図表

### 図1: 個別事例のSHAP可視化

```
[例1: ポジティブ事例]
入力: "授業内容が興味深く、先生の説明も丁寧でした"
予測: ポジティブ（確率 88%）

重要度:
  興味深く   ██████████ +0.40
  丁寧      ████████   +0.32
  説明      ██         +0.08
  授業内容   █          +0.05
```

### 図2: ネガティブ事例

```
[例2: ネガティブ事例]
入力: "授業が難しすぎてついていけなかった"
予測: ネガティブ（確率 92%）

重要度:
  難しすぎて     ██████████ -0.42
  ついていけない  ████████   -0.35
  授業          ██         -0.08
```

### 図3: 誤分類事例（間違えた例）

```
[例3: 誤分類]
入力: "授業は普通。特に印象に残らなかった"
予測: ニュートラル（確率 65%）← 正解
実際: ニュートラル

重要度:
  普通          ██████████ +0.38（ニュートラル方向）
  特に〜ない     ████████   +0.30
  印象に残らない  ██████     +0.25
```

---

## 🎯 あなたの研究での応用

### 分析1: 満足度を高める要因

```
質問: 学生の満足度を高めるには？

SHAP分析結果より:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
満足度向上に重要な要素（ポジティブ語）:
1. わかりやすさ  +0.42
2. 楽しさ        +0.38
3. 興味深さ      +0.26
━━━━━━━━━━━━━━━━━━━━━━━━━━━━

教員への提言:
→ これらの要素を授業設計に取り入れることで
  学生満足度の向上が期待できる
```

### 分析2: 不満の原因

```
質問: 学生が不満に感じる要因は？

SHAP分析結果より:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
不満に繋がる要素（ネガティブ語）:
1. つまらない    -0.45
2. 難しすぎる    -0.42
3. わかりにくい  -0.40
━━━━━━━━━━━━━━━━━━━━━━━━━━━━

改善提案:
→ 難易度調整と説明方法の改善が必要
```

---

## 💻 実装イメージ（簡略版）

```python
import shap
from transformers import BertJapaneseTokenizer, BertForSequenceClassification

# モデルとトークナイザー
model = BertForSequenceClassification.from_pretrained("あなたのモデル")
tokenizer = BertJapaneseTokenizer.from_pretrained("あなたのモデル")

# 予測関数
def predict(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=-1)
    return probs[:, 2].detach().numpy()  # ポジティブの確率

# SHAP分析
explainer = shap.Explainer(predict, tokenizer)
texts = ["先生の説明がわかりやすく、楽しく学べた"]
shap_values = explainer(texts)

# 可視化
shap.plots.text(shap_values[0])  # 個別文
shap.plots.bar(shap_values)      # 重要度ランキング
```

**これで日本語でも「意味のある単語」単位でSHAPできます！**

---

## ✅ まとめ

### イメージできた？

1. 入力文を単語（サブワード）に分解
2. 各単語がポジティブ/ネガティブにどれくらい寄与するか計算
3. 重要な単語を特定（ランキング）
4. 卒論で図表として使える

**これが日本語SHAPの全体像です！**

---

次: 実際のコードを書いて試してみますか？

