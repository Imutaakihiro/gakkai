#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆæ¥­å˜ä½ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ç”¨ SHAP beeswarmåˆ†æãƒ»å¯è¦–åŒ–
2ã¤ã®ç›®çš„é–¢æ•°ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ãƒ»æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼‰ãã‚Œãã‚Œã§SHAPãƒ»beeswarmãƒ—ãƒ­ãƒƒãƒˆ
"""

import torch
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUIä¸è¦ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
import matplotlib.pyplot as plt

# SHAPã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
try:
    import shap
except ImportError as e:
    print(f"âŒ SHAPã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ä¿®æ­£ã—ã¦ãã ã•ã„:")
    print("   python fix_shap_dependencies.py")
    sys.exit(1)

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: PyTorchã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå•é¡Œã‚’å›é¿
try:
    from train_class_level_ordinal_llp import CourseOrdinalLLPModel, BASE_MODEL
    from transformers import BertJapaneseTokenizer
except ImportError as e:
    print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("\nğŸ’¡ è§£æ±ºæ–¹æ³•:")
    print("   1. NumPyã‚’1.xç³»ã«ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰:")
    print("      python safe_fix_for_shap.py")
    print("   2. ã¾ãŸã¯ã€PyTorchã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
    sys.exit(1)

import glob
import os
import sys

# ãƒ‘ã‚¹è¨­å®šãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦æ›¸ãæ›ãˆãƒ»ã‚³ãƒãƒ³ãƒ‰å¼•æ•°åŒ–å¯ï¼‰
MODEL_PATH = "C:/Users/takahashi.Jupiter/Desktop/å’æ¥­ç ”ç©¶ï¼ˆæ–°ï¼‰/02_ãƒ¢ãƒ‡ãƒ«/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«/class_level_ordinal_llp_20251030_162353.pth"
print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«é‡ã¿: {MODEL_PATH}")
CSV_PATH = "C:/Users/takahashi.Jupiter/Desktop/å’æ¥­ç ”ç©¶ï¼ˆæ–°ï¼‰/01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ å›ç­”åˆ†å¸ƒä»˜ã.csv"
OUTPUT_DIR = "03_åˆ†æçµæœ/ã‚¯ãƒ©ã‚¹ãƒ¬ãƒ™ãƒ«SHAP_Beeswarm"
BATCH = 128  # GPUä½¿ç”¨ç‡æœ€å¤§åŒ–ã®ãŸã‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§å¹…å¢—åŠ 
MAX_SAMPLES = 50
MAX_LENGTH = 192

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼ˆDirectMLå¯¾å¿œï¼‰
def get_device():
    if torch.cuda.is_available():
        try:
            _ = torch.tensor([1.0]).cuda()
            print("âœ… CUDA åˆ©ç”¨")
            return torch.device("cuda")
        except Exception:
            pass
    try:
        import torch_directml as dml
        if dml.is_available():
            print("âœ… DirectML åˆ©ç”¨")
            return dml.device()
    except Exception:
        pass
    print("ğŸ”„ CPU åˆ©ç”¨")
    return torch.device("cpu")

device = get_device()

# 1. ãƒ¢ãƒ‡ãƒ«/ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æº–å‚™
tokenizer = BertJapaneseTokenizer.from_pretrained(BASE_MODEL)
model = CourseOrdinalLLPModel(BASE_MODEL)
model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
model.to(device)
model.eval()

# 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ï¼‰
print("Loading dataset...")
df = pd.read_csv(CSV_PATH)
texts = df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].fillna("").astype(str).tolist()[:MAX_SAMPLES]

print(f"Loaded {len(texts)} samples.")

def predict_sentiment(list_of_texts):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ï¼ˆGPUæœ€å„ªå…ˆãƒ»æœ€é©åŒ–ç‰ˆï¼‰"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    pred = []
    
    # GPUãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    elif hasattr(device, 'empty_cache'):
        device.empty_cache()
    
    model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æ˜ç¤º
    
    with torch.no_grad():  # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        for i in range(0, len(list_of_texts), BATCH):
            batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH]]
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆCPUï¼‰
            encoding = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
            
            # GPUã«éåŒæœŸè»¢é€ï¼ˆé«˜é€ŸåŒ–ï¼‰
            input_ids = encoding['input_ids'].to(device, non_blocking=True)
            attention_mask = encoding['attention_mask'].to(device, non_blocking=True)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            
            # GPUã§æ¨è«–å®Ÿè¡Œ
            out = model(input_ids, attention_mask, chunk_mask)
            y_sent_pred = out[3]  # GPUä¸Šã§ä¿æŒ
            
            # çµæœã®ã¿CPUã«è»¢é€ï¼ˆæœ€å°é™ã®è»¢é€ï¼‰
            pred.extend(y_sent_pred.cpu().numpy().tolist())
    
    return np.array(pred).reshape(-1, 1)

def predict_course(list_of_texts):
    """æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ï¼ˆGPUæœ€å„ªå…ˆãƒ»æœ€é©åŒ–ç‰ˆï¼‰"""
    if isinstance(list_of_texts, str):
        list_of_texts = [list_of_texts]
    pred = []
    
    # GPUãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    elif hasattr(device, 'empty_cache'):
        device.empty_cache()
    
    model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æ˜ç¤º
    
    with torch.no_grad():  # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        for i in range(0, len(list_of_texts), BATCH):
            batch = [str(x) if not isinstance(x, str) else x for x in list_of_texts[i:i+BATCH]]
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆCPUï¼‰
            encoding = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
            
            # GPUã«éåŒæœŸè»¢é€ï¼ˆé«˜é€ŸåŒ–ï¼‰
            input_ids = encoding['input_ids'].to(device, non_blocking=True)
            attention_mask = encoding['attention_mask'].to(device, non_blocking=True)
            chunk_mask = torch.ones(input_ids.shape[:2], dtype=torch.bool, device=device)
            
            # GPUã§æ¨è«–å®Ÿè¡Œ
            out = model(input_ids, attention_mask, chunk_mask)
            y_course_pred = out[4]  # GPUä¸Šã§ä¿æŒ
            
            # çµæœã®ã¿CPUã«è»¢é€ï¼ˆæœ€å°é™ã®è»¢é€ï¼‰
            pred.extend(y_course_pred.cpu().numpy().tolist())
    
    return np.array(pred).reshape(-1, 1)

print("\n=== SHAP(æ„Ÿæƒ…ã‚¹ã‚³ã‚¢)è§£æãƒ»å¯è¦–åŒ– ===")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH} (GPUä½¿ç”¨ç‡æœ€å¤§åŒ–)")
if device.type == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")

# SHAPã®ä¸¦åˆ—å‡¦ç†ã‚’èª¿æ•´ï¼ˆGPUä½¿ç”¨ã‚’ä¿ƒé€²ï¼‰
print("SHAP Explainerä½œæˆä¸­ï¼ˆGPUæ¨è«–ã‚’ä½¿ç”¨ï¼‰...")
explainer_sent = shap.Explainer(predict_sentiment, tokenizer)
print("SHAPå€¤è¨ˆç®—ä¸­ï¼ˆGPUã§æ¨è«–å®Ÿè¡Œï¼‰...")
shap_values_sent = explainer_sent(texts)
plt.figure(figsize=(14, 8))
shap.summary_plot(shap_values_sent, texts, show=False)
plt.title("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬SHAP Beeswarm", fontsize=16)
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/shap_beeswarm_sentiment.png", dpi=300)
plt.close()

print("\n=== SHAP(æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢)è§£æãƒ»å¯è¦–åŒ– ===")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH} (GPUä½¿ç”¨ç‡æœ€å¤§åŒ–)")
if device.type == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")

# SHAPã®ä¸¦åˆ—å‡¦ç†ã‚’èª¿æ•´ï¼ˆGPUä½¿ç”¨ã‚’ä¿ƒé€²ï¼‰
print("SHAP Explainerä½œæˆä¸­ï¼ˆGPUæ¨è«–ã‚’ä½¿ç”¨ï¼‰...")
explainer_course = shap.Explainer(predict_course, tokenizer)
print("SHAPå€¤è¨ˆç®—ä¸­ï¼ˆGPUã§æ¨è«–å®Ÿè¡Œï¼‰...")
shap_values_course = explainer_course(texts)
plt.figure(figsize=(14, 8))
shap.summary_plot(shap_values_course, texts, show=False)
plt.title("æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬SHAP Beeswarm", fontsize=16)
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/shap_beeswarm_course.png", dpi=300)
plt.close()

# é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚‚CSVã§ä¿å­˜
def get_topwords(shap_values, texts, n=30):
    importance = np.abs(shap_values.values).mean(axis=0)
    # tokenizerç³»ã¯ç‰¹å¾´åã®æ•´åˆæ€§ã«æ³¨æ„
    words = shap_values.feature_names if hasattr(shap_values, "feature_names") else list(range(len(importance)))
    idx = np.argsort(importance)[::-1][:n]
    return [(words[i], importance[i]) for i in idx]

sent_top30 = get_topwords(shap_values_sent, texts, n=30)
pd.DataFrame(sent_top30, columns=["word","importance"]).to_csv(f"{OUTPUT_DIR}/shap_top30_words_sentiment.csv", index=False)
course_top30 = get_topwords(shap_values_course, texts, n=30)
pd.DataFrame(course_top30, columns=["word","importance"]).to_csv(f"{OUTPUT_DIR}/shap_top30_words_course.csv", index=False)

print("å…¨ã¦å®Œäº†ï¼å‡ºåŠ›ãƒ‘ã‚¹ï¼š", OUTPUT_DIR)
