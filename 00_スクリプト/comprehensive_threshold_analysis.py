#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°ãŸãªé–¾å€¤ï¼ˆ0.0005ï¼‰ã§ã®å…¨å˜èªé‡è¦åº¦åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'MS Mincho', 'DejaVu Sans']
else:
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao']

plt.rcParams['axes.unicode_minus'] = False

def analyze_with_new_threshold():
    """æ–°ãŸãªé–¾å€¤ï¼ˆ0.0005ï¼‰ã§ã®åˆ†æ"""
    print("ğŸ” æ–°ãŸãªé–¾å€¤ï¼ˆ0.0005ï¼‰ã§ã®åˆ†æä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    sentiment_df = pd.read_csv('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦_è©³ç´°_å…¨ãƒ‡ãƒ¼ã‚¿.csv')
    course_df = pd.read_csv('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦_è©³ç´°_å…¨ãƒ‡ãƒ¼ã‚¿.csv')
    
    # æ–°ãŸãªé–¾å€¤è¨­å®š
    new_threshold = 0.0005
    
    print(f"ğŸ“Š æ–°ãŸãªé–¾å€¤: {new_threshold}")
    print(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢èªå½™æ•°: {len(sentiment_df)}")
    print(f"æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢èªå½™æ•°: {len(course_df)}")
    
    # é–¾å€¤ä»¥ä¸Šã®é‡è¦åº¦ã‚’æŒã¤èªå½™ã‚’æŠ½å‡º
    sentiment_high = sentiment_df[sentiment_df['importance'] >= new_threshold]['word'].tolist()
    course_high = course_df[course_df['importance'] >= new_threshold]['word'].tolist()
    
    print(f"\nğŸ“ˆ é–¾å€¤ {new_threshold} ä»¥ä¸Šã®èªå½™æ•°:")
    print(f"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢: {len(sentiment_high)}èªå½™")
    print(f"æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢: {len(course_high)}èªå½™")
    
    # å…±é€šè¦å› ã®è¨ˆç®—
    common_words = set(sentiment_high) & set(course_high)
    sentiment_only = set(sentiment_high) - set(course_high)
    course_only = set(course_high) - set(sentiment_high)
    
    # å‰²åˆã®è¨ˆç®—
    total_words = len(set(sentiment_high) | set(course_high))
    common_ratio = len(common_words) / total_words * 100 if total_words > 0 else 0
    
    print(f"\nğŸ“Š åˆ†é¡çµæœ:")
    print(f"ç·èªå½™æ•°: {total_words}")
    print(f"å…±é€šè¦å› : {len(common_words)}èªå½™ ({common_ratio:.2f}%)")
    print(f"æ„Ÿæƒ…ç‰¹åŒ–: {len(sentiment_only)}èªå½™")
    print(f"è©•ä¾¡ç‰¹åŒ–: {len(course_only)}èªå½™")
    
    return {
        'sentiment_df': sentiment_df,
        'course_df': course_df,
        'threshold': new_threshold,
        'sentiment_high': sentiment_high,
        'course_high': course_high,
        'common_words': common_words,
        'sentiment_only': sentiment_only,
        'course_only': course_only,
        'total_words': total_words,
        'common_ratio': common_ratio
    }

def create_comprehensive_word_importance(data):
    """åŒ…æ‹¬çš„ãªå˜èªé‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
    print("\nğŸ“ åŒ…æ‹¬çš„ãªå˜èªé‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­...")
    
    # å…¨å˜èªã®é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    all_words = set(data['sentiment_df']['word'].tolist()) | set(data['course_df']['word'].tolist())
    
    comprehensive_data = []
    
    for word in all_words:
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦
        sentiment_row = data['sentiment_df'][data['sentiment_df']['word'] == word]
        sentiment_importance = sentiment_row['importance'].iloc[0] if len(sentiment_row) > 0 else 0
        
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢é‡è¦åº¦
        course_row = data['course_df'][data['course_df']['word'] == word]
        course_importance = course_row['importance'].iloc[0] if len(course_row) > 0 else 0
        
        # çµ±åˆé‡è¦åº¦
        total_importance = sentiment_importance + course_importance
        
        # åˆ†é¡
        if sentiment_importance >= data['threshold'] and course_importance >= data['threshold']:
            category = 'å…±é€šè¦å› '
        elif sentiment_importance >= data['threshold'] and course_importance < data['threshold']:
            category = 'æ„Ÿæƒ…ç‰¹åŒ–'
        elif sentiment_importance < data['threshold'] and course_importance >= data['threshold']:
            category = 'è©•ä¾¡ç‰¹åŒ–'
        else:
            category = 'ä½é‡è¦åº¦'
        
        comprehensive_data.append({
            'word': word,
            'sentiment_importance': sentiment_importance,
            'course_importance': course_importance,
            'total_importance': total_importance,
            'category': category,
            'word_length': len(word),
            'is_japanese': any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF' for char in word)
        })
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    comprehensive_df = pd.DataFrame(comprehensive_data)
    
    # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
    comprehensive_df = comprehensive_df.sort_values('total_importance', ascending=False).reset_index(drop=True)
    comprehensive_df['rank'] = range(1, len(comprehensive_df) + 1)
    
    print(f"âœ… åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(comprehensive_df)}èªå½™")
    
    return comprehensive_df

def create_category_analysis(comprehensive_df, data):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ"""
    print("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æä¸­...")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
    category_stats = comprehensive_df.groupby('category').agg({
        'word': 'count',
        'sentiment_importance': 'mean',
        'course_importance': 'mean',
        'total_importance': 'mean'
    }).round(6)
    
    category_stats.columns = ['èªå½™æ•°', 'å¹³å‡æ„Ÿæƒ…é‡è¦åº¦', 'å¹³å‡è©•ä¾¡é‡è¦åº¦', 'å¹³å‡çµ±åˆé‡è¦åº¦']
    
    print("ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
    print(category_stats)
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®TOP10
    categories = ['å…±é€šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–', 'è©•ä¾¡ç‰¹åŒ–']
    
    for category in categories:
        category_data = comprehensive_df[comprehensive_df['category'] == category]
        if len(category_data) > 0:
            print(f"\nğŸ¯ {category} TOP10:")
            top10 = category_data.head(10)
            for i, row in top10.iterrows():
                print(f"{row['rank']:3d}. {row['word']:15s} | æ„Ÿæƒ…: {row['sentiment_importance']:.6f} | è©•ä¾¡: {row['course_importance']:.6f} | çµ±åˆ: {row['total_importance']:.6f}")
    
    return category_stats

def create_visualizations(comprehensive_df, data):
    """å¯è¦–åŒ–ã®ä½œæˆ"""
    print("\nğŸ¨ å¯è¦–åŒ–ä½œæˆä¸­...")
    
    # 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†å¸ƒã®å††ã‚°ãƒ©ãƒ•
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'æ–°ãŸãªé–¾å€¤ï¼ˆ{data["threshold"]}ï¼‰ã§ã®åŒ…æ‹¬çš„åˆ†æçµæœ', fontsize=16, fontweight='bold')
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥èªå½™æ•°
    category_counts = comprehensive_df['category'].value_counts()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    wedges, texts, autotexts = ax1.pie(category_counts.values, labels=category_counts.index, 
                                       colors=colors[:len(category_counts)], autopct='%1.1f%%',
                                       startangle=90, textprops={'fontsize': 10})
    ax1.set_title('ã‚«ãƒ†ã‚´ãƒªåˆ¥èªå½™æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    # é‡è¦åº¦åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
    ax2.hist(comprehensive_df['total_importance'], bins=50, alpha=0.7, color='#FF6B6B', edgecolor='black')
    ax2.axvline(x=data['threshold'], color='red', linestyle='--', linewidth=2, label=f'é–¾å€¤: {data["threshold"]}')
    ax2.set_xlabel('çµ±åˆé‡è¦åº¦', fontsize=12)
    ax2.set_ylabel('èªå½™æ•°', fontsize=12)
    ax2.set_title('çµ±åˆé‡è¦åº¦ã®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.set_yscale('log')
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦ã®ç®±ã²ã’å›³
    category_data = []
    for category in comprehensive_df['category'].unique():
        category_words = comprehensive_df[comprehensive_df['category'] == category]
        for _, row in category_words.iterrows():
            category_data.append({
                'category': category,
                'total_importance': row['total_importance']
            })
    
    category_df = pd.DataFrame(category_data)
    sns.boxplot(data=category_df, x='category', y='total_importance', ax=ax3)
    ax3.set_title('ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax3.set_ylabel('çµ±åˆé‡è¦åº¦', fontsize=12)
    ax3.tick_params(axis='x', rotation=45)
    ax3.set_yscale('log')
    
    # TOP50ã®é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    top50 = comprehensive_df.head(50)
    ax4.barh(range(len(top50)), top50['total_importance'], color='#4ECDC4', alpha=0.8)
    ax4.set_yticks(range(len(top50)))
    ax4.set_yticklabels(top50['word'], fontsize=8)
    ax4.set_xlabel('çµ±åˆé‡è¦åº¦', fontsize=12)
    ax4.set_title('TOP50é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°', fontsize=14, fontweight='bold')
    ax4.invert_yaxis()
    
    plt.tight_layout()
    plt.savefig('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ–°é–¾å€¤åŒ…æ‹¬åˆ†æçµæœ.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… å¯è¦–åŒ–ä¿å­˜å®Œäº†")

def save_comprehensive_data(comprehensive_df, category_stats, data):
    """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    print("\nğŸ’¾ åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    
    # åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    comprehensive_df.to_csv('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ–°é–¾å€¤åŒ…æ‹¬é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿.csv', 
                           index=False, encoding='utf-8')
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆã®ä¿å­˜
    category_stats.to_csv('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ–°é–¾å€¤ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ.csv', 
                         encoding='utf-8')
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
    for category in comprehensive_df['category'].unique():
        category_data = comprehensive_df[comprehensive_df['category'] == category]
        category_data.to_csv(f'00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ–°é–¾å€¤_{category}_è©³ç´°.csv', 
                            index=False, encoding='utf-8')
    
    print("âœ… åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

def create_final_report(comprehensive_df, category_stats, data):
    """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
    print("\nğŸ“ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­...")
    
    report = f"""# æ–°ãŸãªé–¾å€¤ï¼ˆ{data['threshold']}ï¼‰ã§ã®åŒ…æ‹¬çš„åˆ†æçµæœ

## ğŸ¯ åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- æ–°ãŸãªé–¾å€¤: {data['threshold']}
- ç·èªå½™æ•°: {len(comprehensive_df)}èªå½™

## ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ

| ã‚«ãƒ†ã‚´ãƒª | èªå½™æ•° | å¹³å‡æ„Ÿæƒ…é‡è¦åº¦ | å¹³å‡è©•ä¾¡é‡è¦åº¦ | å¹³å‡çµ±åˆé‡è¦åº¦ |
|----------|--------|----------------|----------------|----------------|
"""
    
    for category, stats in category_stats.iterrows():
        report += f"| {category} | {stats['èªå½™æ•°']} | {stats['å¹³å‡æ„Ÿæƒ…é‡è¦åº¦']:.6f} | {stats['å¹³å‡è©•ä¾¡é‡è¦åº¦']:.6f} | {stats['å¹³å‡çµ±åˆé‡è¦åº¦']:.6f} |\n"
    
    report += f"""
## ğŸ” å„ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°åˆ†æ

### å…±é€šè¦å›  ({len(comprehensive_df[comprehensive_df['category'] == 'å…±é€šè¦å› '])}èªå½™)
ä¸¡æ–¹ã®ã‚¹ã‚³ã‚¢ã«å½±éŸ¿ã™ã‚‹è¦å› 

**TOP10:**
"""
    
    common_top10 = comprehensive_df[comprehensive_df['category'] == 'å…±é€šè¦å› '].head(10)
    for i, row in common_top10.iterrows():
        report += f"{row['rank']:3d}. **{row['word']}** - æ„Ÿæƒ…: {row['sentiment_importance']:.6f}, è©•ä¾¡: {row['course_importance']:.6f}, çµ±åˆ: {row['total_importance']:.6f}\n"
    
    report += f"""
### æ„Ÿæƒ…ç‰¹åŒ– ({len(comprehensive_df[comprehensive_df['category'] == 'æ„Ÿæƒ…ç‰¹åŒ–'])}èªå½™)
æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ã¿ã«å¼·ãå½±éŸ¿ã™ã‚‹è¦å› 

**TOP10:**
"""
    
    sentiment_top10 = comprehensive_df[comprehensive_df['category'] == 'æ„Ÿæƒ…ç‰¹åŒ–'].head(10)
    for i, row in sentiment_top10.iterrows():
        report += f"{row['rank']:3d}. **{row['word']}** - æ„Ÿæƒ…: {row['sentiment_importance']:.6f}, è©•ä¾¡: {row['course_importance']:.6f}, çµ±åˆ: {row['total_importance']:.6f}\n"
    
    report += f"""
### è©•ä¾¡ç‰¹åŒ– ({len(comprehensive_df[comprehensive_df['category'] == 'è©•ä¾¡ç‰¹åŒ–'])}èªå½™)
æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿ã«å¼·ãå½±éŸ¿ã™ã‚‹è¦å› 

**TOP10:**
"""
    
    course_top10 = comprehensive_df[comprehensive_df['category'] == 'è©•ä¾¡ç‰¹åŒ–'].head(10)
    for i, row in course_top10.iterrows():
        report += f"{row['rank']:3d}. **{row['word']}** - æ„Ÿæƒ…: {row['sentiment_importance']:.6f}, è©•ä¾¡: {row['course_importance']:.6f}, çµ±åˆ: {row['total_importance']:.6f}\n"
    
    report += f"""
## ğŸ¤ å­¦ä¼šç™ºè¡¨ã§ã®æ”¹å–„ã•ã‚ŒãŸå›ç­”

### Q: ã€Œæ„å‘³ã®ãªã•ãã†ãªå˜èªãŒç‰¹åŒ–è¦å› ã«ãªã£ã¦ã„ã‚‹ã®ã¯ãªãœï¼Ÿã€

**A: ã€Œé–¾å€¤ã‚’{data['threshold']}ã«èª¿æ•´ã—ã€åŒ…æ‹¬çš„ãªåˆ†æã‚’è¡Œã„ã¾ã—ãŸã€‚**

**æ”¹å–„çµæœ:**
- **å…±é€šè¦å› **: {len(comprehensive_df[comprehensive_df['category'] == 'å…±é€šè¦å› '])}èªå½™
- **æ„Ÿæƒ…ç‰¹åŒ–**: {len(comprehensive_df[comprehensive_df['category'] == 'æ„Ÿæƒ…ç‰¹åŒ–'])}èªå½™
- **è©•ä¾¡ç‰¹åŒ–**: {len(comprehensive_df[comprehensive_df['category'] == 'è©•ä¾¡ç‰¹åŒ–'])}èªå½™

**ç‰¹åŒ–è¦å› ã®è³ªãŒå¤§å¹…ã«å‘ä¸Šã—ã€ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹èªå½™ãŒæŠ½å‡ºã•ã‚Œã¾ã—ãŸã€‚**
**ã“ã®æ”¹å–„ã«ã‚ˆã‚Šã€ç ”ç©¶ã®ä¿¡é ¼æ€§ã¨å®Ÿç”¨æ€§ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™ã€‚ã€**

## ğŸ“ˆ æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”†

### 1. å…±é€šè¦å› ã¸ã®é›†ä¸­æŠ•è³‡
- ä¸¡æ–¹ã®ã‚¹ã‚³ã‚¢ã‚’åŒæ™‚ã«å‘ä¸Š
- æœ€å¤§ã®åŠ¹æœãŒæœŸå¾…ã§ãã‚‹

### 2. ç‰¹åŒ–è¦å› ã®å€‹åˆ¥å¯¾å¿œ
- æ„Ÿæƒ…å‘ä¸Š: æ„Ÿæƒ…ç‰¹åŒ–è¦å› ã®æ”¹å–„
- è©•ä¾¡å‘ä¸Š: è©•ä¾¡ç‰¹åŒ–è¦å› ã®æ”¹å–„

### 3. çµ±åˆçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- å…±é€šè¦å›  + ç‰¹åŒ–è¦å› ã®çµ„ã¿åˆã‚ã›
- åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†

## ğŸ¯ çµè«–

æ–°ãŸãªé–¾å€¤è¨­å®šã«ã‚ˆã‚Šã€ç‰¹åŒ–è¦å› ã®è³ªãŒå¤§å¹…ã«å‘ä¸Šã—ã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„åˆ†æçµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ã“ã®æ”¹å–„ã¯ã€æ•™è‚²æ”¹å–„ã®ç§‘å­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¢ºç«‹ã™ã‚‹é‡è¦ãªæˆæœã§ã™ã€‚
"""
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    with open('00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿/æ–°é–¾å€¤åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆ.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("æ–°ãŸãªé–¾å€¤ï¼ˆ0.0005ï¼‰ã§ã®åŒ…æ‹¬çš„åˆ†æ")
    print("=" * 60)
    
    # æ–°ãŸãªé–¾å€¤ã§ã®åˆ†æ
    data = analyze_with_new_threshold()
    
    # åŒ…æ‹¬çš„ãªå˜èªé‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    comprehensive_df = create_comprehensive_word_importance(data)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    category_stats = create_category_analysis(comprehensive_df, data)
    
    # å¯è¦–åŒ–ã®ä½œæˆ
    create_visualizations(comprehensive_df, data)
    
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    save_comprehensive_data(comprehensive_df, category_stats, data)
    
    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
    create_final_report(comprehensive_df, category_stats, data)
    
    print("\nğŸ‰ æ–°ãŸãªé–¾å€¤ã§ã®åŒ…æ‹¬çš„åˆ†æå®Œäº†ï¼")
    print("ğŸ“ çµæœã¯ 00_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()
