#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬ç•ªç”¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ
å®Ÿéš›ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæœ¬æ ¼çš„ãªè¦å› åˆ†æ
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

# PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’æ ¹æœ¬çš„ã«å›é¿
os.environ['TORCH_DISABLE_SAFETENSORS_WARNING'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['HF_HUB_OFFLINE'] = '1'

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import json
from datetime import datetime
import pickle
import shap

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

print("="*60)
print("æœ¬ç•ªç”¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ")
print("å®Ÿéš›ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæœ¬æ ¼çš„ãªè¦å› åˆ†æ")
print("="*60)

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"PyTorch version: {torch.__version__}")

def load_real_multitask_model():
    """å®Ÿéš›ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    print("ğŸ“¥ å®Ÿéš›ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    model_path = "../02_ãƒ¢ãƒ‡ãƒ«/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"
    
    try:
        # pickleã§ç›´æ¥èª­ã¿è¾¼ã¿
        with open(f"{model_path}/best_class_level_multitask_model.pth", 'rb') as f:
            state_dict = pickle.load(f)
        print("âœ… pickleã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        return state_dict
    except Exception as e:
        print(f"âš ï¸ pickleèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ä»£æ›¿æ–¹æ³•ï¼štorch.loadã‚’ç›´æ¥ä½¿ç”¨
        try:
            state_dict = torch.load(f"{model_path}/best_class_level_multitask_model.pth", 
                                  map_location=device, weights_only=False)
            print("âœ… torch.loadã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            return state_dict
        except Exception as e2:
            print(f"âŒ ã™ã¹ã¦ã®æ–¹æ³•ã§å¤±æ•—: {e2}")
            return None

def create_production_model():
    """æœ¬ç•ªç”¨ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½œæˆï¼ˆæ ¹æœ¬è§£æ±ºç‰ˆãƒ™ãƒ¼ã‚¹ï¼‰"""
    print("ğŸ—ï¸ æœ¬ç•ªç”¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½œæˆä¸­...")
    
    # æ ¹æœ¬è§£æ±ºç‰ˆã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿãªãƒ¢ãƒ‡ãƒ«
    class ProductionMultitaskModel(torch.nn.Module):
        def __init__(self, vocab_size=30000, embedding_dim=128, hidden_dim=64, dropout_rate=0.3):
            super(ProductionMultitaskModel, self).__init__()
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªåŸ‹ã‚è¾¼ã¿å±¤
            self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)
            self.dropout = torch.nn.Dropout(dropout_rate)
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªLSTMï¼ˆåŒæ–¹å‘ãªã—ï¼‰
            self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
            
            # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
            self.sentiment_head = torch.nn.Sequential(
                torch.nn.Linear(hidden_dim, 64),
                torch.nn.ReLU(),
                torch.nn.Dropout(dropout_rate),
                torch.nn.Linear(64, 32),
                torch.nn.ReLU(),
                torch.nn.Dropout(dropout_rate),
                torch.nn.Linear(32, 1)
            )
            
            # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
            self.course_head = torch.nn.Sequential(
                torch.nn.Linear(hidden_dim, 64),
                torch.nn.ReLU(),
                torch.nn.Dropout(dropout_rate),
                torch.nn.Linear(64, 32),
                torch.nn.ReLU(),
                torch.nn.Dropout(dropout_rate),
                torch.nn.Linear(32, 1)
            )
        
        def forward(self, input_ids, attention_mask=None):
            # åŸ‹ã‚è¾¼ã¿
            embedded = self.embedding(input_ids)
            embedded = self.dropout(embedded)
            
            # LSTM
            lstm_out, (hidden, _) = self.lstm(embedded)
            
            # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
            pooled = lstm_out.mean(dim=1)
            
            # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
            sentiment_pred = self.sentiment_head(pooled)
            course_pred = self.course_head(pooled)
            
            return sentiment_pred, course_pred
    
    return ProductionMultitaskModel()

def production_stratified_sampling(df, n_samples=1000):
    """æœ¬ç•ªç”¨ã®å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ1,000ä»¶ï¼‰"""
    print(f"ğŸ“Š æœ¬ç•ªç”¨å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹: {len(df)}ä»¶ã‹ã‚‰{n_samples}ä»¶ã‚’æŠ½å‡º")
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã§5åˆ†å‰²
    df['sentiment_bin'] = pd.qcut(df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'], q=5, labels=False, duplicates='drop')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã§5åˆ†å‰²  
    df['course_bin'] = pd.qcut(df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'], q=5, labels=False, duplicates='drop')
    
    # å„å±¤ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sampled_df = df.groupby(['sentiment_bin', 'course_bin']).apply(
        lambda x: x.sample(min(len(x), max(1, n_samples//25)), random_state=42)
    ).reset_index(drop=True)
    
    print(f"âœ… æœ¬ç•ªç”¨å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(sampled_df)}ä»¶ã‚’æŠ½å‡º")
    return sampled_df

def production_text_preprocessing(texts):
    """æœ¬ç•ªç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰"""
    print("ğŸ”¤ æœ¬ç•ªç”¨ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰...")
    
    processed_texts = []
    word_to_id = {}
    id_counter = 1
    
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
    word_to_id['<PAD>'] = 0
    word_to_id['<UNK>'] = 1
    word_to_id['<START>'] = 2
    word_to_id['<END>'] = 3
    id_counter = 4
    
    for text in texts:
        # æ—¥æœ¬èªã®æ–‡å­—å˜ä½å‡¦ç†ï¼ˆã‚ˆã‚Šé©åˆ‡ï¼‰
        text = str(text).replace('\n', ' ').replace('\t', ' ')
        
        # æ–‡å­—å˜ä½ã§åˆ†å‰²ï¼ˆæ—¥æœ¬èªã«é©ã—ãŸæ–¹æ³•ï¼‰
        chars = list(text)
        
        word_ids = [word_to_id['<START>']]  # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³
        
        for char in chars:
            if char.strip():  # ç©ºç™½æ–‡å­—ä»¥å¤–
                if char not in word_to_id:
                    word_to_id[char] = id_counter
                    id_counter += 1
                word_ids.append(word_to_id[char])
        
        word_ids.append(word_to_id['<END>'])  # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³
        processed_texts.append(word_ids)
    
    print(f"âœ… æœ¬ç•ªç”¨ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†å®Œäº†: {len(word_to_id)}æ–‡å­—")
    return processed_texts, word_to_id

def production_shap_analysis(model, texts, word_to_id, target='sentiment', max_length=256):
    """æœ¬ç•ªç”¨ã®SHAPåˆ†æï¼ˆæ ¹æœ¬è§£æ±ºç‰ˆãƒ™ãƒ¼ã‚¹ï¼‰"""
    print(f"ğŸ§  {target}ã®æœ¬ç•ªç”¨SHAPåˆ†æä¸­...")
    
    word_importance = {}
    
    for i, text_ids in enumerate(texts):
        if len(text_ids) == 0:
            continue
        
        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ‡ã‚Šè©°ã‚
        if len(text_ids) > max_length:
            text_ids = text_ids[:max_length]
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        padded_ids = text_ids + [word_to_id['<PAD>']] * (max_length - len(text_ids))
        
        with torch.no_grad():
            # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã§ã®äºˆæ¸¬
            input_tensor = torch.tensor([padded_ids], dtype=torch.long).to(device)
            
            sentiment_pred, course_pred = model(input_tensor)
            original_pred = sentiment_pred if target == 'sentiment' else course_pred
            
            # å„å˜èªã‚’é™¤å»ã—ãŸå ´åˆã®äºˆæ¸¬
            for j in range(1, len(text_ids) - 1):  # é–‹å§‹ãƒ»çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã¯é™¤å¤–
                # å˜èªã‚’é™¤å»
                modified_ids = text_ids[:j] + text_ids[j+1:]
                if len(modified_ids) > max_length:
                    modified_ids = modified_ids[:max_length]
                
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                padded_modified = modified_ids + [word_to_id['<PAD>']] * (max_length - len(modified_ids))
                
                modified_tensor = torch.tensor([padded_modified], dtype=torch.long).to(device)
                
                sentiment_pred_mod, course_pred_mod = model(modified_tensor)
                modified_pred = sentiment_pred_mod if target == 'sentiment' else course_pred_mod
                
                # é‡è¦åº¦ = äºˆæ¸¬ã®å¤‰åŒ–é‡
                importance = abs(float(original_pred - modified_pred))
                
                # æ–‡å­—IDã‚’æ–‡å­—ã«å¤‰æ›
                char = None
                for c, cid in word_to_id.items():
                    if cid == text_ids[j]:
                        char = c
                        break
                
                if char and char not in ['<PAD>', '<UNK>', '<START>', '<END>'] and char.strip():
                    if char not in word_importance:
                        word_importance[char] = []
                    word_importance[char].append(importance)
    
    # å¹³å‡é‡è¦åº¦ã‚’è¨ˆç®—ï¼ˆå‡ºç¾3å›ä»¥ä¸Šï¼‰
    avg_importance = {}
    for word, importances in word_importance.items():
        if len(importances) >= 3:
            avg_importance[word] = np.mean(importances)
    
    print(f"âœ… {target}ã®æœ¬ç•ªç”¨SHAPåˆ†æå®Œäº†: {len(avg_importance)}æ–‡å­—")
    return avg_importance

def production_classify_factors(sentiment_importance, course_importance):
    """æœ¬ç•ªç”¨ã®è¦å› åˆ†é¡"""
    print("ğŸ” æœ¬ç•ªç”¨è¦å› ã®åˆ†é¡é–‹å§‹...")
    
    sentiment_values = list(sentiment_importance.values())
    course_values = list(course_importance.values())
    
    if len(sentiment_values) == 0 or len(course_values) == 0:
        return {
            'strong_common': [],
            'sentiment_leaning': [],
            'course_leaning': [],
            'sentiment_specific': [],
            'course_specific': []
        }
    
    # ã‚ˆã‚Šå³å¯†ãªé–¾å€¤è¨­å®š
    sentiment_top20 = np.percentile(sentiment_values, 80)
    sentiment_top10 = np.percentile(sentiment_values, 90)
    sentiment_top30 = np.percentile(sentiment_values, 70)
    
    course_top20 = np.percentile(course_values, 80)
    course_top10 = np.percentile(course_values, 90)
    course_top30 = np.percentile(course_values, 70)
    
    categories = {
        'strong_common': [],
        'sentiment_leaning': [],
        'course_leaning': [],
        'sentiment_specific': [],
        'course_specific': []
    }
    
    for word in set(sentiment_importance.keys()) | set(course_importance.keys()):
        s_imp = sentiment_importance.get(word, 0)
        c_imp = course_importance.get(word, 0)
        
        # å¼·ã„å…±é€šè¦å› ï¼ˆä¸¡æ–¹ã§ä¸Šä½20%ä»¥ä¸Šï¼‰
        if s_imp >= sentiment_top20 and c_imp >= course_top20:
            categories['strong_common'].append((word, s_imp, c_imp))
        
        # æ„Ÿæƒ…å¯„ã‚Šè¦å› ï¼ˆæ„Ÿæƒ…ã§ä¸Šä½10%ã€è©•ä¾¡ã§ä¸Šä½30%ä»¥ä¸Šï¼‰
        elif s_imp >= sentiment_top10 and c_imp >= course_top30:
            categories['sentiment_leaning'].append((word, s_imp, c_imp))
        
        # è©•ä¾¡å¯„ã‚Šè¦å› ï¼ˆè©•ä¾¡ã§ä¸Šä½10%ã€æ„Ÿæƒ…ã§ä¸Šä½30%ä»¥ä¸Šï¼‰
        elif c_imp >= course_top10 and s_imp >= sentiment_top30:
            categories['course_leaning'].append((word, s_imp, c_imp))
        
        # æ„Ÿæƒ…ç‰¹åŒ–è¦å› ï¼ˆæ„Ÿæƒ…ã§ä¸Šä½20%ã€è©•ä¾¡ã§ä¸Šä½30%æœªæº€ï¼‰
        elif s_imp >= sentiment_top20 and c_imp < course_top30:
            categories['sentiment_specific'].append((word, s_imp, c_imp))
        
        # è©•ä¾¡ç‰¹åŒ–è¦å› ï¼ˆè©•ä¾¡ã§ä¸Šä½20%ã€æ„Ÿæƒ…ã§ä¸Šä½30%æœªæº€ï¼‰
        elif c_imp >= course_top20 and s_imp < sentiment_top30:
            categories['course_specific'].append((word, s_imp, c_imp))
    
    # å„ã‚«ãƒ†ã‚´ãƒªã‚’é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
    for category in categories:
        categories[category].sort(key=lambda x: abs(x[1]) + abs(x[2]), reverse=True)
    
    print("âœ… æœ¬ç•ªç”¨è¦å› ã®åˆ†é¡å®Œäº†")
    return categories

def create_beeswarm_plots(model, texts, word_to_id, output_dir, max_samples=50):
    """Beeswarmãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ"""
    print("ğŸ Beeswarmãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆé–‹å§‹...")
    
    # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™
    sample_texts = texts[:max_samples]
    print(f"ğŸ“ Beeswarmç”¨ã‚µãƒ³ãƒ—ãƒ«: {len(sample_texts)}ä»¶")
    
    # äºˆæ¸¬é–¢æ•°ã‚’ä½œæˆ
    def predict_sentiment(text_ids_list):
        """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬é–¢æ•°"""
        predictions = []
        for text_ids in text_ids_list:
            if len(text_ids) == 0:
                predictions.append([0.5])
                continue
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            max_length = 256
            if len(text_ids) > max_length:
                text_ids = text_ids[:max_length]
            
            padded_ids = text_ids + [word_to_id['<PAD>']] * (max_length - len(text_ids))
            
            with torch.no_grad():
                input_tensor = torch.tensor([padded_ids], dtype=torch.long).to(device)
                sentiment_pred, course_pred = model(input_tensor)
                predictions.append(sentiment_pred.cpu().numpy()[0])
        
        return np.array(predictions)
    
    def predict_course(text_ids_list):
        """æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬é–¢æ•°"""
        predictions = []
        for text_ids in text_ids_list:
            if len(text_ids) == 0:
                predictions.append([0.5])
                continue
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            max_length = 256
            if len(text_ids) > max_length:
                text_ids = text_ids[:max_length]
            
            padded_ids = text_ids + [word_to_id['<PAD>']] * (max_length - len(text_ids))
            
            with torch.no_grad():
                input_tensor = torch.tensor([padded_ids], dtype=torch.long).to(device)
                sentiment_pred, course_pred = model(input_tensor)
                predictions.append(course_pred.cpu().numpy()[0])
        
        return np.array(predictions)
    
    try:
        # 1. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®Beeswarmãƒ—ãƒ­ãƒƒãƒˆ
        print("ğŸ§  æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®SHAPåˆ†æå®Ÿè¡Œä¸­...")
        explainer_sentiment = shap.Explainer(predict_sentiment)
        shap_values_sentiment = explainer_sentiment(sample_texts)
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values_sentiment, sample_texts, show=False)
        plt.title("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAP Beeswarm Plot", fontsize=16, pad=20, color='#2C3E50')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/multitask_sentiment_beeswarm_production.png", 
                    dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("âœ… æ„Ÿæƒ…ã‚¹ã‚³ã‚¢Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
        # 2. æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®Beeswarmãƒ—ãƒ­ãƒƒãƒˆ
        print("ğŸ“Š æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®SHAPåˆ†æå®Ÿè¡Œä¸­...")
        explainer_course = shap.Explainer(predict_course)
        shap_values_course = explainer_course(sample_texts)
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values_course, sample_texts, show=False)
        plt.title("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢SHAP Beeswarm Plot", fontsize=16, pad=20, color='#2C3E50')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/multitask_course_beeswarm_production.png", 
                    dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("âœ… æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
        # 3. æ¯”è¼ƒç”¨ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ
        print("ğŸ“Š æ¯”è¼ƒç”¨ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆä¸­...")
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        fig.suptitle("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®SHAP Beeswarm Plot æ¯”è¼ƒ", 
                     fontsize=18, color='#2C3E50')
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
        shap.summary_plot(shap_values_sentiment, sample_texts, show=False, ax=axes[0])
        axes[0].set_title("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬", fontsize=14)
        
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢
        shap.summary_plot(shap_values_course, sample_texts, show=False, ax=axes[1])
        axes[1].set_title("æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬", fontsize=14)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/multitask_beeswarm_comparison_production.png", 
                    dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("âœ… æ¯”è¼ƒç”¨ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
        # çµæœã®ä¿å­˜
        beeswarm_results = {
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "analysis_type": "beeswarm_production",
            "sample_size": len(sample_texts),
            "models": {
                "sentiment": {
                    "shap_values_shape": shap_values_sentiment.shape,
                    "model_type": "sentiment_regression"
                },
                "course": {
                    "shap_values_shape": shap_values_course.shape,
                    "model_type": "course_regression"
                }
            },
            "output_files": [
                "multitask_sentiment_beeswarm_production.png",
                "multitask_course_beeswarm_production.png",
                "multitask_beeswarm_comparison_production.png"
            ]
        }
        
        with open(f"{output_dir}/beeswarm_results_production.json", 'w', encoding='utf-8') as f:
            json.dump(beeswarm_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Beeswarmãƒ—ãƒ­ãƒƒãƒˆçµæœä¿å­˜å®Œäº†: {output_dir}")
        
    except Exception as e:
        print(f"âŒ Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”„ ç°¡æ˜“ç‰ˆã‚’å®Ÿè¡Œã—ã¾ã™...")
        
        # ç°¡æ˜“ç‰ˆï¼ˆã‚ˆã‚Šå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰
        try:
            sample_texts_small = sample_texts[:10]  # 10ä»¶ã§ãƒ†ã‚¹ãƒˆ
            
            # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
            explainer_sentiment = shap.Explainer(predict_sentiment)
            shap_values_sentiment = explainer_sentiment(sample_texts_small)
            
            plt.figure(figsize=(10, 6))
            shap.summary_plot(shap_values_sentiment, sample_texts_small, show=False)
            plt.title("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAP Beeswarm Plot (ç°¡æ˜“ç‰ˆ)", fontsize=14)
            plt.tight_layout()
            plt.savefig(f"{output_dir}/multitask_sentiment_beeswarm_simple_production.png", 
                        dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print("âœ… ç°¡æ˜“ç‰ˆBeeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
            
        except Exception as e2:
            print(f"âŒ ç°¡æ˜“ç‰ˆã‚‚ã‚¨ãƒ©ãƒ¼: {e2}")

def production_visualizations(sentiment_importance, course_importance, categories, output_dir):
    """æœ¬ç•ªç”¨ã®å¯è¦–åŒ–"""
    print("ğŸ“Š æœ¬ç•ªç”¨å¯è¦–åŒ–ã®ä½œæˆé–‹å§‹...")
    
    # 1. å€‹åˆ¥ã‚¿ã‚¹ã‚¯åˆ†æï¼ˆTOP30ï¼‰
    sentiment_top30 = sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True)[:30]
    plt.figure(figsize=(14, 10))
    words, values = zip(*sentiment_top30)
    colors = ['red' if v > 0 else 'blue' for v in values]
    plt.barh(range(len(words)), values, color=colors, alpha=0.7)
    plt.yticks(range(len(words)), words)
    plt.xlabel('é‡è¦åº¦')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP30 (æœ¬ç•ªç”¨ãƒ»æ–‡å­—å˜ä½)', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/sentiment_top30_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    course_top30 = sorted(course_importance.items(), key=lambda x: x[1], reverse=True)[:30]
    plt.figure(figsize=(14, 10))
    words, values = zip(*course_top30)
    colors = ['red' if v > 0 else 'blue' for v in values]
    plt.barh(range(len(words)), values, color=colors, alpha=0.7)
    plt.yticks(range(len(words)), words)
    plt.xlabel('é‡è¦åº¦')
    plt.title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP30 (æœ¬ç•ªç”¨ãƒ»æ–‡å­—å˜ä½)', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/course_top30_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 1-2. TOP100åˆ†æï¼ˆè©³ç´°ç‰ˆï¼‰
    sentiment_top100 = sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True)[:100]
    plt.figure(figsize=(16, 20))
    words, values = zip(*sentiment_top100)
    colors = ['red' if v > 0 else 'blue' for v in values]
    plt.barh(range(len(words)), values, color=colors, alpha=0.7)
    plt.yticks(range(len(words)), words, fontsize=8)
    plt.xlabel('é‡è¦åº¦', fontsize=12)
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP100 (æœ¬ç•ªç”¨ãƒ»æ–‡å­—å˜ä½)', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/sentiment_top100_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    course_top100 = sorted(course_importance.items(), key=lambda x: x[1], reverse=True)[:100]
    plt.figure(figsize=(16, 20))
    words, values = zip(*course_top100)
    colors = ['red' if v > 0 else 'blue' for v in values]
    plt.barh(range(len(words)), values, color=colors, alpha=0.7)
    plt.yticks(range(len(words)), words, fontsize=8)
    plt.xlabel('é‡è¦åº¦', fontsize=12)
    plt.title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP100 (æœ¬ç•ªç”¨ãƒ»æ–‡å­—å˜ä½)', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/course_top100_factors_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. æ¯”è¼ƒåˆ†æï¼ˆæ•£å¸ƒå›³ï¼‰
    plt.figure(figsize=(12, 10))
    common_words = set(sentiment_importance.keys()) & set(course_importance.keys())
    x_values = [sentiment_importance[word] for word in common_words]
    y_values = [course_importance[word] for word in common_words]
    
    # ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
    correlation = np.corrcoef(x_values, y_values)[0, 1]
    
    plt.scatter(x_values, y_values, alpha=0.6, s=50)
    plt.xlabel('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬é‡è¦åº¦', fontsize=12)
    plt.ylabel('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬é‡è¦åº¦', fontsize=12)
    plt.title(f'2ã‚¿ã‚¹ã‚¯ã®é‡è¦åº¦æ•£å¸ƒå›³ (æœ¬ç•ªç”¨)\nç›¸é–¢ä¿‚æ•°: {correlation:.3f}', fontsize=16)
    plt.grid(True, alpha=0.3)
    
    # å¯¾è§’ç·šã‚’è¿½åŠ 
    min_val = min(min(x_values), min(y_values))
    max_val = max(max(x_values), max(y_values))
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5, label='y=x')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_comparison_scatter_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. çµ±åˆåˆ†æï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°ï¼‰
    category_counts = {cat: len(items) for cat, items in categories.items()}
    plt.figure(figsize=(12, 8))
    categories_names = ['å¼·ã„å…±é€šè¦å› ', 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 'è©•ä¾¡å¯„ã‚Šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ', 'è©•ä¾¡ç‰¹åŒ–è¦å› ']
    counts = list(category_counts.values())
    colors = ['gold', 'lightcoral', 'lightblue', 'lightgreen', 'lightpink']
    
    bars = plt.bar(categories_names, counts, color=colors, alpha=0.8)
    plt.title('ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•° (æœ¬ç•ªç”¨)', fontsize=16)
    plt.ylabel('è¦å› æ•°', fontsize=12)
    plt.xticks(rotation=45)
    
    # æ•°å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, count in zip(bars, counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                str(count), ha='center', va='bottom', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_categories_chart_production.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆå…±é€šè¦å› ã®è©³ç´°ï¼‰
    if len(categories['strong_common']) > 0:
        plt.figure(figsize=(12, 8))
        common_factors = categories['strong_common'][:20]  # TOP20
        words = [item[0] for item in common_factors]
        sentiment_vals = [item[1] for item in common_factors]
        course_vals = [item[2] for item in common_factors]
        
        data = np.array([sentiment_vals, course_vals]).T
        plt.imshow(data, cmap='RdYlBu_r', aspect='auto')
        plt.colorbar(label='é‡è¦åº¦')
        plt.yticks(range(len(words)), words)
        plt.xticks([0, 1], ['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢', 'æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'])
        plt.title('å¼·ã„å…±é€šè¦å› ã®é‡è¦åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— (æœ¬ç•ªç”¨)', fontsize=16)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/common_factors_heatmap_production.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    print("âœ… æœ¬ç•ªç”¨å¯è¦–åŒ–ã®ä½œæˆå®Œäº†")

def load_single_model_results():
    """æ—¢å­˜ã®å˜ä¸€ãƒ¢ãƒ‡ãƒ«çµæœã‚’èª­ã¿è¾¼ã‚€"""
    print("ğŸ“¥ æ—¢å­˜ã®å˜ä¸€ãƒ¢ãƒ‡ãƒ«çµæœã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    try:
        # æ—¢å­˜ã®SHAPåˆ†æçµæœã‚’èª­ã¿è¾¼ã¿
        single_sentiment_path = "../03_åˆ†æçµæœ/SHAPåˆ†æ/ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°5000ä»¶/word_importance_sample5000.csv"
        single_course_path = "../03_åˆ†æçµæœ/SHAPåˆ†æ/ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°5000ä»¶/word_importance_sample5000.csv"
        
        single_sentiment_df = pd.read_csv(single_sentiment_path)
        single_sentiment_dict = dict(zip(single_sentiment_df['word'], single_sentiment_df['shap_value']))
        
        print(f"âœ… å˜ä¸€ãƒ¢ãƒ‡ãƒ«çµæœèª­ã¿è¾¼ã¿æˆåŠŸ: {len(single_sentiment_dict)}å˜èª")
        return single_sentiment_dict, {}
        
    except Exception as e:
        print(f"âš ï¸ å˜ä¸€ãƒ¢ãƒ‡ãƒ«çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}, {}

def create_model_comparison_visualizations(multitask_sentiment, multitask_course, single_sentiment, single_course, output_dir):
    """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã¨å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå¯è¦–åŒ–"""
    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå¯è¦–åŒ–ã®ä½œæˆé–‹å§‹...")
    
    # 1. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®æ¯”è¼ƒï¼ˆTOP50ï¼‰
    multitask_top50 = sorted(multitask_sentiment.items(), key=lambda x: x[1], reverse=True)[:50]
    single_top50 = sorted(single_sentiment.items(), key=lambda x: x[1], reverse=True)[:50]
    
    # å…±é€šè¦å› ã®ç‰¹å®š
    multitask_words = set(multitask_sentiment.keys())
    single_words = set(single_sentiment.keys())
    common_words = multitask_words & single_words
    
    # æ¯”è¼ƒæ•£å¸ƒå›³
    plt.figure(figsize=(14, 10))
    x_values = [multitask_sentiment[word] for word in common_words]
    y_values = [single_sentiment[word] for word in common_words]
    
    correlation = np.corrcoef(x_values, y_values)[0, 1] if len(x_values) > 1 else 0.0
    
    plt.scatter(x_values, y_values, alpha=0.6, s=50)
    plt.xlabel('ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«é‡è¦åº¦', fontsize=12)
    plt.ylabel('å˜ä¸€ãƒ¢ãƒ‡ãƒ«é‡è¦åº¦', fontsize=12)
    plt.title(f'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ vs å˜ä¸€ãƒ¢ãƒ‡ãƒ«\nç›¸é–¢ä¿‚æ•°: {correlation:.3f}', fontsize=16)
    plt.grid(True, alpha=0.3)
    
    # å¯¾è§’ç·šã‚’è¿½åŠ 
    if len(x_values) > 0 and len(y_values) > 0:
        min_val = min(min(x_values), min(y_values))
        max_val = max(max(x_values), max(y_values))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5, label='y=x')
        plt.legend()
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/model_comparison_sentiment_scatter.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. TOP20ã®æ¯”è¼ƒãƒãƒ¼
    plt.figure(figsize=(16, 10))
    
    # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯TOP20
    multitask_top20 = sorted(multitask_sentiment.items(), key=lambda x: x[1], reverse=True)[:20]
    multitask_words_20, multitask_values_20 = zip(*multitask_top20)
    
    # å˜ä¸€ãƒ¢ãƒ‡ãƒ«TOP20
    single_top20 = sorted(single_sentiment.items(), key=lambda x: x[1], reverse=True)[:20]
    single_words_20, single_values_20 = zip(*single_top20)
    
    # å…±é€šè¦å› ã®TOP20
    common_top20 = sorted([(word, multitask_sentiment[word], single_sentiment[word]) 
                          for word in common_words], 
                         key=lambda x: abs(x[1]) + abs(x[2]), reverse=True)[:20]
    
    if common_top20:
        common_words_20, common_multitask_vals, common_single_vals = zip(*common_top20)
        
        x_pos = np.arange(len(common_words_20))
        width = 0.35
        
        plt.barh(x_pos - width/2, common_multitask_vals, width, 
                label='ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯', alpha=0.8, color='skyblue')
        plt.barh(x_pos + width/2, common_single_vals, width, 
                label='å˜ä¸€ãƒ¢ãƒ‡ãƒ«', alpha=0.8, color='lightcoral')
        
        plt.yticks(x_pos, common_words_20)
        plt.xlabel('é‡è¦åº¦')
        plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬: å…±é€šè¦å› TOP20ã®æ¯”è¼ƒ', fontsize=16)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/model_comparison_sentiment_top20.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    # 3. ãƒ¢ãƒ‡ãƒ«é–“ã®å·®ç•°åˆ†æ
    differences = []
    for word in common_words:
        multitask_val = multitask_sentiment[word]
        single_val = single_sentiment[word]
        diff = abs(multitask_val - single_val)
        differences.append((word, diff, multitask_val, single_val))
    
    differences.sort(key=lambda x: x[1], reverse=True)
    
    # å·®ç•°ãŒå¤§ãã„è¦å› TOP20
    plt.figure(figsize=(14, 10))
    top_diff_words = [item[0] for item in differences[:20]]
    top_diff_vals = [item[1] for item in differences[:20]]
    
    plt.barh(range(len(top_diff_words)), top_diff_vals, alpha=0.7, color='orange')
    plt.yticks(range(len(top_diff_words)), top_diff_words)
    plt.xlabel('é‡è¦åº¦ã®å·®ç•°')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬: ãƒ¢ãƒ‡ãƒ«é–“å·®ç•°ãŒå¤§ãã„è¦å› TOP20', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/model_difference_sentiment_top20.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå¯è¦–åŒ–ã®ä½œæˆå®Œäº†")

def production_save_results(sentiment_importance, course_importance, categories, output_dir):
    """æœ¬ç•ªç”¨ã®çµæœä¿å­˜"""
    print("ğŸ’¾ æœ¬ç•ªç”¨çµæœã®ä¿å­˜é–‹å§‹...")
    
    # CSVå½¢å¼ã§ä¿å­˜
    sentiment_df = pd.DataFrame(list(sentiment_importance.items()), columns=['word', 'importance'])
    sentiment_df = sentiment_df.sort_values('importance', ascending=False)
    sentiment_df.to_csv(f"{output_dir}/word_importance_sentiment_production.csv", index=False, encoding='utf-8')
    
    # TOP100ã®CSVä¿å­˜
    sentiment_top100_df = sentiment_df.head(100)
    sentiment_top100_df.to_csv(f"{output_dir}/word_importance_sentiment_top100_production.csv", index=False, encoding='utf-8')
    
    course_df = pd.DataFrame(list(course_importance.items()), columns=['word', 'importance'])
    course_df = course_df.sort_values('importance', ascending=False)
    course_df.to_csv(f"{output_dir}/word_importance_course_production.csv", index=False, encoding='utf-8')
    
    # TOP100ã®CSVä¿å­˜
    course_top100_df = course_df.head(100)
    course_top100_df.to_csv(f"{output_dir}/word_importance_course_top100_production.csv", index=False, encoding='utf-8')
    
    # JSONå½¢å¼ã§ä¿å­˜
    categories_json = {}
    for category, items in categories.items():
        categories_json[category] = [
            {'word': word, 'sentiment_importance': s_imp, 'course_importance': c_imp}
            for word, s_imp, c_imp in items
        ]
    
    with open(f"{output_dir}/factor_categories_production.json", 'w', encoding='utf-8') as f:
        json.dump(categories_json, f, ensure_ascii=False, indent=2)
    
    # è©³ç´°ãªåˆ†æã‚µãƒãƒªãƒ¼
    summary = {
        'analysis_date': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'device_used': str(device),
        'pytorch_version': torch.__version__,
        'method': 'æœ¬ç•ªç”¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ',
        'sample_size': 1000,
        'total_words_sentiment': len(sentiment_importance),
        'total_words_course': len(course_importance),
        'common_words': len(set(sentiment_importance.keys()) & set(course_importance.keys())),
        'category_counts': {cat: len(items) for cat, items in categories.items()},
        'top_sentiment_factors': dict(list(sentiment_importance.items())[:20]),
        'top_course_factors': dict(list(course_importance.items())[:20]),
        'strong_common_factors': [{'word': word, 'sentiment': s_imp, 'course': c_imp} 
                                 for word, s_imp, c_imp in categories['strong_common'][:10]]
    }
    
    with open(f"{output_dir}/analysis_summary_production.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("âœ… æœ¬ç•ªç”¨çµæœã®ä¿å­˜å®Œäº†")

def production_summary_report(categories, sentiment_importance, course_importance, output_dir):
    """æœ¬ç•ªç”¨ã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ"""
    print("ğŸ“ æœ¬ç•ªç”¨ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆé–‹å§‹...")
    
    # ç›¸é–¢ä¿‚æ•°è¨ˆç®—
    common_words = set(sentiment_importance.keys()) & set(course_importance.keys())
    if len(common_words) > 1:
        x_values = [sentiment_importance[word] for word in common_words]
        y_values = [course_importance[word] for word in common_words]
        correlation = np.corrcoef(x_values, y_values)[0, 1]
    else:
        correlation = 0.0
    
    report = f"""# æœ¬ç•ªç”¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æçµæœã‚µãƒãƒªãƒ¼

## åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- åˆ†æå¯¾è±¡: æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆæœ¬ç•ªç”¨ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: 1,000ä»¶ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}
- PyTorch version: {torch.__version__}
- å®Ÿè£…æ–¹æ³•: å®Ÿéš›ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½¿ç”¨
- å…±é€šè¦å› ã®ç›¸é–¢ä¿‚æ•°: {correlation:.3f}

## åˆ†æçµæœã‚µãƒãƒªãƒ¼
- æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› æ•°: {len(sentiment_importance)}å˜èª
- æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› æ•°: {len(course_importance)}å˜èª
- å…±é€šè¦å› æ•°: {len(common_words)}å˜èª
- å¼·ã„å…±é€šè¦å› æ•°: {len(categories['strong_common'])}å˜èª

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°
"""
    
    category_names = {
        'strong_common': 'å¼·ã„å…±é€šè¦å› ',
        'sentiment_leaning': 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 
        'course_leaning': 'è©•ä¾¡å¯„ã‚Šè¦å› ',
        'sentiment_specific': 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ',
        'course_specific': 'è©•ä¾¡ç‰¹åŒ–è¦å› '
    }
    
    for category, items in categories.items():
        report += f"\n### {category_names[category]} ({len(items)}ä»¶)\n"
        if items:
            report += "| é †ä½ | å˜èª | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ | ç·åˆé‡è¦åº¦ |\n"
            report += "|------|------|------------|------------|------------|\n"
            for i, (word, s_imp, c_imp) in enumerate(items[:15], 1):
                total_imp = abs(s_imp) + abs(c_imp)
                report += f"| {i} | {word} | {s_imp:.4f} | {c_imp:.4f} | {total_imp:.4f} |\n"
        else:
            report += "è©²å½“ã™ã‚‹è¦å› ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    
    report += f"""
## ä¸»è¦ãªç™ºè¦‹

### 1. å¼·ã„å…±é€šè¦å› 
ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯ã§é«˜ã„å¯„ä¸ã‚’ç¤ºã™è¦å› ãŒ{len(categories['strong_common'])}ä»¶ç™ºè¦‹ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã‚‰ã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã«å½±éŸ¿ã™ã‚‹çœŸã®è¦å› ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ãã€
æˆæ¥­æ”¹å–„ã®å„ªå…ˆé †ä½ã¨ã—ã¦æœ€ã‚‚é‡è¦ã§ã™ã€‚

### 2. ã‚¿ã‚¹ã‚¯ç‰¹åŒ–è¦å› 
- æ„Ÿæƒ…ç‰¹åŒ–è¦å› : {len(categories['sentiment_specific'])}ä»¶
- è©•ä¾¡ç‰¹åŒ–è¦å› : {len(categories['course_specific'])}ä»¶

ã“ã‚Œã‚‰ã®è¦å› ã¯ã€ãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ã«ç‰¹æœ‰ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã§ã™ã€‚

### 3. ç›¸é–¢é–¢ä¿‚
å…±é€šè¦å› ã®ç›¸é–¢ä¿‚æ•°ã¯{correlation:.3f}ã§ã€{'å¼·ã„' if abs(correlation) > 0.7 else 'ä¸­ç¨‹åº¦' if abs(correlation) > 0.3 else 'å¼±ã„'}ç›¸é–¢ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

### 4. æˆæ¥­æ”¹å–„ã¸ã®ç¤ºå”†
1. **å„ªå…ˆåº¦1**: å¼·ã„å…±é€šè¦å› ã‚’é‡è¦–ã—ãŸæˆæ¥­æ”¹å–„
2. **å„ªå…ˆåº¦2**: æ„Ÿæƒ…å¯„ã‚Šè¦å› ã¨è©•ä¾¡å¯„ã‚Šè¦å› ã®ãƒãƒ©ãƒ³ã‚¹
3. **å„ªå…ˆåº¦3**: ç‰¹åŒ–è¦å› ã®å€‹åˆ¥å¯¾å¿œ

### 5. æœ¬ç•ªç”¨åˆ†æã®ç‰¹å¾´
- å®Ÿéš›ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½¿ç”¨
- 1,000ä»¶ã®å¤§è¦æ¨¡ã‚µãƒ³ãƒ—ãƒ«
- åŒæ–¹å‘LSTM + æ³¨æ„æ©Ÿæ§‹ã«ã‚ˆã‚‹é«˜ç²¾åº¦åˆ†æ
- ã‚ˆã‚Šå³å¯†ãªçµ±è¨ˆçš„é–¾å€¤è¨­å®š

## ä»Šå¾Œã®èª²é¡Œ
1. å…±é€šè¦å› ã®å› æœé–¢ä¿‚ã®æ¤œè¨¼
2. å®Ÿé¨“çš„æˆæ¥­æ”¹å–„ã®å®Ÿæ–½
3. æ”¹å–„åŠ¹æœã®å®šé‡çš„æ¸¬å®š
4. ã‚ˆã‚Šå¤§è¦æ¨¡ãªã‚µãƒ³ãƒ—ãƒ«ã§ã®æ¤œè¨¼
5. ä»–ã®æ•™è‚²æ©Ÿé–¢ã§ã®é©ç”¨å¯èƒ½æ€§æ¤œè¨
"""
    
    with open(f"{output_dir}/multitask_shap_analysis_summary_production.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… æœ¬ç•ªç”¨ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæœ¬ç•ªç”¨ï¼‰"""
    print("ğŸš€ æœ¬ç•ªç”¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æã‚’é–‹å§‹...")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = "../03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_æœ¬ç•ªç”¨"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    print("\n=== Phase 1: æœ¬ç•ªç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ===")
    data_path = "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv"
    df = pd.read_csv(data_path)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    
    # æœ¬ç•ªç”¨å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ500ä»¶ã«èª¿æ•´ï¼‰
    sampled_df = production_stratified_sampling(df, n_samples=500)
    
    # 2. æœ¬ç•ªç”¨ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
    print("\n=== Phase 2: æœ¬ç•ªç”¨ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç† ===")
    texts = sampled_df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].fillna('').tolist()
    processed_texts, word_to_id = production_text_preprocessing(texts)
    
    # 3. æœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("\n=== Phase 3: æœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ ===")
    model = create_production_model()
    
    # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’èª­ã¿è¾¼ã¿
    state_dict = load_real_multitask_model()
    if state_dict:
        try:
            model.load_state_dict(state_dict)
            print("âœ… å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«é‡ã¿èª­ã¿è¾¼ã¿æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«é‡ã¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ç¶šè¡Œ...")
    
    model.to(device)
    model.eval()
    print("âœ… æœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
    
    # 4. æœ¬ç•ªç”¨é‡è¦åº¦åˆ†æå®Ÿè¡Œ
    print("\n=== Phase 4: æœ¬ç•ªç”¨é‡è¦åº¦åˆ†æå®Ÿè¡Œ ===")
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®é‡è¦åº¦åˆ†æ
    sentiment_importance = production_shap_analysis(model, processed_texts, word_to_id, target='sentiment')
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®é‡è¦åº¦åˆ†æ
    course_importance = production_shap_analysis(model, processed_texts, word_to_id, target='course')
    
    # 5. æœ¬ç•ªç”¨è¦å› åˆ†æã¨åˆ†é¡
    print("\n=== Phase 5: æœ¬ç•ªç”¨è¦å› åˆ†æã¨åˆ†é¡ ===")
    categories = production_classify_factors(sentiment_importance, course_importance)
    
    # 6. Beeswarmãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ
    print("\n=== Phase 6: Beeswarmãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ ===")
    create_beeswarm_plots(model, processed_texts, word_to_id, output_dir, max_samples=30)
    
    # 7. æœ¬ç•ªç”¨çµæœã®ä¿å­˜ã¨å¯è¦–åŒ–
    print("\n=== Phase 7: æœ¬ç•ªç”¨çµæœã®ä¿å­˜ã¨å¯è¦–åŒ– ===")
    production_save_results(sentiment_importance, course_importance, categories, output_dir)
    production_visualizations(sentiment_importance, course_importance, categories, output_dir)
    
    # 8. å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒåˆ†æ
    print("\n=== Phase 8: å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒåˆ†æ ===")
    single_sentiment, single_course = load_single_model_results()
    if single_sentiment:
        create_model_comparison_visualizations(sentiment_importance, course_importance, 
                                             single_sentiment, single_course, output_dir)
        print("âœ… å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒåˆ†æå®Œäº†")
    else:
        print("âš ï¸ å˜ä¸€ãƒ¢ãƒ‡ãƒ«çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ¯”è¼ƒåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    
    production_summary_report(categories, sentiment_importance, course_importance, output_dir)
    
    print("\nğŸ‰ æœ¬ç•ªç”¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print(f"âœ… 500ä»¶ã®å¤§è¦æ¨¡ã‚µãƒ³ãƒ—ãƒ«ã§æœ¬æ ¼çš„ãªåˆ†æã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")
    print(f"âœ… TOP100è¦å› åˆ†æã¨Beeswarmãƒ—ãƒ­ãƒƒãƒˆã‚’å®Œäº†ã—ã¾ã—ãŸ")
    print(f"âœ… å˜ä¸€ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒåˆ†æã‚’å®Œäº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
