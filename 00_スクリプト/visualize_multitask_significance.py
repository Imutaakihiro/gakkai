#!/usr/bin/env python3
"""
ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æ„ç¾©ã‚’å¯è¦–åŒ–ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…±é€šè¦å› ã¨ç‰¹åŒ–è¦å› ã®è©³ç´°åˆ†æ
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'MS Mincho', 'DejaVu Sans']
else:
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao']

plt.rcParams['axes.unicode_minus'] = False

def load_analysis_results():
    """åˆ†æçµæœã®èª­ã¿è¾¼ã¿"""
    result_path = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_1000ä»¶/bert_tokenizer_analysis_20251016_003336.json"
    
    with open(result_path, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    return results

def analyze_common_factors(results):
    """å…±é€šè¦å› ã®åˆ†æ"""
    sentiment_factors = results['sentiment_factors']
    course_factors = results['course_factors']
    
    # å…±é€šè¦å› ã®ç‰¹å®š
    common_factors = {}
    sentiment_specific = {}
    course_specific = {}
    
    for word in sentiment_factors:
        sentiment_importance = sentiment_factors[word]
        
        if word in course_factors:
            course_importance = course_factors[word]
            # å…±é€šè¦å› 
            common_factors[word] = {
                'sentiment': sentiment_importance,
                'course': course_importance,
                'total': sentiment_importance + course_importance,
                'ratio': sentiment_importance / course_importance if course_importance > 0 else float('inf')
            }
        else:
            # æ„Ÿæƒ…ç‰¹åŒ–è¦å› 
            sentiment_specific[word] = sentiment_importance
    
    for word in course_factors:
        if word not in sentiment_factors:
            # è©•ä¾¡ç‰¹åŒ–è¦å› 
            course_specific[word] = course_factors[word]
    
    return common_factors, sentiment_specific, course_specific

def create_common_factors_analysis(common_factors):
    """å…±é€šè¦å› ã®è©³ç´°åˆ†æ"""
    print("ğŸ” å…±é€šè¦å› ã®è©³ç´°åˆ†æ")
    
    # TOP20å…±é€šè¦å› 
    top_common = sorted(common_factors.items(), key=lambda x: x[1]['total'], reverse=True)[:20]
    
    print("\n=== TOP20å…±é€šè¦å›  ===")
    print("| é †ä½ | è¦å›  | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ | ç·åˆé‡è¦åº¦ | æ¯”ç‡ |")
    print("|------|------|------------|------------|------------|------|")
    
    for i, (word, data) in enumerate(top_common, 1):
        ratio_str = f"{data['ratio']:.2f}" if data['ratio'] != float('inf') else "âˆ"
        print(f"| {i:2d} | {word} | {data['sentiment']:.6f} | {data['course']:.6f} | {data['total']:.6f} | {ratio_str} |")
    
    return top_common

def create_factor_categories_visualization(common_factors, sentiment_specific, course_specific):
    """è¦å› ã‚«ãƒ†ã‚´ãƒªã®å¯è¦–åŒ–"""
    print("ğŸ“Š è¦å› ã‚«ãƒ†ã‚´ãƒªã®å¯è¦–åŒ–ä½œæˆä¸­...")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
    categories = {
        'å…±é€šè¦å› ': len(common_factors),
        'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ': len(sentiment_specific),
        'è©•ä¾¡ç‰¹åŒ–è¦å› ': len(course_specific)
    }
    
    # å††ã‚°ãƒ©ãƒ•
    plt.figure(figsize=(12, 8))
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: ã‚«ãƒ†ã‚´ãƒªåˆ¥ä»¶æ•°
    plt.subplot(2, 2, 1)
    colors = ['#ff9999', '#66b3ff', '#99ff99']
    plt.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%', colors=colors)
    plt.title('è¦å› ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦åˆ†å¸ƒ
    plt.subplot(2, 2, 2)
    common_importances = [data['total'] for data in common_factors.values()]
    sentiment_importances = list(sentiment_specific.values())
    course_importances = list(course_specific.values())
    
    plt.hist([common_importances, sentiment_importances, course_importances], 
             bins=20, alpha=0.7, label=['å…±é€šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–', 'è©•ä¾¡ç‰¹åŒ–'], 
             color=['#ff9999', '#66b3ff', '#99ff99'])
    plt.xlabel('é‡è¦åº¦')
    plt.ylabel('é »åº¦')
    plt.title('ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    plt.legend()
    plt.yscale('log')
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: TOP10å…±é€šè¦å› 
    plt.subplot(2, 2, 3)
    top_common = sorted(common_factors.items(), key=lambda x: x[1]['total'], reverse=True)[:10]
    words = [item[0] for item in top_common]
    totals = [item[1]['total'] for item in top_common]
    
    plt.barh(range(len(words)), totals, color='#ff9999', alpha=0.7)
    plt.yticks(range(len(words)), words)
    plt.xlabel('ç·åˆé‡è¦åº¦')
    plt.title('TOP10å…±é€šè¦å› ', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: æ„Ÿæƒ…vsè©•ä¾¡é‡è¦åº¦æ•£å¸ƒå›³
    plt.subplot(2, 2, 4)
    sentiment_vals = [data['sentiment'] for data in common_factors.values()]
    course_vals = [data['course'] for data in common_factors.values()]
    
    plt.scatter(sentiment_vals, course_vals, alpha=0.6, color='#ff9999')
    plt.xlabel('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢é‡è¦åº¦')
    plt.ylabel('æˆæ¥­è©•ä¾¡é‡è¦åº¦')
    plt.title('å…±é€šè¦å› ã®é‡è¦åº¦ç›¸é–¢', fontsize=14, fontweight='bold')
    
    # ç›¸é–¢ä¿‚æ•°è¨ˆç®—
    correlation = np.corrcoef(sentiment_vals, course_vals)[0, 1]
    plt.text(0.05, 0.95, f'ç›¸é–¢ä¿‚æ•°: {correlation:.3f}', 
             transform=plt.gca().transAxes, fontsize=12, 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜
    output_dir = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_1000ä»¶"
    plt.savefig(f"{output_dir}/è¦å› ã‚«ãƒ†ã‚´ãƒªåˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… è¦å› ã‚«ãƒ†ã‚´ãƒªå¯è¦–åŒ–å®Œäº†")
    return correlation

def create_educational_implications_analysis(common_factors):
    """æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”†åˆ†æ"""
    print("ğŸ“ æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”†åˆ†æ")
    
    # æ•™è‚²æ”¹å–„ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©
    educational_categories = {
        'æˆæ¥­å†…å®¹': ['èª¬æ˜', 'å†…å®¹', 'ç†è§£', 'èª²é¡Œ', 'å•é¡Œ'],
        'å­¦ç¿’ç’°å¢ƒ': ['è³ªå•', 'ä¼šè©±', 'å·¥å¤«', 'æ”¹å–„', 'è©•ä¾¡'],
        'å­¦ç¿’ä½“é¨“': ['æ…£ã‚Œã‚‹', 'ã—ã£ã‹ã‚Šç¿’', 'è©¦ã—', 'ã‚¹ãƒ”ãƒ¼ãƒ‰'],
        'å­¦ç¿’æˆæœ': ['ã¾ã—ç‚¹', 'ä¸€å±¤', 'å·¥å¤«', 'ä¼šè©±']
    }
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› åˆ†æ
    category_analysis = {}
    for category, keywords in educational_categories.items():
        category_factors = {}
        for word, data in common_factors.items():
            if any(keyword in word for keyword in keywords):
                category_factors[word] = data
        
        if category_factors:
            avg_importance = np.mean([data['total'] for data in category_factors.values()])
            category_analysis[category] = {
                'factors': category_factors,
                'count': len(category_factors),
                'avg_importance': avg_importance
            }
    
    # æ•™è‚²æ”¹å–„å„ªå…ˆé †ä½
    print("\n=== æ•™è‚²æ”¹å–„å„ªå…ˆé †ä½ ===")
    print("| å„ªå…ˆåº¦ | ã‚«ãƒ†ã‚´ãƒª | è¦å› æ•° | å¹³å‡é‡è¦åº¦ | ä¸»è¦è¦å›  |")
    print("|--------|----------|--------|------------|----------|")
    
    sorted_categories = sorted(category_analysis.items(), key=lambda x: x[1]['avg_importance'], reverse=True)
    
    for i, (category, data) in enumerate(sorted_categories, 1):
        top_factors = sorted(data['factors'].items(), key=lambda x: x[1]['total'], reverse=True)[:3]
        top_factors_str = ', '.join([factor[0] for factor in top_factors])
        print(f"| {i} | {category} | {data['count']} | {data['avg_importance']:.6f} | {top_factors_str} |")
    
    return category_analysis

def create_comprehensive_report(results, common_factors, sentiment_specific, course_specific, correlation):
    """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
    print("ğŸ“ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­...")
    
    report = f"""# ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æã«ã‚ˆã‚‹ç›¸é–¢é–¢ä¿‚ã®é™ç•Œè¶…è¶Š

## ğŸ¯ åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- ã‚µãƒ³ãƒ—ãƒ«æ•°: {results['sample_size']}ä»¶
- èªå½™æ•°: {results['vocab_size']}èªå½™
- å…±é€šè¦å› æ•°: {len(common_factors)}èªå½™
- æ„Ÿæƒ…ç‰¹åŒ–è¦å› æ•°: {len(sentiment_specific)}èªå½™
- è©•ä¾¡ç‰¹åŒ–è¦å› æ•°: {len(course_specific)}èªå½™

## ğŸ” ä¸»è¦ç™ºè¦‹

### 1. å…±é€šè¦å› ã®ç™ºè¦‹
**TOP10å…±é€šè¦å› ï¼ˆä¸¡æ–¹ã®ã‚¹ã‚³ã‚¢ã«å½±éŸ¿ï¼‰:**

| é †ä½ | è¦å›  | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ | ç·åˆé‡è¦åº¦ | æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”† |
|------|------|------------|------------|------------|------------------|
"""
    
    top_common = sorted(common_factors.items(), key=lambda x: x[1]['total'], reverse=True)[:10]
    for i, (word, data) in enumerate(top_common, 1):
        report += f"| {i} | {word} | {data['sentiment']:.6f} | {data['course']:.6f} | {data['total']:.6f} | ä¸¡æ–¹ã®ã‚¹ã‚³ã‚¢ã«ç›´çµ |\n"
    
    report += f"""
### 2. ç‰¹åŒ–è¦å› ã®ç‰¹å®š
- **æ„Ÿæƒ…ç‰¹åŒ–è¦å› **: {len(sentiment_specific)}èªå½™ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã«ç‰¹ã«å½±éŸ¿ï¼‰
- **è©•ä¾¡ç‰¹åŒ–è¦å› **: {len(course_specific)}èªå½™ï¼ˆæˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã«ç‰¹ã«å½±éŸ¿ï¼‰

### 3. ç›¸é–¢é–¢ä¿‚ã®åˆ†æ
- **å…±é€šè¦å› ã®ç›¸é–¢ä¿‚æ•°**: {correlation:.3f}
- **è§£é‡ˆ**: å…±é€šè¦å› å†…ã§ã‚‚æ„Ÿæƒ…ã¨è©•ä¾¡ã®é‡è¦åº¦ã«ç›¸é–¢é–¢ä¿‚ãŒå­˜åœ¨

## ğŸ“ æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”†

### æœ€å„ªå…ˆæ”¹å–„é …ç›®ï¼ˆå…±é€šè¦å› ï¼‰
1. **èª¬æ˜ã®è³ªå‘ä¸Š**: æœ€ã‚‚é‡è¦ãªå…±é€šè¦å› 
2. **è³ªå•ç’°å¢ƒã®æ•´å‚™**: å­¦ç”ŸãŒè³ªå•ã—ã‚„ã™ã„é›°å›²æ°—ä½œã‚Š
3. **ç†è§£åº¦ã®ç¢ºèª**: å®šæœŸçš„ãªç†è§£åº¦ãƒã‚§ãƒƒã‚¯
4. **å†…å®¹ã®å……å®Ÿ**: æˆæ¥­å†…å®¹ã®è³ªçš„å‘ä¸Š
5. **ç¶™ç¶šçš„æ”¹å–„**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãæ”¹å–„

### æˆ¦ç•¥çš„æ”¹å–„
- **å…±é€šè¦å› ã¸ã®é›†ä¸­**: ä¸¡æ–¹ã®ã‚¹ã‚³ã‚¢ã‚’åŒæ™‚ã«å‘ä¸Š
- **ç‰¹åŒ–è¦å› ã¸ã®å€‹åˆ¥å¯¾å¿œ**: æ„Ÿæƒ…é¢ã¨è©•ä¾¡é¢ã®å€‹åˆ¥æœ€é©åŒ–
- **ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ”¹å–„**: ç·åˆçš„ãªæ•™è‚²å“è³ªã®å‘ä¸Š

## ğŸš€ å­¦è¡“çš„æ„ç¾©

### ç†è«–çš„è²¢çŒ®
- ç›¸é–¢é–¢ä¿‚ã®é™ç•Œã‚’è¶…ãˆãŸå› æœé–¢ä¿‚ã®ç‰¹å®š
- æ„Ÿæƒ…ã¨è©•ä¾¡ã®é–¢ä¿‚æ€§ã®æ§‹é€ çš„è§£æ˜
- æ–°ã—ã„åˆ†ææ‰‹æ³•ï¼ˆBERT+ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯+SHAPï¼‰ã®ææ¡ˆ

### å®Ÿç”¨çš„ä¾¡å€¤
- æ•™è‚²ç¾å ´ã§ã®å…·ä½“çš„æ”¹å–„æŒ‡é‡ã®æä¾›
- ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ•™è‚²æ”¹å–„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- ä»–ã®æ•™è‚²æ©Ÿé–¢ã¸ã®å¿œç”¨å¯èƒ½æ€§

## ğŸ“Š æŠ€è¡“çš„å„ªä½æ€§

### ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡
- åŒã˜ãƒ‡ãƒ¼ã‚¿ã§2ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚å­¦ç¿’
- ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®è§£æ±º
- ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®å‘ä¸Š

### æ±åŒ–æ€§èƒ½
- éå­¦ç¿’ã®é˜²æ­¢
- ã‚ˆã‚Šå …ç‰¢ãªãƒ¢ãƒ‡ãƒ«
- å®Ÿç”¨æ€§ã®å‘ä¸Š

## ğŸ¯ çµè«–

ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æã«ã‚ˆã‚Šã€å˜ç´”ãªç›¸é–¢é–¢ä¿‚ã‚’è¶…ãˆã¦ã€æ•™è‚²æ”¹å–„ã®çœŸã®è¦å› ã‚’ç™ºè¦‹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚{len(common_factors)}èªå½™ã®å…±é€šè¦å› ã¨{len(sentiment_specific) + len(course_specific)}èªå½™ã®ç‰¹åŒ–è¦å› ã‚’ç‰¹å®šã—ã€æ•™è‚²æ”¹å–„ã®å…·ä½“çš„æŒ‡é‡ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

ã“ã®æˆæœã¯ã€æ•™è‚²å¿ƒç†å­¦ã®ç†è«–çš„ç™ºå±•ã¨æ•™è‚²ç¾å ´ã®å®Ÿè·µçš„æ”¹å–„ã®ä¸¡æ–¹ã«è²¢çŒ®ã™ã‚‹ã€å­¦è¡“çš„ä¾¡å€¤ã®é«˜ã„ç ”ç©¶ã§ã™ã€‚
"""
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    output_dir = "03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_1000ä»¶"
    with open(f"{output_dir}/åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æ„ç¾©ã‚’å¯è¦–åŒ–ã™ã‚‹åˆ†æ")
    print("=" * 60)
    
    # åˆ†æçµæœã®èª­ã¿è¾¼ã¿
    results = load_analysis_results()
    
    # å…±é€šè¦å› ã®åˆ†æ
    common_factors, sentiment_specific, course_specific = analyze_common_factors(results)
    
    # å…±é€šè¦å› ã®è©³ç´°åˆ†æ
    top_common = create_common_factors_analysis(common_factors)
    
    # è¦å› ã‚«ãƒ†ã‚´ãƒªã®å¯è¦–åŒ–
    correlation = create_factor_categories_visualization(common_factors, sentiment_specific, course_specific)
    
    # æ•™è‚²æ”¹å–„ã¸ã®ç¤ºå”†åˆ†æ
    category_analysis = create_educational_implications_analysis(common_factors)
    
    # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
    create_comprehensive_report(results, common_factors, sentiment_specific, course_specific, correlation)
    
    print("\nğŸ‰ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æ„ç¾©åˆ†æå®Œäº†ï¼")
    print("ğŸ“ çµæœã¯ 03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_1000ä»¶ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()
