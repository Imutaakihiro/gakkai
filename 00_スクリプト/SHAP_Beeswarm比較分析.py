#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®Beeswarmãƒ—ãƒ­ãƒƒãƒˆæ¯”è¼ƒ
SHAPåˆ†ææ‰‹æ³•ã®è§£èª¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨ã®å¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

# PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’æ ¹æœ¬çš„ã«å›é¿
os.environ['TORCH_DISABLE_SAFETENSORS_WARNING'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['HF_HUB_OFFLINE'] = '1'

# DirectMLç’°å¢ƒã®è¨­å®š
os.environ['PYTORCH_DISABLE_DIRECTML'] = '0'  # DirectMLã‚’æœ‰åŠ¹åŒ–

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from transformers import BertForSequenceClassification, BertJapaneseTokenizer
from collections import defaultdict
import json
from datetime import datetime
import pickle

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
def setup_japanese_font():
    """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š"""
    import matplotlib.font_manager as fm
    
    try:
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        japanese_fonts = [
            'MS Gothic', 'MS Mincho', 'Yu Gothic', 'Meiryo', 'Hiragino Sans',
            'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP',
            'DejaVu Sans', 'Arial Unicode MS'
        ]
        
        for font in japanese_fonts:
            if font in available_fonts:
                plt.rcParams['font.family'] = font
                plt.rcParams['axes.unicode_minus'] = False
                print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šå®Œäº†: {font}")
                return True
        
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        print("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š")
        return False
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        return False

# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šå®Ÿè¡Œ
font_success = setup_japanese_font()

print("="*60)
print("å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®Beeswarmãƒ—ãƒ­ãƒƒãƒˆæ¯”è¼ƒ")
print("SHAPåˆ†ææ‰‹æ³•ã®è§£èª¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨")
print("="*60)

# DirectMLç’°å¢ƒã®è¨­å®š
try:
    import torch_directml
    if torch_directml.is_available():
        device = torch_directml.device()
        print(f"âœ… DirectMLä½¿ç”¨å¯èƒ½: {device}")
    else:
        device = torch.device("cpu")
        print("âš ï¸ DirectMLä½¿ç”¨ä¸å¯ã€CPUã‚’ä½¿ç”¨ã—ã¾ã™")
except ImportError:
    print("âš ï¸ torch_directmlãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    device = torch.device("cpu")
    print("âš ï¸ CPUã‚’ä½¿ç”¨ã—ã¾ã™")

print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

def load_single_task_model():
    """å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    print("ğŸ“¥ å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # è¤‡æ•°ã®ãƒ‘ã‚¹ã‚’è©¦è¡Œï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ã‚‚å«ã‚€ï¼‰
    model_paths = [
        "02_ãƒ¢ãƒ‡ãƒ«/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«",  # ç›¸å¯¾ãƒ‘ã‚¹
        "02_ãƒ¢ãƒ‡ãƒ«/å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«2_è©•ä¾¡ã‚¹ã‚³ã‚¢",
        "finetuned_bert_model_20250718_step2_fixed_classweights_variant1_positiveé‡ç‚¹å¼·åŒ–",
        "../02_ãƒ¢ãƒ‡ãƒ«/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«",  # ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆä¸Šä½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
        "../02_ãƒ¢ãƒ‡ãƒ«/å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«2_è©•ä¾¡ã‚¹ã‚³ã‚¢"
    ]
    
    for model_path in model_paths:
        try:
            print(f"ğŸ”„ è©¦è¡Œä¸­: {model_path}")
            
            # ãƒ‘ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if not os.path.exists(model_path):
                print(f"âš ï¸ ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}")
                continue
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿
            if os.path.exists(f"{model_path}/tokenizer_config.json"):
                tokenizer = BertJapaneseTokenizer.from_pretrained(model_path)
            else:
                print(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                continue
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            if os.path.exists(f"{model_path}/best_model.pth"):
                # PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
                model = BertForSequenceClassification.from_pretrained("cl-tohoku/bert-base-japanese-v3")
                state_dict = torch.load(f"{model_path}/best_model.pth", map_location=device, weights_only=False)
                model.load_state_dict(state_dict)
            elif os.path.exists(f"{model_path}/best_multitask_model.pth"):
                # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
                model = BertForSequenceClassification.from_pretrained("cl-tohoku/bert-base-japanese-v3")
                state_dict = torch.load(f"{model_path}/best_multitask_model.pth", map_location=device, weights_only=False)
                model.load_state_dict(state_dict)
            else:
                # HuggingFaceå½¢å¼ã®å ´åˆ
                model = BertForSequenceClassification.from_pretrained(model_path)
            
            model.to(device)
            model.eval()
            print(f"âœ… å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {model_path}")
            return model, tokenizer
        except Exception as e:
            print(f"âš ï¸ å¤±æ•—: {e}")
            continue
    
    print("âŒ å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
    return None, None

def load_multitask_model():
    """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    print("ğŸ“¥ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # è¤‡æ•°ã®ãƒ‘ã‚¹ã‚’è©¦è¡Œï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ã‚‚å«ã‚€ï¼‰
    model_paths = [
        "02_ãƒ¢ãƒ‡ãƒ«/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«",  # ç›¸å¯¾ãƒ‘ã‚¹
        "02_ãƒ¢ãƒ‡ãƒ«/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«",
        "../02_ãƒ¢ãƒ‡ãƒ«/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«",  # ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆä¸Šä½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
        "../02_ãƒ¢ãƒ‡ãƒ«/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"
    ]
    
    for model_path in model_paths:
        try:
            print(f"ğŸ”„ è©¦è¡Œä¸­: {model_path}")
            
            # ãƒ‘ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if not os.path.exists(model_path):
                print(f"âš ï¸ ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}")
                continue
            
            # å®Ÿéš›ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½œæˆï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã‚‹ï¼‰
            class ClassLevelMultitaskModel(torch.nn.Module):
                def __init__(self, vocab_size=30000, hidden_size=768, dropout_rate=0.3):
                    super(ClassLevelMultitaskModel, self).__init__()
                    
                    # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
                    from transformers import BertModel
                    self.bert = BertModel.from_pretrained("cl-tohoku/bert-base-japanese-v3")
                    
                    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
                    self.sentiment_classifier = torch.nn.Linear(hidden_size, 1)
                    
                    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
                    self.score_regressor = torch.nn.Linear(hidden_size, 1)
                
                def forward(self, input_ids, attention_mask=None):
                    # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
                    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
                    pooled_output = outputs.pooler_output  # [batch_size, hidden_size]
                    
                    # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
                    sentiment_pred = self.sentiment_classifier(pooled_output)
                    course_pred = self.score_regressor(pooled_output)
                    
                    return sentiment_pred, course_pred
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model = ClassLevelMultitaskModel()
            
            # è¤‡æ•°ã®èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’è©¦è¡Œ
            model_files = [
                f"{model_path}/best_class_level_multitask_model.pth",
                f"{model_path}/best_multitask_model.pth",
                f"{model_path}/best_model.pth"
            ]
            
            loaded = False
            for model_file in model_files:
                if os.path.exists(model_file):
                    try:
                        state_dict = torch.load(model_file, map_location=device, weights_only=False)
                        model.load_state_dict(state_dict)
                        print(f"âœ… torch.loadã§ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {model_file}")
                        loaded = True
                        break
                    except Exception as e:
                        print(f"âš ï¸ torch.loadå¤±æ•—: {e}")
                        # ä»£æ›¿æ–¹æ³•
                        try:
                            with open(model_file, 'rb') as f:
                                state_dict = pickle.load(f)
                            model.load_state_dict(state_dict)
                            print(f"âœ… pickleã§ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {model_file}")
                            loaded = True
                            break
                        except Exception as e2:
                            print(f"âš ï¸ pickleã‚‚å¤±æ•—: {e2}")
                            continue
            
            if not loaded:
                print(f"âŒ ã™ã¹ã¦ã®æ–¹æ³•ã§å¤±æ•—: {model_path}")
                continue
            
            model.to(device)
            model.eval()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆBERTãƒ™ãƒ¼ã‚¹ï¼‰
            tokenizer = BertJapaneseTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-v3")
            
            return model, tokenizer
            
        except Exception as e:
            print(f"âš ï¸ å¤±æ•—: {e}")
            continue
    
    print("âŒ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
    return None, None

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    data_paths = [
        "01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv",  # ç›¸å¯¾ãƒ‘ã‚¹
        "01_ãƒ‡ãƒ¼ã‚¿/è‡ªç”±è¨˜è¿°â†’æ„Ÿæƒ…ã‚¹ã‚³ã‚¢/finetuning_val_20250710_220621.csv",
        "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv",  # ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆä¸Šä½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
        "../01_ãƒ‡ãƒ¼ã‚¿/è‡ªç”±è¨˜è¿°â†’æ„Ÿæƒ…ã‚¹ã‚³ã‚¢/finetuning_val_20250710_220621.csv"
    ]
    
    for data_path in data_paths:
        try:
            df = pd.read_csv(data_path)
            if 'è‡ªç”±è¨˜è¿°ã¾ã¨ã‚' in df.columns or 'è‡ªç”±è¨˜è¿°' in df.columns:
                print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {data_path} ({len(df)}ä»¶)")
                return df
        except Exception as e:
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            continue
    
    print("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
    return None

def create_prediction_functions(single_model, single_tokenizer, multitask_model, multitask_tokenizer):
    """äºˆæ¸¬é–¢æ•°ã‚’ä½œæˆ"""
    
    def predict_single_sentiment(texts):
        """å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æã®äºˆæ¸¬é–¢æ•°"""
        if isinstance(texts, str):
            texts = [texts]
        elif isinstance(texts, np.ndarray):
            texts = texts.tolist()
        elif not isinstance(texts, list):
            texts = [str(texts)]
        
        texts = [str(t) if t else "" for t in texts]
        
        probs = []
        for text in texts:
            try:
                inputs = single_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = single_model(**inputs)
                    logits = outputs.logits
                    prob = torch.softmax(logits, dim=-1)
                    probs.append(prob.cpu().numpy()[0])
            except Exception as e:
                print(f"å˜ä¸€ã‚¿ã‚¹ã‚¯äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é©åˆ‡ãªå½¢çŠ¶ã®é…åˆ—ã‚’è¿”ã™
                probs.append(np.array([0.33, 0.33, 0.34]))
        
        return np.array(probs)
    
    def predict_multitask_sentiment(texts):
        """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬é–¢æ•°"""
        if isinstance(texts, str):
            texts = [texts]
        elif isinstance(texts, np.ndarray):
            texts = texts.tolist()
        elif not isinstance(texts, list):
            texts = [str(texts)]
        
        texts = [str(t) if t else "" for t in texts]
        
        predictions = []
        for text in texts:
            try:
                # BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                inputs = multitask_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    sentiment_pred, course_pred = multitask_model(**inputs)
                    predictions.append(sentiment_pred.cpu().numpy()[0])
            except Exception as e:
                print(f"ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                predictions.append([0.5])  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        return np.array(predictions)
    
    def predict_multitask_course(texts):
        """ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬é–¢æ•°"""
        if isinstance(texts, str):
            texts = [texts]
        elif isinstance(texts, np.ndarray):
            texts = texts.tolist()
        elif not isinstance(texts, list):
            texts = [str(texts)]
        
        texts = [str(t) if t else "" for t in texts]
        
        predictions = []
        for text in texts:
            try:
                # BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                inputs = multitask_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    sentiment_pred, course_pred = multitask_model(**inputs)
                    predictions.append(course_pred.cpu().numpy()[0])
            except Exception as e:
                print(f"ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                predictions.append([0.5])  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        return np.array(predictions)
    
    return predict_single_sentiment, predict_multitask_sentiment, predict_multitask_course

def create_beeswarm_plots(single_model, single_tokenizer, multitask_model, multitask_tokenizer, df):
    """Beeswarmãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ"""
    print("ğŸ Beeswarmãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆä¸­...")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = "03_åˆ†æçµæœ/SHAP_Beeswarmæ¯”è¼ƒ"
    os.makedirs(output_dir, exist_ok=True)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æº–å‚™
    if 'è‡ªç”±è¨˜è¿°ã¾ã¨ã‚' in df.columns:
        texts = df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].dropna().tolist()
    elif 'è‡ªç”±è¨˜è¿°' in df.columns:
        texts = df['è‡ªç”±è¨˜è¿°'].dropna().tolist()
    else:
        print("âŒ é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ20ä»¶ã§ãƒ†ã‚¹ãƒˆï¼‰
    sample_size = min(20, len(texts))
    sample_texts = np.random.choice(texts, size=sample_size, replace=False).tolist()
    print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ: {len(sample_texts)}ä»¶")
    
    # äºˆæ¸¬é–¢æ•°ä½œæˆ
    predict_single, predict_multitask_sentiment, predict_multitask_course = create_prediction_functions(
        single_model, single_tokenizer, multitask_model, multitask_tokenizer
    )
    
    try:
        # 1. å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æã®Beeswarmãƒ—ãƒ­ãƒƒãƒˆ
        print("ğŸ”¬ å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æã®SHAPåˆ†æå®Ÿè¡Œä¸­...")
        explainer_single = shap.Explainer(predict_single, single_tokenizer)
        shap_values_single = explainer_single(sample_texts)
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values_single, sample_texts, show=False)
        title = "å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®SHAP Beeswarm Plot" if font_success else "Single Task Sentiment Analysis SHAP Beeswarm Plot"
        plt.title(title, fontsize=16, pad=20, color='#2C3E50')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/single_task_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                    dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("âœ… å˜ä¸€ã‚¿ã‚¹ã‚¯Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
        # 2. ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢Beeswarmãƒ—ãƒ­ãƒƒãƒˆ
        print("ğŸ”¬ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAPåˆ†æå®Ÿè¡Œä¸­...")
        explainer_multitask_sentiment = shap.Explainer(predict_multitask_sentiment, multitask_tokenizer)
        shap_values_multitask_sentiment = explainer_multitask_sentiment(sample_texts)
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values_multitask_sentiment, sample_texts, show=False)
        title = "ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢SHAP Beeswarm Plot" if font_success else "Multitask Learning Model Sentiment Score SHAP Beeswarm Plot"
        plt.title(title, fontsize=16, pad=20, color='#2C3E50')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/multitask_sentiment_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                    dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("âœ… ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
        # 3. ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢Beeswarmãƒ—ãƒ­ãƒƒãƒˆ
        print("ğŸ”¬ ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢SHAPåˆ†æå®Ÿè¡Œä¸­...")
        explainer_multitask_course = shap.Explainer(predict_multitask_course, multitask_tokenizer)
        shap_values_multitask_course = explainer_multitask_course(sample_texts)
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values_multitask_course, sample_texts, show=False)
        title = "ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢SHAP Beeswarm Plot" if font_success else "Multitask Learning Model Course Score SHAP Beeswarm Plot"
        plt.title(title, fontsize=16, pad=20, color='#2C3E50')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/multitask_course_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                    dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("âœ… ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
        # 4. æ¯”è¼ƒç”¨ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ
        print("ğŸ“Š æ¯”è¼ƒç”¨ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆä¸­...")
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle("SHAP Beeswarm Plot æ¯”è¼ƒ: å˜ä¸€ã‚¿ã‚¹ã‚¯ vs ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’" if font_success else "SHAP Beeswarm Plot Comparison: Single Task vs Multitask Learning", 
                     fontsize=18, color='#2C3E50')
        
        # å˜ä¸€ã‚¿ã‚¹ã‚¯
        shap.summary_plot(shap_values_single, sample_texts, show=False, ax=axes[0,0])
        axes[0,0].set_title("å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æ" if font_success else "Single Task Sentiment", fontsize=14)
        
        # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
        shap.summary_plot(shap_values_multitask_sentiment, sample_texts, show=False, ax=axes[0,1])
        axes[0,1].set_title("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢" if font_success else "Multitask Sentiment Score", fontsize=14)
        
        # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢
        shap.summary_plot(shap_values_multitask_course, sample_texts, show=False, ax=axes[1,0])
        axes[1,0].set_title("ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢" if font_success else "Multitask Course Score", fontsize=14)
        
        # ç©ºã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
        axes[1,1].text(0.5, 0.5, "å°†æ¥ã®æ‹¡å¼µç”¨" if font_success else "Future Extension", 
                       ha='center', va='center', fontsize=16, color='gray')
        axes[1,1].set_title("æ‹¡å¼µäºˆå®š" if font_success else "Future Extension", fontsize=14)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/comparison_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                    dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("âœ… æ¯”è¼ƒç”¨ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
        
        # çµæœã®ä¿å­˜
        results = {
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "analysis_type": "beeswarm_comparison",
            "sample_size": len(sample_texts),
            "models": {
                "single_task": {
                    "shap_values_shape": shap_values_single.shape,
                    "model_type": "sentiment_classification"
                },
                "multitask_sentiment": {
                    "shap_values_shape": shap_values_multitask_sentiment.shape,
                    "model_type": "sentiment_regression"
                },
                "multitask_course": {
                    "shap_values_shape": shap_values_multitask_course.shape,
                    "model_type": "course_regression"
                }
            },
            "output_files": [
                f"single_task_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                f"multitask_sentiment_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                f"multitask_course_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                f"comparison_beeswarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            ],
            "font_success": font_success
        }
        
        with open(f"{output_dir}/beeswarm_comparison_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… çµæœä¿å­˜å®Œäº†: {output_dir}")
        
    except Exception as e:
        print(f"âŒ Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”„ ç°¡æ˜“ç‰ˆã‚’å®Ÿè¡Œã—ã¾ã™...")
        
        # ç°¡æ˜“ç‰ˆï¼ˆã‚ˆã‚Šå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰
        try:
            sample_texts_small = sample_texts[:5]  # 5ä»¶ã§ãƒ†ã‚¹ãƒˆ
            
            # å˜ä¸€ã‚¿ã‚¹ã‚¯
            explainer_single = shap.Explainer(predict_single, single_tokenizer)
            shap_values_single = explainer_single(sample_texts_small)
            
            plt.figure(figsize=(10, 6))
            shap.summary_plot(shap_values_single, sample_texts_small, show=False)
            plt.title("å˜ä¸€ã‚¿ã‚¹ã‚¯æ„Ÿæƒ…åˆ†æ (ç°¡æ˜“ç‰ˆ)" if font_success else "Single Task Sentiment (Simple)", fontsize=14)
            plt.tight_layout()
            plt.savefig(f"{output_dir}/single_task_beeswarm_simple_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                        dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print("âœ… ç°¡æ˜“ç‰ˆBeeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆå®Œäº†")
            
        except Exception as e2:
            print(f"âŒ ç°¡æ˜“ç‰ˆã‚‚ã‚¨ãƒ©ãƒ¼: {e2}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Beeswarmãƒ—ãƒ­ãƒƒãƒˆæ¯”è¼ƒåˆ†æã‚’é–‹å§‹...")
    
    # 1. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    single_model, single_tokenizer = load_single_task_model()
    multitask_model, multitask_tokenizer = load_multitask_model()
    
    if single_model is None or multitask_model is None:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_data()
    if df is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # 3. Beeswarmãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    create_beeswarm_plots(single_model, single_tokenizer, multitask_model, multitask_tokenizer, df)
    
    print("\nğŸ‰ Beeswarmãƒ—ãƒ­ãƒƒãƒˆæ¯”è¼ƒåˆ†æå®Œäº†ï¼")
    print("ğŸ“ çµæœã¯ '03_åˆ†æçµæœ/SHAP_Beeswarmæ¯”è¼ƒ' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()
