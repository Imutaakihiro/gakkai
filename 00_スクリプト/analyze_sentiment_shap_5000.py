#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SHAPåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ5,000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å°‚ç”¨ï¼‰
8ä¸‡ä»¶ã‹ã‚‰5,000ä»¶ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ã€èªå˜ä½ã§SHAPé›†è¨ˆã®ã¿å®Ÿæ–½
"""

import torch
import pandas as pd
import numpy as np
from transformers import BertForSequenceClassification, BertJapaneseTokenizer
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from tqdm import tqdm
import json
import os
from datetime import datetime

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

print("="*60)
print("SHAPåˆ†æ: 5,000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‰ˆ")
print("="*60)

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
MODEL_PATH = r"C:\Users\takahashi.DESKTOP-U0T5SUB\Downloads\BERT\git_excluded\finetuned_bert_model_20250718_step2_fixed_classweights_variant1_positiveé‡ç‚¹å¼·åŒ–"
print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_PATH}")
tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.to(device)
model.eval()

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‹ã‚‰å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
RAW_TEXT_PATH = r"../æ„Ÿæƒ…åˆ†é¡çµæœ_å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿çµåˆ_20250729_154855.csv"
SAMPLE_SIZE = 5000

print(f"\nå…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {RAW_TEXT_PATH}")
raw_df = pd.read_csv(RAW_TEXT_PATH)

# å¿…è¦ãªåˆ—: è‡ªç”±è¨˜è¿°, tuned_model_label
print(f"å…ƒãƒ‡ãƒ¼ã‚¿ç·ä»¶æ•°: {len(raw_df):,}ä»¶")

# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
raw_df_clean = raw_df[['è‡ªç”±è¨˜è¿°', 'tuned_model_label']].dropna()
print(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {len(raw_df_clean):,}ä»¶")

# ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã‚’ç¢ºèª
label_counts = raw_df_clean['tuned_model_label'].value_counts()
print(f"\nå…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
for label, count in label_counts.items():
    print(f"  {label}: {count:,}ä»¶ ({count/len(raw_df_clean)*100:.1f}%)")

# å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã¿ã€å„2,500ä»¶ãšã¤ï¼‰
samples_per_class = SAMPLE_SIZE // 2  # å„ã‚¯ãƒ©ã‚¹2,500ä»¶
print(f"\nå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: POSITIVE/NEGATIVEã®ã¿ã€å„{samples_per_class}ä»¶ãšã¤")
print("ï¼ˆæ³¨: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã¯é™¤å¤–ã—ã€æ˜ç¢ºãªãƒã‚¸/ãƒã‚¬ã®å¯¾æ¯”ã‚’åˆ†æï¼‰")

sampled_dfs = []
for label in ['POSITIVE', 'NEGATIVE']:
    df_label = raw_df_clean[raw_df_clean['tuned_model_label'] == label]
    n_sample = min(samples_per_class, len(df_label))
    sampled_dfs.append(df_label.sample(n=n_sample, random_state=42))
    print(f"  {label}: {n_sample}ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")

sample_df = pd.concat(sampled_dfs, ignore_index=True)
sample_texts = sample_df['è‡ªç”±è¨˜è¿°'].astype(str).tolist()
sample_labels = sample_df['tuned_model_label'].tolist()

print(f"\næœ€çµ‚ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {len(sample_texts)}ä»¶")
print(f"ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
for label in ['POSITIVE', 'NEGATIVE']:
    count = sample_labels.count(label)
    print(f"  {label}: {count}ä»¶ ({count/len(sample_labels)*100:.1f}%)")


# äºˆæ¸¬é–¢æ•°ï¼ˆSHAPç”¨ï¼‰
def predict_proba(texts):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’è¿”ã™"""
    # SHAPã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿å‹ã‚’å‡¦ç†
    if isinstance(texts, str):
        texts = [texts]
    elif isinstance(texts, np.ndarray):
        texts = texts.tolist()
    elif not isinstance(texts, list):
        try:
            texts = list(texts)
        except:
            texts = [str(texts)]
    
    # ç©ºæ–‡å­—åˆ—ã‚„ç„¡åŠ¹ãªå…¥åŠ›ã‚’å‡¦ç†
    texts = [str(t) if t else "" for t in texts]
    
    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
    
    return probs.cpu().numpy()

# SHAP Explainerã®ä½œæˆ
print("\n" + "="*60)
print("SHAP Explainerã®ä½œæˆ")
print("="*60)
print("Explainerã‚’åˆæœŸåŒ–ä¸­...")
masker = shap.maskers.Text(tokenizer)
explainer = shap.Explainer(predict_proba, masker, algorithm="partition")

# ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰çµ±åˆé–¢æ•°
def merge_wordpieces(tokens, shap_vals_pos):
    """WordPieceã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆ##ï¼‰ã‚’å‰ã®èªã«çµåˆã—ã¦é›†ç´„ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: (merged_tokens, merged_shap_vals)
    """
    merged_tokens = []
    merged_vals = []
    current = ''
    current_val = 0.0
    for tok, val in zip(tokens, shap_vals_pos):
        t = str(tok)
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
        if t in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:
            continue
        if t.startswith('##'):
            # é€£çµï¼ˆæ¥é ­ã®##ã‚’é™¤å»ã—ã¦å‰èªã«è¿½åŠ ï¼‰
            current += t[2:]
            current_val += float(val)
        else:
            # ç›´å‰ã®èªã‚’ç¢ºå®š
            if current:
                merged_tokens.append(current)
                merged_vals.append(current_val)
            current = t
            current_val = float(val)
    if current:
        merged_tokens.append(current)
        merged_vals.append(current_val)
    return merged_tokens, merged_vals

# SHAPåˆ†æï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ é›†è¨ˆï¼‰
print("\n" + "="*60)
print("SHAPåˆ†æé–‹å§‹ï¼ˆ5,000ä»¶ï¼‰")
print("="*60)

word_importance_sample = defaultdict(lambda: {'shap_values': [], 'count': 0})
batch_size = 64

for i in tqdm(range(0, len(sample_texts), batch_size), desc="SHAPé›†è¨ˆ"):
    bt = sample_texts[i:i+batch_size]
    sv_batch = explainer(bt)
    
    for sv in sv_batch:
        tokens = sv.data
        vals = sv.values
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚¯ãƒ©ã‚¹ï¼ˆindex=2ï¼‰ã®SHAPå€¤ã‚’ä½¿ç”¨
        if len(vals.shape) > 1:
            vals_pos = vals[:, 2]
        else:
            vals_pos = vals
        
        # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰çµ±åˆ
        m_toks, m_vals = merge_wordpieces(tokens, vals_pos)
        
        for t, v in zip(m_toks, m_vals):
            if not t:
                continue
            word_importance_sample[t]['shap_values'].append(float(v))
            word_importance_sample[t]['count'] += 1

# DataFrameåŒ–ï¼ˆå‡ºç¾5å›ä»¥ä¸Šã®ã¿ï¼‰
print("\nå˜èªçµ±è¨ˆã‚’é›†è¨ˆä¸­...")
word_stats_sample = {
    w: {
        'mean_shap': float(np.mean(d['shap_values'])),
        'abs_mean_shap': float(np.mean(np.abs(d['shap_values']))),
        'std_shap': float(np.std(d['shap_values'])),
        'count': int(d['count'])
    }
    for w, d in word_importance_sample.items() if d['count'] >= 5
}
df_sample = pd.DataFrame(word_stats_sample).T.sort_values('mean_shap', ascending=False)

# çµæœä¿å­˜
print("\n" + "="*60)
print("çµæœã‚’ä¿å­˜ä¸­")
print("="*60)

out_dir = "../03_åˆ†æçµæœ/SHAPåˆ†æ/ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°5000ä»¶"
os.makedirs(out_dir, exist_ok=True)
os.makedirs(f"{out_dir}/å¯è¦–åŒ–", exist_ok=True)

# CSVä¿å­˜
csv_path = f"{out_dir}/word_importance_sample5000.csv"
df_sample.to_csv(csv_path, encoding='utf-8-sig')
print(f"âœ“ CSVä¿å­˜: {csv_path}")

# JSONä¿å­˜
json_path = f"{out_dir}/global_importance_sample5000.json"
with open(json_path, 'w', encoding='utf-8') as f:
    json.dump({
        'analysis_date': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'dataset_size': len(sample_texts),
        'model_path': MODEL_PATH,
        'sample_method': 'random_sampling',
        'random_state': 42,
        'top_positive_words': df_sample.head(50).to_dict('index'),
        'top_negative_words': df_sample.tail(50).to_dict('index')
    }, f, ensure_ascii=False, indent=2)
print(f"âœ“ JSONä¿å­˜: {json_path}")

# TOP20å¯è¦–åŒ–ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰
print("\nTOP20ã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰...")
top_positive = df_sample.head(20)
plt.figure(figsize=(10, 8))
plt.barh(range(len(top_positive)), top_positive['mean_shap'], color='green', alpha=0.7)
plt.yticks(range(len(top_positive)), top_positive.index)
plt.xlabel('å¹³å‡SHAPå€¤ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–å¯„ä¸ï¼‰', fontsize=12)
plt.title('ãƒã‚¸ãƒ†ã‚£ãƒ–åˆ¤å®šã«å¯„ä¸ã™ã‚‹é‡è¦èª TOP20ï¼ˆ5,000ä»¶ã‚µãƒ³ãƒ—ãƒ«ï¼‰', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
pos_path = f"{out_dir}/å¯è¦–åŒ–/top20_positive_sample5000.png"
plt.savefig(pos_path, dpi=300, bbox_inches='tight')
print(f"âœ“ ã‚°ãƒ©ãƒ•ä¿å­˜: {pos_path}")
plt.close()

# TOP20å¯è¦–åŒ–ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
print("TOP20ã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰...")
top_negative = df_sample.tail(20).iloc[::-1]
plt.figure(figsize=(10, 8))
plt.barh(range(len(top_negative)), top_negative['mean_shap'], color='red', alpha=0.7)
plt.yticks(range(len(top_negative)), top_negative.index)
plt.xlabel('å¹³å‡SHAPå€¤ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–å¯„ä¸ï¼‰', fontsize=12)
plt.title('ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¤å®šã«å¯„ä¸ã™ã‚‹é‡è¦èª TOP20ï¼ˆ5,000ä»¶ã‚µãƒ³ãƒ—ãƒ«ï¼‰', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
neg_path = f"{out_dir}/å¯è¦–åŒ–/top20_negative_sample5000.png"
plt.savefig(neg_path, dpi=300, bbox_inches='tight')
print(f"âœ“ ã‚°ãƒ©ãƒ•ä¿å­˜: {neg_path}")
plt.close()

# ã‚µãƒãƒªãƒ¼Markdownä½œæˆ
print("\nã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­...")
summary_md = f"""# SHAPåˆ†æã‚µãƒãƒªãƒ¼ï¼ˆ5,000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰

**åˆ†ææ—¥æ™‚:** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  
**å¯¾è±¡ãƒ‡ãƒ¼ã‚¿:** ç´„8.3ä¸‡ä»¶ã‹ã‚‰å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°5,000ä»¶ï¼ˆPOSITIVE/NEGATIVEã®ã¿ï¼‰  
**ãƒ¢ãƒ‡ãƒ«:** å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«1ï¼ˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼‰  
**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹æ³•:** å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–2,500ä»¶ + ãƒã‚¬ãƒ†ã‚£ãƒ–2,500ä»¶ã€random_state=42ï¼‰  
**æ³¨:** ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‚’é™¤å¤–ã—ã€æº€è¶³/ä¸æº€ã®æ˜ç¢ºãªå¯¾æ¯”ã‚’åˆ†æ

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦

- **ç·ãƒ‡ãƒ¼ã‚¿æ•°:** {len(raw_df_clean):,}ä»¶
- **ã‚µãƒ³ãƒ—ãƒ«æ•°:** {len(sample_texts):,}ä»¶ï¼ˆPOSITIVE: {samples_per_class}ä»¶ã€NEGATIVE: {samples_per_class}ä»¶ï¼‰
- **ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«:** é™¤å¤–ï¼ˆæ˜ç¢ºãªæº€è¶³/ä¸æº€ã®å¯¾æ¯”ã‚’åˆ†æã™ã‚‹ãŸã‚ï¼‰
- **åˆ†æå¯¾è±¡å˜èªæ•°:** {len(df_sample)}èªï¼ˆå‡ºç¾5å›ä»¥ä¸Šï¼‰

---

## ğŸ” ãƒã‚¸ãƒ†ã‚£ãƒ–åˆ¤å®šã«å¯„ä¸ã™ã‚‹é‡è¦èª TOP20

| é †ä½ | å˜èª | å¹³å‡SHAPå€¤ | å‡ºç¾å›æ•° |
|------|------|-----------|---------|
"""

for i, (word, row) in enumerate(df_sample.head(20).iterrows(), 1):
    summary_md += f"| {i} | {word} | {row['mean_shap']:.4f} | {row['count']} |\n"

summary_md += """
---

## ğŸ”» ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¤å®šã«å¯„ä¸ã™ã‚‹é‡è¦èª TOP20

| é †ä½ | å˜èª | å¹³å‡SHAPå€¤ | å‡ºç¾å›æ•° |
|------|------|-----------|---------|
"""

for i, (word, row) in enumerate(df_sample.tail(20).iloc[::-1].iterrows(), 1):
    summary_md += f"| {i} | {word} | {row['mean_shap']:.4f} | {row['count']} |\n"

summary_md += f"""
---

## ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«

- `word_importance_sample5000.csv` - å…¨å˜èªã®é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆExcelç”¨ï¼‰
- `global_importance_sample5000.json` - JSONå½¢å¼ã®é›†è¨ˆçµæœ
- `å¯è¦–åŒ–/top20_positive_sample5000.png` - ãƒã‚¸ãƒ†ã‚£ãƒ–TOP20ã‚°ãƒ©ãƒ•
- `å¯è¦–åŒ–/top20_negative_sample5000.png` - ãƒã‚¬ãƒ†ã‚£ãƒ–TOP20ã‚°ãƒ©ãƒ•

---

## ğŸ’¡ ä¸»è¦ãªç™ºè¦‹

### æº€è¶³åº¦ã‚’é«˜ã‚ã‚‹è¦å› ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–TOP5ï¼‰
{chr(10).join([f"- **{word}**: {row['mean_shap']:.4f}ï¼ˆ{row['count']}å›å‡ºç¾ï¼‰" for word, row in df_sample.head(5).iterrows()])}

### ä¸æº€ã®åŸå› ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–TOP5ï¼‰
{chr(10).join([f"- **{word}**: {row['mean_shap']:.4f}ï¼ˆ{row['count']}å›å‡ºç¾ï¼‰" for word, row in df_sample.tail(5).iloc[::-1].iterrows()])}

---

**åˆ†æå®Œäº†ï¼**  
çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ `{out_dir}/` ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚
"""

summary_path = f"{out_dir}/SHAPåˆ†æã‚µãƒãƒªãƒ¼_sample5000.md"
with open(summary_path, 'w', encoding='utf-8') as f:
    f.write(summary_md)
print(f"âœ“ ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_path}")

print("\n" + "="*60)
print("SHAPåˆ†æå®Œäº†ï¼ˆ5,000ä»¶ï¼‰ï¼")
print("="*60)
print(f"\nçµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
print(f"  {out_dir}/")
print(f"\nãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
print(f"  - word_importance_sample5000.csv")
print(f"  - global_importance_sample5000.json")
print(f"  - SHAPåˆ†æã‚µãƒãƒªãƒ¼_sample5000.md")
print(f"  - å¯è¦–åŒ–/top20_positive_sample5000.png")
print(f"  - å¯è¦–åŒ–/top20_negative_sample5000.png")

