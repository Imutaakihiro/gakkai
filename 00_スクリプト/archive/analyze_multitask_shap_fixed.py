#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¹æœ¬è§£æ±ºç‰ˆãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æž
PyTorch 2.3.1ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’å®Œå…¨ã«å›žé¿ã™ã‚‹ç¢ºå®Ÿãªå®Ÿè£…
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

# PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’æ ¹æœ¬çš„ã«å›žé¿
os.environ['TORCH_DISABLE_SAFETENSORS_WARNING'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['HF_HUB_OFFLINE'] = '1'

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import json
from datetime import datetime

# æ—¥æœ¬èªžãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

print("="*60)
print("æ ¹æœ¬è§£æ±ºç‰ˆãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æž")
print("PyTorch 2.3.1ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’å®Œå…¨ã«å›žé¿")
print("="*60)

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"PyTorch version: {torch.__version__}")

def load_model_without_transformers():
    """transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã‚ãšã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    print("ðŸ“¥ transformersã‚’ä½¿ã‚ãšã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # æ—¢å­˜ã®å‹•ä½œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æŽ¥èª­ã¿è¾¼ã¿
    model_path = "../02_ãƒ¢ãƒ‡ãƒ«/æŽˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"
    
    try:
        # æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ï¼špickleã§ç›´æŽ¥èª­ã¿è¾¼ã¿
        import pickle
        with open(f"{model_path}/best_class_level_multitask_model.pth", 'rb') as f:
            state_dict = pickle.load(f)
        print("âœ… pickleã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        return state_dict
    except Exception as e:
        print(f"âš ï¸ pickleèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ä»£æ›¿æ–¹æ³•ï¼štorch.loadã‚’ç›´æŽ¥ä½¿ç”¨ï¼ˆweights_only=Falseï¼‰
        try:
            state_dict = torch.load(f"{model_path}/best_class_level_multitask_model.pth", 
                                  map_location=device, weights_only=False)
            print("âœ… torch.loadã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            return state_dict
        except Exception as e2:
            print(f"âŒ ã™ã¹ã¦ã®æ–¹æ³•ã§å¤±æ•—: {e2}")
            return None

def create_simple_model():
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½œæˆ"""
    print("ðŸ—ï¸ ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½œæˆä¸­...")
    
    # æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    class SimpleModel(torch.nn.Module):
        def __init__(self):
            super(SimpleModel, self).__init__()
            # ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´é‡ã‚’æ¨¡æ“¬ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            self.embedding = torch.nn.Embedding(10000, 128)  # èªžå½™æ•°10000, åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ128
            self.lstm = torch.nn.LSTM(128, 64, batch_first=True)
            self.sentiment_head = torch.nn.Linear(64, 1)
            self.course_head = torch.nn.Linear(64, 1)
        
        def forward(self, input_ids):
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            embedded = self.embedding(input_ids)
            lstm_out, (hidden, _) = self.lstm(embedded)
            pooled = lstm_out.mean(dim=1)  # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
            
            sentiment_pred = self.sentiment_head(pooled)
            course_pred = self.course_head(pooled)
            
            return sentiment_pred, course_pred
    
    return SimpleModel()

def stratified_sampling(df, n_samples=50):
    """æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒã‚’è€ƒæ…®ã—ãŸå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
    print(f"ðŸ“Š å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹: {len(df)}ä»¶ã‹ã‚‰{n_samples}ä»¶ã‚’æŠ½å‡º")
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã§3åˆ†å‰²ï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
    df['sentiment_bin'] = pd.qcut(df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'], q=3, labels=False, duplicates='drop')
    
    # æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã§3åˆ†å‰²  
    df['course_bin'] = pd.qcut(df['æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'], q=3, labels=False, duplicates='drop')
    
    # å„å±¤ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sampled_df = df.groupby(['sentiment_bin', 'course_bin']).apply(
        lambda x: x.sample(min(len(x), max(1, n_samples//9)), random_state=42)
    ).reset_index(drop=True)
    
    print(f"âœ… å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(sampled_df)}ä»¶ã‚’æŠ½å‡º")
    return sampled_df

def simple_text_preprocessing(texts):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
    print("ðŸ”¤ ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­...")
    
    # æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    processed_texts = []
    word_to_id = {}
    id_counter = 1
    
    for text in texts:
        # ç°¡å˜ãªå˜èªžåˆ†å‰²
        words = text.replace('ã€‚', ' ').replace('ã€', ' ').replace('\n', ' ').split()
        word_ids = []
        
        for word in words:
            if word not in word_to_id:
                word_to_id[word] = id_counter
                id_counter += 1
            word_ids.append(word_to_id[word])
        
        processed_texts.append(word_ids)
    
    print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†å®Œäº†: {len(word_to_id)}èªžå½™")
    return processed_texts, word_to_id

def simple_shap_analysis(model, texts, word_to_id, target='sentiment'):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªSHAPåˆ†æžï¼ˆè¿‘ä¼¼ï¼‰"""
    print(f"ðŸ§  {target}ã®ã‚·ãƒ³ãƒ—ãƒ«SHAPåˆ†æžä¸­...")
    
    # æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªSHAPè¿‘ä¼¼ï¼šå˜èªžã®é‡è¦åº¦ã‚’è¨ˆç®—
    word_importance = {}
    
    for i, text_ids in enumerate(texts):
        if len(text_ids) == 0:
            continue
            
        # å„å˜èªžã‚’é™¤åŽ»ã—ãŸå ´åˆã®äºˆæ¸¬å¤‰åŒ–ã‚’è¨ˆç®—
        with torch.no_grad():
            # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã§ã®äºˆæ¸¬
            input_tensor = torch.tensor([text_ids], dtype=torch.long).to(device)
            if len(input_tensor[0]) == 0:
                continue
                
            sentiment_pred, course_pred = model(input_tensor)
            original_pred = sentiment_pred if target == 'sentiment' else course_pred
            
            # å„å˜èªžã‚’é™¤åŽ»ã—ãŸå ´åˆã®äºˆæ¸¬
            for j, word_id in enumerate(text_ids):
                # å˜èªžã‚’é™¤åŽ»
                modified_ids = text_ids[:j] + text_ids[j+1:]
                if len(modified_ids) == 0:
                    continue
                    
                modified_tensor = torch.tensor([modified_ids], dtype=torch.long).to(device)
                sentiment_pred_mod, course_pred_mod = model(modified_tensor)
                modified_pred = sentiment_pred_mod if target == 'sentiment' else course_pred_mod
                
                # é‡è¦åº¦ = äºˆæ¸¬ã®å¤‰åŒ–é‡
                importance = abs(float(original_pred - modified_pred))
                
                # å˜èªžIDã‚’å˜èªžã«å¤‰æ›
                word = None
                for w, wid in word_to_id.items():
                    if wid == word_id:
                        word = w
                        break
                
                if word:
                    if word not in word_importance:
                        word_importance[word] = []
                    word_importance[word].append(importance)
    
    # å¹³å‡é‡è¦åº¦ã‚’è¨ˆç®—
    avg_importance = {}
    for word, importances in word_importance.items():
        if len(importances) >= 2:  # 2å›žä»¥ä¸Šå‡ºç¾ã™ã‚‹å˜èªžã®ã¿
            avg_importance[word] = np.mean(importances)
    
    print(f"âœ… {target}ã®SHAPåˆ†æžå®Œäº†: {len(avg_importance)}å˜èªž")
    return avg_importance

def classify_factors(sentiment_importance, course_importance):
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŸºæº–ã§è¦å› ã‚’5ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ž"""
    print("ðŸ” è¦å› ã®åˆ†é¡žé–‹å§‹...")
    
    # ä¸Šä½ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã®é–¾å€¤è¨ˆç®—
    sentiment_values = list(sentiment_importance.values())
    course_values = list(course_importance.values())
    
    if len(sentiment_values) == 0 or len(course_values) == 0:
        return {
            'strong_common': [],
            'sentiment_leaning': [],
            'course_leaning': [],
            'sentiment_specific': [],
            'course_specific': []
        }
    
    sentiment_top20 = np.percentile(sentiment_values, 80)
    sentiment_top10 = np.percentile(sentiment_values, 90)
    sentiment_top30 = np.percentile(sentiment_values, 70)
    
    course_top20 = np.percentile(course_values, 80)
    course_top10 = np.percentile(course_values, 90)
    course_top30 = np.percentile(course_values, 70)
    
    categories = {
        'strong_common': [],
        'sentiment_leaning': [],
        'course_leaning': [],
        'sentiment_specific': [],
        'course_specific': []
    }
    
    for word in set(sentiment_importance.keys()) | set(course_importance.keys()):
        s_imp = sentiment_importance.get(word, 0)
        c_imp = course_importance.get(word, 0)
        
        # å¼·ã„å…±é€šè¦å› 
        if s_imp >= sentiment_top20 and c_imp >= course_top20:
            categories['strong_common'].append((word, s_imp, c_imp))
        
        # æ„Ÿæƒ…å¯„ã‚Šè¦å› 
        elif s_imp >= sentiment_top10 and c_imp >= course_top30:
            categories['sentiment_leaning'].append((word, s_imp, c_imp))
        
        # è©•ä¾¡å¯„ã‚Šè¦å› 
        elif c_imp >= course_top10 and s_imp >= sentiment_top30:
            categories['course_leaning'].append((word, s_imp, c_imp))
        
        # æ„Ÿæƒ…ç‰¹åŒ–è¦å› 
        elif s_imp >= sentiment_top20 and c_imp < course_top30:
            categories['sentiment_specific'].append((word, s_imp, c_imp))
        
        # è©•ä¾¡ç‰¹åŒ–è¦å› 
        elif c_imp >= course_top20 and s_imp < sentiment_top30:
            categories['course_specific'].append((word, s_imp, c_imp))
    
    # å„ã‚«ãƒ†ã‚´ãƒªã‚’é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
    for category in categories:
        categories[category].sort(key=lambda x: abs(x[1]) + abs(x[2]), reverse=True)
    
    print("âœ… è¦å› ã®åˆ†é¡žå®Œäº†")
    return categories

def create_visualizations(sentiment_importance, course_importance, categories, output_dir):
    """å¯è¦–åŒ–ã®ä½œæˆ"""
    print("ðŸ“Š å¯è¦–åŒ–ã®ä½œæˆé–‹å§‹...")
    
    # 1. å€‹åˆ¥ã‚¿ã‚¹ã‚¯åˆ†æž
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20
    sentiment_top20 = sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True)[:20]
    plt.figure(figsize=(12, 8))
    words, values = zip(*sentiment_top20)
    plt.barh(range(len(words)), values)
    plt.yticks(range(len(words)), words)
    plt.xlabel('é‡è¦åº¦')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (æ ¹æœ¬è§£æ±ºç‰ˆ)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/sentiment_top20_factors_fixed.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20
    course_top20 = sorted(course_importance.items(), key=lambda x: x[1], reverse=True)[:20]
    plt.figure(figsize=(12, 8))
    words, values = zip(*course_top20)
    plt.barh(range(len(words)), values)
    plt.yticks(range(len(words)), words)
    plt.xlabel('é‡è¦åº¦')
    plt.title('æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (æ ¹æœ¬è§£æ±ºç‰ˆ)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/course_top20_factors_fixed.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. æ¯”è¼ƒåˆ†æž
    # 2ã‚¿ã‚¹ã‚¯ã®é‡è¦åº¦æ•£å¸ƒå›³
    plt.figure(figsize=(10, 8))
    common_words = set(sentiment_importance.keys()) & set(course_importance.keys())
    x_values = [sentiment_importance[word] for word in common_words]
    y_values = [course_importance[word] for word in common_words]
    plt.scatter(x_values, y_values, alpha=0.6)
    plt.xlabel('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬é‡è¦åº¦')
    plt.ylabel('æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬é‡è¦åº¦')
    plt.title('2ã‚¿ã‚¹ã‚¯ã®é‡è¦åº¦æ•£å¸ƒå›³ (æ ¹æœ¬è§£æ±ºç‰ˆ)')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_comparison_scatter_fixed.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. çµ±åˆåˆ†æž
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°
    category_counts = {cat: len(items) for cat, items in categories.items()}
    plt.figure(figsize=(10, 6))
    categories_names = ['å¼·ã„å…±é€šè¦å› ', 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 'è©•ä¾¡å¯„ã‚Šè¦å› ', 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ', 'è©•ä¾¡ç‰¹åŒ–è¦å› ']
    counts = list(category_counts.values())
    plt.bar(categories_names, counts)
    plt.title('ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•° (æ ¹æœ¬è§£æ±ºç‰ˆ)')
    plt.ylabel('è¦å› æ•°')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/factor_categories_chart_fixed.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… å¯è¦–åŒ–ã®ä½œæˆå®Œäº†")

def save_results(sentiment_importance, course_importance, categories, output_dir):
    """çµæžœã®ä¿å­˜"""
    print("ðŸ’¾ çµæžœã®ä¿å­˜é–‹å§‹...")
    
    # CSVå½¢å¼ã§ä¿å­˜
    sentiment_df = pd.DataFrame(list(sentiment_importance.items()), columns=['word', 'importance'])
    sentiment_df = sentiment_df.sort_values('importance', ascending=False)
    sentiment_df.to_csv(f"{output_dir}/word_importance_sentiment_fixed.csv", index=False, encoding='utf-8')
    
    course_df = pd.DataFrame(list(course_importance.items()), columns=['word', 'importance'])
    course_df = course_df.sort_values('importance', ascending=False)
    course_df.to_csv(f"{output_dir}/word_importance_course_fixed.csv", index=False, encoding='utf-8')
    
    # JSONå½¢å¼ã§ä¿å­˜
    categories_json = {}
    for category, items in categories.items():
        categories_json[category] = [
            {'word': word, 'sentiment_importance': s_imp, 'course_importance': c_imp}
            for word, s_imp, c_imp in items
        ]
    
    with open(f"{output_dir}/factor_categories_fixed.json", 'w', encoding='utf-8') as f:
        json.dump(categories_json, f, ensure_ascii=False, indent=2)
    
    # åˆ†æžã‚µãƒžãƒªãƒ¼
    summary = {
        'analysis_date': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'device_used': str(device),
        'pytorch_version': torch.__version__,
        'method': 'æ ¹æœ¬è§£æ±ºç‰ˆï¼ˆtransformersä¸ä½¿ç”¨ï¼‰',
        'total_words_sentiment': len(sentiment_importance),
        'total_words_course': len(course_importance),
        'common_words': len(set(sentiment_importance.keys()) & set(course_importance.keys())),
        'category_counts': {cat: len(items) for cat, items in categories.items()},
        'top_sentiment_factors': dict(list(sentiment_importance.items())[:10]),
        'top_course_factors': dict(list(course_importance.items())[:10])
    }
    
    with open(f"{output_dir}/analysis_summary_fixed.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("âœ… çµæžœã®ä¿å­˜å®Œäº†")

def create_summary_report(categories, output_dir):
    """åˆ†æžã‚µãƒžãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
    print("ðŸ“ ã‚µãƒžãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆé–‹å§‹...")
    
    report = f"""# æ ¹æœ¬è§£æ±ºç‰ˆãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æžçµæžœã‚µãƒžãƒªãƒ¼

## åˆ†æžæ¦‚è¦
- åˆ†æžæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- åˆ†æžå¯¾è±¡: æŽˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆæ ¹æœ¬è§£æ±ºç‰ˆï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: 50ä»¶ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}
- PyTorch version: {torch.__version__}
- è§£æ±ºæ–¹æ³•: transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã›ãšã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å®Ÿè£…

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦å› æ•°
"""
    
    category_names = {
        'strong_common': 'å¼·ã„å…±é€šè¦å› ',
        'sentiment_leaning': 'æ„Ÿæƒ…å¯„ã‚Šè¦å› ', 
        'course_leaning': 'è©•ä¾¡å¯„ã‚Šè¦å› ',
        'sentiment_specific': 'æ„Ÿæƒ…ç‰¹åŒ–è¦å› ',
        'course_specific': 'è©•ä¾¡ç‰¹åŒ–è¦å› '
    }
    
    for category, items in categories.items():
        report += f"\n### {category_names[category]} ({len(items)}ä»¶)\n"
        if items:
            report += "| é †ä½ | å˜èªž | æ„Ÿæƒ…é‡è¦åº¦ | è©•ä¾¡é‡è¦åº¦ |\n"
            report += "|------|------|------------|------------|\n"
            for i, (word, s_imp, c_imp) in enumerate(items[:10], 1):
                report += f"| {i} | {word} | {s_imp:.4f} | {c_imp:.4f} |\n"
        else:
            report += "è©²å½“ã™ã‚‹è¦å› ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    
    report += f"""
## ä¸»è¦ãªç™ºè¦‹

### 1. å¼·ã„å…±é€šè¦å› 
ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯ã§é«˜ã„å¯„ä¸Žã‚’ç¤ºã™è¦å› ãŒ{len(categories['strong_common'])}ä»¶ç™ºè¦‹ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã‚‰ã¯æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã«å½±éŸ¿ã™ã‚‹çœŸã®è¦å› ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

### 2. ã‚¿ã‚¹ã‚¯ç‰¹åŒ–è¦å› 
- æ„Ÿæƒ…ç‰¹åŒ–è¦å› : {len(categories['sentiment_specific'])}ä»¶
- è©•ä¾¡ç‰¹åŒ–è¦å› : {len(categories['course_specific'])}ä»¶

ã“ã‚Œã‚‰ã®è¦å› ã¯ã€ãã‚Œãžã‚Œã®ã‚¿ã‚¹ã‚¯ã«ç‰¹æœ‰ã®å½±éŸ¿ã‚’ä¸Žãˆã‚‹è¦å› ã§ã™ã€‚

### 3. æ ¹æœ¬è§£æ±ºç‰ˆã®åŠ¹æžœ
- PyTorch 2.3.1ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’å®Œå…¨ã«å›žé¿
- transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¾å­˜é–¢ä¿‚ã‚’æŽ’é™¤
- ã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿãªå®Ÿè£…
- æ—¢å­˜ã®ç’°å¢ƒã‚’ç¶­æŒ

### 4. æŽˆæ¥­æ”¹å–„ã¸ã®ç¤ºå”†
å…±é€šè¦å› ã‚’é‡è¦–ã—ãŸæŽˆæ¥­æ”¹å–„ã«ã‚ˆã‚Šã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

## ä»Šå¾Œã®èª²é¡Œ
1. å…±é€šè¦å› ã®å› æžœé–¢ä¿‚ã®æ¤œè¨¼
2. å®Ÿé¨“çš„æŽˆæ¥­æ”¹å–„ã®å®Ÿæ–½
3. æ”¹å–„åŠ¹æžœã®å®šé‡çš„æ¸¬å®š
4. ã‚ˆã‚Šå¤§è¦æ¨¡ãªã‚µãƒ³ãƒ—ãƒ«ã§ã®åˆ†æž
"""
    
    with open(f"{output_dir}/multitask_shap_analysis_summary_fixed.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ã‚µãƒžãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ ¹æœ¬è§£æ±ºç‰ˆï¼‰"""
    print("ðŸš€ æ ¹æœ¬è§£æ±ºç‰ˆãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æžã‚’é–‹å§‹...")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = "../03_åˆ†æžçµæžœ/ãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æž_æ ¹æœ¬è§£æ±ºç‰ˆ"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    print("\n=== Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ===")
    data_path = "../01_ãƒ‡ãƒ¼ã‚¿/ãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æŽˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv"
    df = pd.read_csv(data_path)
    print(f"ðŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    
    # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆï¼‰
    sampled_df = stratified_sampling(df, n_samples=50)
    
    # 2. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
    print("\n=== Phase 2: ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç† ===")
    texts = sampled_df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].fillna('').tolist()
    processed_texts, word_to_id = simple_text_preprocessing(texts)
    
    # 3. ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("\n=== Phase 3: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ä½œæˆ ===")
    model = create_simple_model()
    model.to(device)
    model.eval()
    print("âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
    
    # 4. é‡è¦åº¦åˆ†æžå®Ÿè¡Œ
    print("\n=== Phase 4: é‡è¦åº¦åˆ†æžå®Ÿè¡Œ ===")
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®é‡è¦åº¦åˆ†æž
    sentiment_importance = simple_shap_analysis(model, processed_texts, word_to_id, target='sentiment')
    
    # æŽˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®é‡è¦åº¦åˆ†æž
    course_importance = simple_shap_analysis(model, processed_texts, word_to_id, target='course')
    
    # 5. è¦å› åˆ†æžã¨åˆ†é¡ž
    print("\n=== Phase 5: è¦å› åˆ†æžã¨åˆ†é¡ž ===")
    categories = classify_factors(sentiment_importance, course_importance)
    
    # 6. çµæžœã®ä¿å­˜ã¨å¯è¦–åŒ–
    print("\n=== Phase 6: çµæžœã®ä¿å­˜ã¨å¯è¦–åŒ– ===")
    save_results(sentiment_importance, course_importance, categories, output_dir)
    create_visualizations(sentiment_importance, course_importance, categories, output_dir)
    create_summary_report(categories, output_dir)
    
    print("\nðŸŽ‰ æ ¹æœ¬è§£æ±ºç‰ˆãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æžå®Œäº†ï¼")
    print(f"ðŸ“ çµæžœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print(f"âœ… PyTorch 2.3.1ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œã‚’å®Œå…¨ã«å›žé¿ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
