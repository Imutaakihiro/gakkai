#!/usr/bin/env python3
"""
BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ãŸãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ
BERTã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§é©åˆ‡ãªæ—¥æœ¬èªå‡¦ç†
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import torch
import torch.nn as nn
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import platform

# Windowsç’°å¢ƒã§ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
if platform.system() == 'Windows':
    # Windowsã§åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®š
    plt.rcParams['font.family'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'MS Mincho', 'DejaVu Sans']
else:
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao']

# æ–‡å­—åŒ–ã‘å¯¾ç­–
plt.rcParams['axes.unicode_minus'] = False

def install_transformers():
    """transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    try:
        import transformers
        print("âœ… transformers ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
        return True
    except ImportError:
        print("ğŸ“¦ transformers ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        os.system("pip install transformers")
        try:
            import transformers
            print("âœ… transformers ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
            return True
        except ImportError:
            print("âŒ transformers ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—")
            return False

def bert_tokenizer_preprocessing(texts):
    """BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
    print("ğŸ”¤ BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­...")
    
    if not install_transformers():
        print("âš ï¸ transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã€‚ç°¡å˜ãªå‰å‡¦ç†ã§ç¶šè¡Œ...")
        return simple_text_preprocessing(texts)
    
    try:
        from transformers import BertJapaneseTokenizer
        
        # BERTæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿
        tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')
        print("âœ… BERTæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
        
        processed_texts = []
        word_to_id = {}
        id_counter = 1
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        word_to_id['<PAD>'] = 0
        word_to_id['<UNK>'] = 1
        word_to_id['<START>'] = 2
        word_to_id['<END>'] = 3
        id_counter = 4
        
        for text in texts:
            text = str(text).replace('\n', ' ').replace('\t', ' ')
            
            # BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            tokens = tokenizer.tokenize(text)
            
            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»ã—ã€æ„å‘³ã®ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿æŠ½å‡º
            meaningful_tokens = []
            for token in tokens:
                # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ##ã§å§‹ã¾ã‚‹ï¼‰ã¯çµåˆ
                if token.startswith('##'):
                    if meaningful_tokens:
                        meaningful_tokens[-1] += token[2:]  # ##ã‚’é™¤å»ã—ã¦çµåˆ
                else:
                    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚„çŸ­ã™ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–
                    if (token not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]'] and 
                        len(token) > 1 and token.strip()):
                        meaningful_tokens.append(token)
            
            word_ids = [word_to_id['<START>']]  # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³
            
            for token in meaningful_tokens:
                if token not in word_to_id:
                    word_to_id[token] = id_counter
                    id_counter += 1
                word_ids.append(word_to_id[token])
            
            word_ids.append(word_to_id['<END>'])  # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³
            processed_texts.append(word_ids)
        
        print(f"âœ… BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‰å‡¦ç†å®Œäº†: {len(word_to_id)}èªå½™")
        return processed_texts, word_to_id
        
    except Exception as e:
        print(f"âš ï¸ BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
        print("ç°¡å˜ãªå‰å‡¦ç†ã§ç¶šè¡Œ...")
        return simple_text_preprocessing(texts)

def simple_text_preprocessing(texts):
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
    print("ğŸ”¤ ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­...")
    
    processed_texts = []
    word_to_id = {}
    id_counter = 1
    
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
    word_to_id['<PAD>'] = 0
    word_to_id['<UNK>'] = 1
    word_to_id['<START>'] = 2
    word_to_id['<END>'] = 3
    id_counter = 4
    
    for text in texts:
        text = str(text).replace('\n', ' ').replace('\t', ' ')
        # ç°¡å˜ãªå˜èªåˆ†å‰²ï¼ˆç©ºç™½ã¨å¥èª­ç‚¹ã§åˆ†å‰²ï¼‰
        text = text.replace('ã€‚', ' ').replace('ã€', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ')
        words = [w for w in text.split() if len(w) > 0]
        
        word_ids = [word_to_id['<START>']]
        
        for word in words:
            if word not in word_to_id:
                word_to_id[word] = id_counter
                id_counter += 1
            word_ids.append(word_to_id[word])
        
        word_ids.append(word_to_id['<END>'])
        processed_texts.append(word_ids)
    
    print(f"âœ… ç°¡å˜ãªå‰å‡¦ç†å®Œäº†: {len(word_to_id)}èªå½™")
    return processed_texts, word_to_id

def create_bert_tokenizer_model(vocab_size, embedding_dim=128, hidden_dim=256):
    """BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œãƒ¢ãƒ‡ãƒ«"""
    print("ğŸ—ï¸ BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
    
    class BertTokenizerMultitaskModel(nn.Module):
        def __init__(self, vocab_size, embedding_dim, hidden_dim):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, embedding_dim)
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
            self.dropout = nn.Dropout(0.3)
            
            # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ˜ãƒƒãƒ‰
            self.sentiment_head = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, 1)
            )
            
            self.course_head = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, 1)
            )
        
        def forward(self, input_ids):
            # åŸ‹ã‚è¾¼ã¿
            embedded = self.embedding(input_ids)
            
            # LSTM
            lstm_out, _ = self.lstm(embedded)
            
            # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
            pooled = torch.mean(lstm_out, dim=1)
            
            # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
            pooled = self.dropout(pooled)
            
            # ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯äºˆæ¸¬
            sentiment_pred = self.sentiment_head(pooled)
            course_pred = self.course_head(pooled)
            
            return sentiment_pred, course_pred
    
    model = BertTokenizerMultitaskModel(vocab_size, embedding_dim, hidden_dim)
    print(f"âœ… BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {vocab_size}èªå½™")
    return model

def bert_tokenizer_shap_analysis(model, texts, word_to_id, target='sentiment', max_length=128):
    """BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œSHAPåˆ†æ"""
    print(f"ğŸ§  {target}ã®BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼SHAPåˆ†æä¸­...")
    
    device = next(model.parameters()).device
    model.eval()
    
    word_importance = {}
    
    for i, text_ids in enumerate(texts):
        if i % 200 == 0:  # å…¨ãƒ‡ãƒ¼ã‚¿ç”¨ã«é€²æ—è¡¨ç¤ºã‚’èª¿æ•´
            print(f"  é€²æ—: {i}/{len(texts)}")
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if len(text_ids) > max_length:
            text_ids = text_ids[:max_length]
        else:
            text_ids = text_ids + [word_to_id['<PAD>']] * (max_length - len(text_ids))
        
        input_tensor = torch.tensor([text_ids], dtype=torch.long).to(device)
        
        with torch.no_grad():
            sentiment_pred, course_pred = model(input_tensor)
            original_pred = sentiment_pred.item() if target == 'sentiment' else course_pred.item()
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®é‡è¦åº¦ã‚’è¨ˆç®—
        for j in range(len(text_ids)):
            if text_ids[j] in [word_to_id['<PAD>'], word_to_id['<UNK>'], word_to_id['<START>'], word_to_id['<END>']]:
                continue
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
            modified_ids = text_ids.copy()
            modified_ids[j] = word_to_id['<UNK>']  # UNKãƒˆãƒ¼ã‚¯ãƒ³ã§ç½®æ›
            
            modified_tensor = torch.tensor([modified_ids], dtype=torch.long).to(device)
            
            with torch.no_grad():
                sentiment_pred_mod, course_pred_mod = model(modified_tensor)
                modified_pred = sentiment_pred_mod.item() if target == 'sentiment' else course_pred_mod.item()
            
            # é‡è¦åº¦ = äºˆæ¸¬ã®å¤‰åŒ–é‡
            importance = abs(float(original_pred - modified_pred))
            
            # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›
            token = None
            for t, tid in word_to_id.items():
                if tid == text_ids[j]:
                    token = t
                    break
            
            if token and token not in ['<PAD>', '<UNK>', '<START>', '<END>']:
                if token not in word_importance:
                    word_importance[token] = []
                word_importance[token].append(importance)
    
    # å¹³å‡é‡è¦åº¦ã‚’è¨ˆç®—ï¼ˆå‡ºç¾5å›ä»¥ä¸Šã€å…¨ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
    avg_importance = {}
    for token, importances in word_importance.items():
        if len(importances) >= 5:  # å…¨ãƒ‡ãƒ¼ã‚¿ç”¨ã«é–¾å€¤ã‚’èª¿æ•´
            avg_importance[token] = np.mean(importances)
    
    print(f"âœ… {target}ã®BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼SHAPåˆ†æå®Œäº†: {len(avg_importance)}ãƒˆãƒ¼ã‚¯ãƒ³")
    return avg_importance

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    
    data_path = "../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv"
    
    if not os.path.exists(data_path):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return None
    
    df = pd.read_csv(data_path)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    
    return df

def stratified_sampling(df, n_samples=200):
    """å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
    if n_samples >= len(df):
        print(f"ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨: {len(df)}ä»¶")
        return df
    
    print(f"ğŸ“Š å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹: {len(df)}ä»¶ã‹ã‚‰{n_samples}ä»¶ã‚’æŠ½å‡º")
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨è©•ä¾¡ã‚¹ã‚³ã‚¢ã§å±¤åŒ–ï¼ˆå®Ÿéš›ã®åˆ—åã‚’ä½¿ç”¨ï¼‰
    df['sentiment_bin'] = pd.cut(df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'], bins=5, labels=False)
    df['course_bin'] = pd.cut(df['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'], bins=5, labels=False)
    
    sampled_df = df.groupby(['sentiment_bin', 'course_bin']).apply(
        lambda x: x.sample(min(len(x), max(1, n_samples // 25)), random_state=42)
    ).reset_index(drop=True)
    
    # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è£œå®Œ
    if len(sampled_df) < n_samples:
        remaining = n_samples - len(sampled_df)
        additional = df.sample(remaining, random_state=42)
        sampled_df = pd.concat([sampled_df, additional]).reset_index(drop=True)
    
    print(f"âœ… å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(sampled_df)}ä»¶ã‚’æŠ½å‡º")
    return sampled_df

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’SHAPåˆ†æ")
    print("BERTã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§é©åˆ‡ãªæ—¥æœ¬èªå‡¦ç†")
    print("=" * 60)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cpu')
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"PyTorch version: {torch.__version__}")
    
    print("ğŸš€ BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æã‚’é–‹å§‹...")
    
    # Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\n=== Phase 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ===")
    df = load_data()
    if df is None:
        return
    
    sampled_df = stratified_sampling(df, n_samples=len(df))  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    
    # Phase 2: BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‰å‡¦ç†
    print("\n=== Phase 2: BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‰å‡¦ç† ===")
    texts = sampled_df['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].tolist()
    processed_texts, word_to_id = bert_tokenizer_preprocessing(texts)
    
    # Phase 3: ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("\n=== Phase 3: BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä½œæˆ ===")
    vocab_size = len(word_to_id)
    model = create_bert_tokenizer_model(vocab_size)
    model.to(device)
    model.eval()
    print("âœ… BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
    
    # Phase 4: SHAPåˆ†æå®Ÿè¡Œ
    print("\n=== Phase 4: BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼SHAPåˆ†æå®Ÿè¡Œ ===")
    sentiment_importance = bert_tokenizer_shap_analysis(model, processed_texts, word_to_id, target='sentiment')
    course_importance = bert_tokenizer_shap_analysis(model, processed_texts, word_to_id, target='course')
    
    # Phase 5: çµæœä¿å­˜
    print("\n=== Phase 5: çµæœä¿å­˜ã¨å¯è¦–åŒ– ===")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = "../03_åˆ†æçµæœ/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ_BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼_å…¨ãƒ‡ãƒ¼ã‚¿"
    os.makedirs(output_dir, exist_ok=True)
    
    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSONå½¢å¼ã§ä¿å­˜
    results = {
        "analysis_date": timestamp,
        "method": "BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ",
        "sample_size": len(sampled_df),
        "vocab_size": vocab_size,
        "sentiment_factors": sentiment_importance,
        "course_factors": course_importance,
        "common_factors": {word: sentiment_importance.get(word, 0) + course_importance.get(word, 0) 
                         for word in set(sentiment_importance.keys()) & set(course_importance.keys())}
    }
    
    with open(f"{output_dir}/bert_tokenizer_analysis_{timestamp}.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # TOP20å¯è¦–åŒ–
    sentiment_top20 = sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True)[:20]
    course_top20 = sorted(course_importance.items(), key=lambda x: x[1], reverse=True)[:20]
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢è¦å› 
    plt.figure(figsize=(12, 8))
    words, values = zip(*sentiment_top20)
    plt.barh(range(len(words)), values, color='red', alpha=0.7)
    plt.yticks(range(len(words)), words)
    plt.xlabel('é‡è¦åº¦')
    plt.title('æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼)', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/sentiment_top20_bert_tokenizer_{timestamp}.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢è¦å› 
    plt.figure(figsize=(12, 8))
    words, values = zip(*course_top20)
    plt.barh(range(len(words)), values, color='blue', alpha=0.7)
    plt.yticks(range(len(words)), words)
    plt.xlabel('é‡è¦åº¦')
    plt.title('æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP20 (BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼)', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/course_top20_bert_tokenizer_{timestamp}.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    report = f"""# BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æçµæœ

## åˆ†ææ¦‚è¦
- åˆ†ææ—¥æ™‚: {timestamp}
- æ–¹æ³•: BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æ
- ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(sampled_df)}ä»¶
- èªå½™æ•°: {vocab_size}èªå½™

## ä¸»è¦çµæœ

### æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP10
"""
    
    for i, (word, importance) in enumerate(sentiment_top20[:10], 1):
        report += f"{i}. {word}: {importance:.6f}\n"
    
    report += "\n### æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬è¦å› TOP10\n"
    for i, (word, importance) in enumerate(course_top20[:10], 1):
        report += f"{i}. {word}: {importance:.6f}\n"
    
    report += f"""
## å…±é€šè¦å› 
å…±é€šè¦å› æ•°: {len(results['common_factors'])}èªå½™

## ç‰¹å¾´
- BERTã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
- æ—¥æœ¬èªã®å½¢æ…‹ç´ ã‚’é©åˆ‡ã«åˆ†å‰²
- å­¦ç¿’æ¸ˆã¿ã®èªå½™ã‚»ãƒƒãƒˆã‚’æ´»ç”¨
- ã‚ˆã‚Šè‡ªç„¶ãªæ—¥æœ¬èªå‡¦ç†
"""
    
    with open(f"{output_dir}/bert_tokenizer_analysis_report_{timestamp}.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ‰ BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯SHAPåˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print(f"âœ… BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã‚Šé©åˆ‡ãªæ—¥æœ¬èªè¦å› ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
