#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆæ¥­å˜ä½ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’
æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ + æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã‚’åŒæ™‚ã«å­¦ç¿’

ãƒ‡ãƒ¼ã‚¿æ§‹æˆ:
- å…¥åŠ›: æˆæ¥­ã®å…¨è‡ªç”±è¨˜è¿°ï¼ˆé›†å›£ãƒ¬ãƒ™ãƒ«ï¼‰
- å‡ºåŠ›1: æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ï¼ˆé›†å›£ãƒ¬ãƒ™ãƒ«ï¼‰
- å‡ºåŠ›2: æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆé›†å›£ãƒ¬ãƒ™ãƒ«ï¼‰
â†’ ãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´ã«ã‚ˆã‚Šå­¦ç¿’å¯èƒ½
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, BertJapaneseTokenizer
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
from datetime import datetime
import json
import os
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
BASE_MODEL = "koheiduck/bert-japanese-finetuned-sentiment"

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
MAX_LENGTH = 512
BATCH_SIZE = 8
LEARNING_RATE = 2e-5
NUM_EPOCHS = 20
ALPHA = 0.5  # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®é‡ã¿
BETA = 0.5   # è©•ä¾¡ã‚¹ã‚³ã‚¢ã®é‡ã¿

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
try:
    plt.rcParams['font.family'] = 'MS Gothic'
except:
    try:
        plt.rcParams['font.family'] = 'Yu Gothic'
    except:
        plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


class ClassLevelDataset(Dataset):
    """æˆæ¥­ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, texts, sentiment_scores, course_scores, tokenizer, max_length=512):
        self.texts = texts
        self.sentiment_scores = sentiment_scores
        self.course_scores = course_scores
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        sentiment_score = self.sentiment_scores[idx]
        course_score = self.course_scores[idx]
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'sentiment_score': torch.tensor(sentiment_score, dtype=torch.float),
            'course_score': torch.tensor(course_score, dtype=torch.float)
        }


class ClassLevelMultitaskModel(nn.Module):
    """æˆæ¥­ãƒ¬ãƒ™ãƒ«ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, base_model_name, dropout_rate=0.1):
        super().__init__()
        
        # BERTã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆå…±æœ‰å±¤ï¼‰
        self.bert = BertModel.from_pretrained(base_model_name)
        hidden_size = self.bert.config.hidden_size
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.sentiment_head = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 1)
        )
        
        # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ï¼ˆå›å¸°ï¼‰
        self.course_head = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 1)
        )
    
    def forward(self, input_ids, attention_mask):
        # BERTå‡ºåŠ›
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        pooled_output = outputs.last_hidden_state[:, 0, :]
        
        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        sentiment_pred = self.sentiment_head(pooled_output).squeeze(-1)
        course_pred = self.course_head(pooled_output).squeeze(-1)
        
        return sentiment_pred, course_pred


def load_data(sample_size=1000):
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆæœ€å°é™ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ“Š æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿")
    print("="*60)
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv('../01_ãƒ‡ãƒ¼ã‚¿/ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿/æˆæ¥­é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_20251012_142504.csv')
    
    print(f"ç·æˆæ¥­æ•°: {len(df)}ä»¶")
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å¾Œã§å®Ÿæ–½ï¼‰
    np.random.seed(42)
    if sample_size < len(df):
        sample_indices = np.random.choice(len(df), sample_size, replace=False)
        df_sampled = df.iloc[sample_indices].reset_index(drop=True)
        print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {sample_size}ä»¶ã‚’æŠ½å‡ºï¼ˆå®Ÿç”¨çš„æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°ï¼‰")
    else:
        df_sampled = df
        print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
    
    print(f"ä½¿ç”¨æˆæ¥­æ•°: {len(df_sampled)}ä»¶")
    print(f"åˆ—å: {list(df_sampled.columns)}")
    
    # å¿…è¦ãªåˆ—ã‚’æŠ½å‡º
    texts = df_sampled['è‡ªç”±è¨˜è¿°ã¾ã¨ã‚'].values
    sentiment_scores = df_sampled['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡'].values
    course_scores = df_sampled['æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢'].values
    
    print(f"\næ„Ÿæƒ…ã‚¹ã‚³ã‚¢å¹³å‡ã®çµ±è¨ˆ:")
    print(f"  å¹³å‡: {sentiment_scores.mean():.4f}")
    print(f"  æ¨™æº–åå·®: {sentiment_scores.std():.4f}")
    print(f"  ç¯„å›²: {sentiment_scores.min():.4f} ã€œ {sentiment_scores.max():.4f}")
    
    print(f"\næˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ:")
    print(f"  å¹³å‡: {course_scores.mean():.4f}")
    print(f"  æ¨™æº–åå·®: {course_scores.std():.4f}")
    print(f"  ç¯„å›²: {course_scores.min():.4f} ã€œ {course_scores.max():.4f}")
    
    return texts, sentiment_scores, course_scores


def prepare_data(texts, sentiment_scores, course_scores, tokenizer):
    """ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
    print("="*60)
    
    # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãŸã‚ã®å±¤ã‚’ä½œæˆ
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã¨æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ã¦å±¤ã‚’ä½œæˆ
    sentiment_bins = pd.qcut(sentiment_scores, q=3, labels=['ä½', 'ä¸­', 'é«˜'], duplicates='drop')
    course_bins = pd.qcut(course_scores, q=3, labels=['ä½', 'ä¸­', 'é«˜'], duplicates='drop')
    
    # å±¤ãƒ©ãƒ™ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦è©³ç´°ãªå±¤ã‚’ä½œæˆ
    stratify_labels = [f'{s}_{c}' for s, c in zip(sentiment_bins, course_bins)]
    
    print(f"\nå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åˆ†å¸ƒ:")
    unique, counts = np.unique(stratify_labels, return_counts=True)
    for label, count in zip(unique, counts):
        print(f"  {label}: {count}ä»¶")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆ70% / 15% / 15%ï¼‰å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    X_temp, X_test, y_sent_temp, y_sent_test, y_course_temp, y_course_test, strat_temp, strat_test = train_test_split(
        texts, sentiment_scores, course_scores, stratify_labels,
        test_size=0.15, random_state=42, stratify=stratify_labels
    )
    
    X_train, X_val, y_sent_train, y_sent_val, y_course_train, y_course_val = train_test_split(
        X_temp, y_sent_temp, y_course_temp, 
        test_size=0.176, random_state=42, stratify=strat_temp  # 0.176 â‰ˆ 15/85
    )
    
    print(f"\nãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶ ({len(X_train)/len(texts)*100:.1f}%)")
    print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}ä»¶ ({len(X_val)/len(texts)*100:.1f}%)")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶ ({len(X_test)/len(texts)*100:.1f}%)")
    
    # å„ã‚»ãƒƒãƒˆã®åˆ†å¸ƒã‚’ç¢ºèª
    print(f"\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
    print(f"  å¹³å‡: {y_sent_train.mean():.4f}, æ¨™æº–åå·®: {y_sent_train.std():.4f}")
    print(f"  ç¯„å›²: {y_sent_train.min():.4f} ã€œ {y_sent_train.max():.4f}")
    
    print(f"\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
    print(f"  å¹³å‡: {y_course_train.mean():.4f}, æ¨™æº–åå·®: {y_course_train.std():.4f}")
    print(f"  ç¯„å›²: {y_course_train.min():.4f} ã€œ {y_course_train.max():.4f}")
    
    # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–
    sentiment_scaler = StandardScaler()
    course_scaler = StandardScaler()
    
    y_sent_train_scaled = sentiment_scaler.fit_transform(y_sent_train.reshape(-1, 1)).flatten()
    y_sent_val_scaled = sentiment_scaler.transform(y_sent_val.reshape(-1, 1)).flatten()
    y_sent_test_scaled = sentiment_scaler.transform(y_sent_test.reshape(-1, 1)).flatten()
    
    y_course_train_scaled = course_scaler.fit_transform(y_course_train.reshape(-1, 1)).flatten()
    y_course_val_scaled = course_scaler.transform(y_course_val.reshape(-1, 1)).flatten()
    y_course_test_scaled = course_scaler.transform(y_course_test.reshape(-1, 1)).flatten()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
    train_dataset = ClassLevelDataset(X_train, y_sent_train_scaled, y_course_train_scaled, tokenizer, MAX_LENGTH)
    val_dataset = ClassLevelDataset(X_val, y_sent_val_scaled, y_course_val_scaled, tokenizer, MAX_LENGTH)
    test_dataset = ClassLevelDataset(X_test, y_sent_test_scaled, y_course_test_scaled, tokenizer, MAX_LENGTH)
    
    # DataLoaderã®ä½œæˆ
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    return train_loader, val_loader, test_loader, sentiment_scaler, course_scaler


def train_epoch(model, train_loader, optimizer, scheduler, epoch, num_epochs):
    """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’"""
    model.train()
    total_loss = 0
    sentiment_losses = 0
    course_losses = 0
    
    criterion = nn.MSELoss()
    
    for batch_idx, batch in enumerate(train_loader):
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        sentiment_true = batch['sentiment_score'].to(device)
        course_true = batch['course_score'].to(device)
        
        # å‹¾é…ã‚’ã‚¼ãƒ­åŒ–
        optimizer.zero_grad()
        
        # äºˆæ¸¬
        sentiment_pred, course_pred = model(input_ids, attention_mask)
        
        # æå¤±è¨ˆç®—
        sentiment_loss = criterion(sentiment_pred, sentiment_true)
        course_loss = criterion(course_pred, course_true)
        loss = ALPHA * sentiment_loss + BETA * course_loss
        
        # é€†ä¼æ’­
        loss.backward()
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        optimizer.step()
        
        # æå¤±ã®è¨˜éŒ²
        total_loss += loss.item()
        sentiment_losses += sentiment_loss.item()
        course_losses += course_loss.item()
        
        # é€²æ—è¡¨ç¤ºï¼ˆ10ãƒãƒƒãƒã”ã¨ï¼‰
        if batch_idx % 10 == 0:
            print(f'  Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, '
                  f'Loss: {loss.item():.4f}')
    
    # å­¦ç¿’ç‡ã®èª¿æ•´
    scheduler.step()
    
    avg_loss = total_loss / len(train_loader)
    avg_sentiment_loss = sentiment_losses / len(train_loader)
    avg_course_loss = course_losses / len(train_loader)
    
    return avg_loss, avg_sentiment_loss, avg_course_loss


def validate(model, val_loader):
    """æ¤œè¨¼"""
    model.eval()
    total_loss = 0
    sentiment_losses = 0
    course_losses = 0
    
    sentiment_preds_list = []
    sentiment_true_list = []
    course_preds_list = []
    course_true_list = []
    
    criterion = nn.MSELoss()
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            sentiment_true = batch['sentiment_score'].to(device)
            course_true = batch['course_score'].to(device)
            
            # äºˆæ¸¬
            sentiment_pred, course_pred = model(input_ids, attention_mask)
            
            # æå¤±è¨ˆç®—
            sentiment_loss = criterion(sentiment_pred, sentiment_true)
            course_loss = criterion(course_pred, course_true)
            loss = ALPHA * sentiment_loss + BETA * course_loss
            
            # æå¤±ã®è¨˜éŒ²
            total_loss += loss.item()
            sentiment_losses += sentiment_loss.item()
            course_losses += course_loss.item()
            
            # äºˆæ¸¬å€¤ã®è¨˜éŒ²
            sentiment_preds_list.extend(sentiment_pred.cpu().numpy())
            sentiment_true_list.extend(sentiment_true.cpu().numpy())
            course_preds_list.extend(course_pred.cpu().numpy())
            course_true_list.extend(course_true.cpu().numpy())
            
            # é€²æ—è¡¨ç¤ºï¼ˆ5ãƒãƒƒãƒã”ã¨ï¼‰
            if batch_idx % 5 == 0:
                print(f'    Validation Batch {batch_idx+1}/{len(val_loader)}')
    
    avg_loss = total_loss / len(val_loader)
    avg_sentiment_loss = sentiment_losses / len(val_loader)
    avg_course_loss = course_losses / len(val_loader)
    
    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    sentiment_preds = np.array(sentiment_preds_list)
    sentiment_true = np.array(sentiment_true_list)
    course_preds = np.array(course_preds_list)
    course_true = np.array(course_true_list)
    
    sentiment_r2 = r2_score(sentiment_true, sentiment_preds)
    sentiment_corr = np.corrcoef(sentiment_true, sentiment_preds)[0, 1]
    
    course_r2 = r2_score(course_true, course_preds)
    course_corr = np.corrcoef(course_true, course_preds)[0, 1]
    
    return avg_loss, avg_sentiment_loss, avg_course_loss, sentiment_r2, sentiment_corr, course_r2, course_corr


def train_model(model, train_loader, val_loader, num_epochs):
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    print("\n" + "="*60)
    print("ğŸš€ æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã‚’é–‹å§‹")
    print("="*60)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    scheduler = torch.optim.lr_scheduler.LinearLR(
        optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs
    )
    
    # å­¦ç¿’å±¥æ­´
    history = {
        'train_loss': [],
        'train_sentiment_loss': [],
        'train_course_loss': [],
        'val_loss': [],
        'val_sentiment_loss': [],
        'val_course_loss': [],
        'val_sentiment_r2': [],
        'val_sentiment_corr': [],
        'val_course_r2': [],
        'val_course_corr': []
    }
    
    best_val_loss = float('inf')
    best_model_state = None
    
    for epoch in range(num_epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"{'='*60}")
        
        # å­¦ç¿’
        train_loss, train_sent_loss, train_course_loss = train_epoch(
            model, train_loader, optimizer, scheduler, epoch, num_epochs
        )
        
        # æ¤œè¨¼
        val_loss, val_sent_loss, val_course_loss, sent_r2, sent_corr, course_r2, course_corr = validate(
            model, val_loader
        )
        
        # å±¥æ­´ã®è¨˜éŒ²
        history['train_loss'].append(train_loss)
        history['train_sentiment_loss'].append(train_sent_loss)
        history['train_course_loss'].append(train_course_loss)
        history['val_loss'].append(val_loss)
        history['val_sentiment_loss'].append(val_sent_loss)
        history['val_course_loss'].append(val_course_loss)
        history['val_sentiment_r2'].append(sent_r2)
        history['val_sentiment_corr'].append(sent_corr)
        history['val_course_r2'].append(course_r2)
        history['val_course_corr'].append(course_corr)
        
        # çµæœè¡¨ç¤º
        print(f"\nå­¦ç¿’çµæœ:")
        print(f"  Total Loss: {train_loss:.4f}")
        print(f"  Sentiment Loss: {train_sent_loss:.4f}")
        print(f"  Course Loss: {train_course_loss:.4f}")
        
        print(f"\næ¤œè¨¼çµæœ:")
        print(f"  Total Loss: {val_loss:.4f}")
        print(f"  Sentiment - RÂ²: {sent_r2:.4f}, ç›¸é–¢: {sent_corr:.4f}")
        print(f"  Course    - RÂ²: {course_r2:.4f}, ç›¸é–¢: {course_corr:.4f}")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            print(f"\nâœ… ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ï¼ (Val Loss: {val_loss:.4f})")
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model.load_state_dict(best_model_state)
    
    return model, history


def evaluate_model(model, test_loader, sentiment_scaler, course_scaler):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("\n" + "="*60)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡")
    print("="*60)
    
    model.eval()
    
    sentiment_preds_list = []
    sentiment_true_list = []
    course_preds_list = []
    course_true_list = []
    
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            sentiment_true = batch['sentiment_score'].to(device)
            course_true = batch['course_score'].to(device)
            
            # äºˆæ¸¬
            sentiment_pred, course_pred = model(input_ids, attention_mask)
            
            # äºˆæ¸¬å€¤ã®è¨˜éŒ²
            sentiment_preds_list.extend(sentiment_pred.cpu().numpy())
            sentiment_true_list.extend(sentiment_true.cpu().numpy())
            course_preds_list.extend(course_pred.cpu().numpy())
            course_true_list.extend(course_true.cpu().numpy())
    
    # numpyé…åˆ—ã«å¤‰æ›
    sentiment_preds = np.array(sentiment_preds_list)
    sentiment_true = np.array(sentiment_true_list)
    course_preds = np.array(course_preds_list)
    course_true = np.array(course_true_list)
    
    # æ­£è¦åŒ–ã‚’æˆ»ã™
    sentiment_preds_original = sentiment_scaler.inverse_transform(sentiment_preds.reshape(-1, 1)).flatten()
    sentiment_true_original = sentiment_scaler.inverse_transform(sentiment_true.reshape(-1, 1)).flatten()
    course_preds_original = course_scaler.inverse_transform(course_preds.reshape(-1, 1)).flatten()
    course_true_original = course_scaler.inverse_transform(course_true.reshape(-1, 1)).flatten()
    
    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    results = {
        'sentiment': {
            'rmse': float(np.sqrt(mean_squared_error(sentiment_true_original, sentiment_preds_original))),
            'mae': float(mean_absolute_error(sentiment_true_original, sentiment_preds_original)),
            'r2': float(r2_score(sentiment_true_original, sentiment_preds_original)),
            'correlation': float(np.corrcoef(sentiment_true_original, sentiment_preds_original)[0, 1])
        },
        'course': {
            'rmse': float(np.sqrt(mean_squared_error(course_true_original, course_preds_original))),
            'mae': float(mean_absolute_error(course_true_original, course_preds_original)),
            'r2': float(r2_score(course_true_original, course_preds_original)),
            'correlation': float(np.corrcoef(course_true_original, course_preds_original)[0, 1])
        }
    }
    
    # çµæœè¡¨ç¤º
    print("\næ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®çµæœ:")
    print(f"  RMSE: {results['sentiment']['rmse']:.4f}")
    print(f"  MAE: {results['sentiment']['mae']:.4f}")
    print(f"  RÂ²: {results['sentiment']['r2']:.4f}")
    print(f"  ç›¸é–¢ä¿‚æ•°: {results['sentiment']['correlation']:.4f}")
    
    print("\næˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã®çµæœ:")
    print(f"  RMSE: {results['course']['rmse']:.4f}")
    print(f"  MAE: {results['course']['mae']:.4f}")
    print(f"  RÂ²: {results['course']['r2']:.4f}")
    print(f"  ç›¸é–¢ä¿‚æ•°: {results['course']['correlation']:.4f}")
    
    return results, sentiment_preds_original, sentiment_true_original, course_preds_original, course_true_original


def save_results(model, history, results, timestamp):
    """çµæœã®ä¿å­˜"""
    print("\n" + "="*60)
    print("ğŸ’¾ çµæœã®ä¿å­˜")
    print("="*60)
    
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = f'../02_ãƒ¢ãƒ‡ãƒ«/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«'
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    model_path = os.path.join(output_dir, 'best_class_level_multitask_model.pth')
    torch.save(model.state_dict(), model_path)
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_path}")
    
    # è¨­å®šã®ä¿å­˜
    config = {
        'model_type': 'ClassLevelMultitaskModel',
        'base_model': BASE_MODEL,
        'max_length': MAX_LENGTH,
        'batch_size': BATCH_SIZE,
        'learning_rate': LEARNING_RATE,
        'num_epochs': NUM_EPOCHS,
        'alpha': ALPHA,
        'beta': BETA,
        'data_level': 'class_level',
        'data_size': 3268
    }
    
    config_path = os.path.join(output_dir, 'model_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"è¨­å®šã‚’ä¿å­˜: {config_path}")
    
    # çµæœã®ä¿å­˜
    results_dir = f'../03_åˆ†æçµæœ/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’'
    os.makedirs(results_dir, exist_ok=True)
    
    results_data = {
        'timestamp': timestamp,
        'data_level': 'class_level',
        'data_size': 3268,
        'results': results,
        'training_history': history
    }
    
    results_path = os.path.join(results_dir, f'class_level_multitask_results_{timestamp}.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, ensure_ascii=False, indent=2)
    print(f"çµæœã‚’ä¿å­˜: {results_path}")


def create_visualizations(history, sentiment_preds, sentiment_true, course_preds, course_true, timestamp):
    """å¯è¦–åŒ–ã®ä½œæˆ"""
    print("\n" + "="*60)
    print("ğŸ“Š å¯è¦–åŒ–ã®ä½œæˆ")
    print("="*60)
    
    # 1. å­¦ç¿’æ›²ç·š
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Loss
    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')
    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training and Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # RÂ²ã‚¹ã‚³ã‚¢
    axes[0, 1].plot(history['val_sentiment_r2'], label='Sentiment R2', marker='o')
    axes[0, 1].plot(history['val_course_r2'], label='Course R2', marker='s')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('R2 Score')
    axes[0, 1].set_title('R2 Score Progress')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # ç›¸é–¢ä¿‚æ•°
    axes[1, 0].plot(history['val_sentiment_corr'], label='Sentiment Correlation', marker='o')
    axes[1, 0].plot(history['val_course_corr'], label='Course Correlation', marker='s')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Correlation')
    axes[1, 0].set_title('Correlation Progress')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # ã‚¿ã‚¹ã‚¯åˆ¥æå¤±
    axes[1, 1].plot(history['train_sentiment_loss'], label='Train Sentiment Loss', marker='o')
    axes[1, 1].plot(history['train_course_loss'], label='Train Course Loss', marker='s')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].set_title('Task-wise Training Loss')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    results_dir = '../03_åˆ†æçµæœ/æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’'
    os.makedirs(results_dir, exist_ok=True)
    plt.savefig(os.path.join(results_dir, f'training_curves_{timestamp}.png'), dpi=300, bbox_inches='tight')
    print(f"å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    plt.close()
    
    # 2. äºˆæ¸¬vsçœŸå€¤ã®æ•£å¸ƒå›³
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
    axes[0].scatter(sentiment_true, sentiment_preds, alpha=0.6, s=20)
    axes[0].plot([sentiment_true.min(), sentiment_true.max()], 
                 [sentiment_true.min(), sentiment_true.max()], 
                 'r--', label='Perfect Prediction')
    axes[0].set_xlabel('True Sentiment Score')
    axes[0].set_ylabel('Predicted Sentiment Score')
    axes[0].set_title(f'Sentiment Score Prediction (RÂ²={r2_score(sentiment_true, sentiment_preds):.4f})')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # æˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢
    axes[1].scatter(course_true, course_preds, alpha=0.6, s=20)
    axes[1].plot([course_true.min(), course_true.max()], 
                 [course_true.min(), course_true.max()], 
                 'r--', label='Perfect Prediction')
    axes[1].set_xlabel('True Course Evaluation Score')
    axes[1].set_ylabel('Predicted Course Evaluation Score')
    axes[1].set_title(f'Course Score Prediction (RÂ²={r2_score(course_true, course_preds):.4f})')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, f'prediction_scatter_{timestamp}.png'), dpi=300, bbox_inches='tight')
    print(f"æ•£å¸ƒå›³ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    plt.close()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        print("\n" + "="*60)
        print("ğŸ¯ æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’")
        print("="*60)
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        print("\nğŸ”§ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–...")
        tokenizer = BertJapaneseTokenizer.from_pretrained(BASE_MODEL)
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        texts, sentiment_scores, course_scores = load_data()
        
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        train_loader, val_loader, test_loader, sentiment_scaler, course_scaler = prepare_data(
            texts, sentiment_scores, course_scores, tokenizer
        )
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        print("\nğŸ”§ ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–...")
        model = ClassLevelMultitaskModel(BASE_MODEL)
        model = model.to(device)
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        print(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
        
        # å­¦ç¿’
        model, history = train_model(model, train_loader, val_loader, NUM_EPOCHS)
        
        # è©•ä¾¡
        results, sentiment_preds, sentiment_true, course_preds, course_true = evaluate_model(
            model, test_loader, sentiment_scaler, course_scaler
        )
        
        # çµæœã®ä¿å­˜
        save_results(model, history, results, timestamp)
        
        # å¯è¦–åŒ–
        create_visualizations(history, sentiment_preds, sentiment_true, 
                            course_preds, course_true, timestamp)
        
        print("\n" + "="*60)
        print("âœ… æˆæ¥­ãƒ¬ãƒ™ãƒ«ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("="*60)
        print(f"çµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
        print("\nğŸ“Š æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼")
        print("="*60)
        print("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢äºˆæ¸¬:")
        print(f"  RÂ²: {results['sentiment']['r2']:.4f}")
        print(f"  ç›¸é–¢ä¿‚æ•°: {results['sentiment']['correlation']:.4f}")
        print(f"  RMSE: {results['sentiment']['rmse']:.4f}")
        
        print("\næˆæ¥­è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬:")
        print(f"  RÂ²: {results['course']['r2']:.4f}")
        print(f"  ç›¸é–¢ä¿‚æ•°: {results['course']['correlation']:.4f}")
        print(f"  RMSE: {results['course']['rmse']:.4f}")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

